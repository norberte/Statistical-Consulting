last ten years witnessed enormous increases computer memory connectivity storage sensor capabilities making possible obtain send information nearly anywhere everywhere pocket size devices easy carry one s wallet long term implications developments conducting surveys likely enormous will happen measured pace technology allows presentation will_discuss transition away long established survey data collection methods towards newer ones challenge data collection methodologists face neither far ahead far behind people time rapidly accelerating change particular will_discuss recent development web push data collection methods e contacting sample units one mode usually postal mail request response Internet withholding response options later contacts
well known methods analyse longitudinal continuous variables regularly spaced measurements particular mixed_models adapted deal longitudinal quantitative traits Nonetheless problem far solved modelling analysis longitudinal multivariate discrete continuous traits poses various challenges accommodate family structure irregular time measurements presentation discuss challenges propose ways addressing problems
detecting genetic variants associated disease trait useful consider ancestral trees gave rise sample s genetic variability rare common disease trait influencing genetic variants expect see haplotypes individuals similar values disease trait clustered together ancestral tree corresponding genomic location variant presentation describe tree based statistics can used detecting rare common genetic variants associated either continuous dichotomous outcomes summarize performance statistics simulated_data known missing tree structures compare results obtained using conventional approaches detect genetic association Finally application tree based method real_data also discussed
Genetic population data result evolution chromosomes population thus found natural try analyze data modeling unknown history population combining approach based coalescent process recombination importance sampling techniques can show stochastic process can used map genes influencing disease address issues pertaining complex diseases whether complexity due incomplete penetrance phenocopy multiple rare variants interaction genes introduce progress made building histories outline remaining challenges
One interesting questions statistics handle duality developing methodology sound also easily interpretable applied space Particularly medical research may understanding problem simplified easy testing power simplified solutions become accepted standard practice methodologies capacity grow worth revisiting simplifications develop methodology better align goal statistician opportunity either aim robust approach present new method new measurement addresses simplifications work incremental approach steps applied user closer goal time will work example composite endpoints used cardiovascular clinical trials trials fatal non fatal events combined single analysis regulatory approval
successful industry collaboration spanned around eight years received support NSERC OCE MITACS described Details many aspects collaboration discussed including funding opportunities exploited Special attention paid research took place part collaboration tangentially
provide overview types disclosure risks traditional forms data dissemination statistical agencies risk information loss can quantified recommended applications disclosure_control techniques Statistical agencies however making use restricting licensing data statistical disclosure_control approach given difficulties producing safe data look new strategies data dissemination discuss potential limitations include web based applications outputs protected without need human intervention check disclosure risks site data enclaves extension researchers can access confidential data personal PCs remote server production release synthetic data based advanced statistical_models link Computer Science definition differential privacy will discussed
synthetic datasets proposed way protect confidentiality respondents reasoning since anyone real dataset pose risk understanding risk datasets however evolved now know risk free Nevertheless synthetic datasets can useful tool privacy protection now growing literature best techniques creating analysing synthetic datasets lead term synthetic dataset used lot different contexts clear anymore essence synthetic dataset really differentiates synthetic dataset perturbed real dataset talk explores main attributes synthetic datasets offers thoughts thinking talking
Statistical agencies interested publishing useful statistical data may lead disclosure individuals private data problem leads trade utility published data risk disclosure confidential data Disclosure control can seen use methods deal problem assessing controlling risk disclosing confidential data also providing researchers useful statistical data paper describes disclosure_control model based largely Bayesian decision theory model allows description concepts disclosure_control terms familiar statistical concepts expectation variance method disclosure_control called random tabular adjustment RTA described method controls risk disclosure randomly adjusting data instead suppressing cells fits naturally disclosure_control model described
consider ill posed inverse problem arising magnetoencephalography MEG electroencephalography EEG used measure electromagnetic brain activity array sensors scalp interest map data back sources neural activity within brain review existing approaches solving inverse problem discuss mesostate space model MSM proposed Daunizeau Friston Neuroimage propose new model builds MSM incorporates three major extensions combine EEG MEG data together formulate joint model source reconstruction ii incorporate Potts model represent spatial dependence allocation process partitions cortical surface small number latent mesostates iii formulate mesostate dynamics flexible manner model can characterize functional connectivity mesosources formulate model discuss computational implementation make comparisons existing_methods
Often geospatial disease outcomes characterized sparseness viewed count distribution sometimes called zero inflation Classically models zero inflation two types zero class modelled structural Poisson hurdle models zeroes treated completely separately truncated Poisson positive counts Often idea structural zeroes made specific attempt made assess zeroes class talk describe approach models spatial structure structural zero class provides estimates spatial distribution probability structural applied spatio temporal context Different assumptions spatial prior distribution structural probability compared using Bayesian formulation example spatio temporal modeling sudden infant death counties Georgia USA will presented
Dr Brown collaborators presented local EM algorithm smoothing aggregated data number times past SSC conferences although methodology computationally intensive labour intensive attract following year s talk_will change situation presentation R_package description methodological computational improvements made method useful practice Efficient use R s spatial data structures parallel processing decreased computational burden method manageable level Re casting algorithm modelling latent Gaussian process rather kernel smoother allowed sparse matrix Markov random field approximation covariance_matrix R_package includes several examples using real datasets
important statistical task disease mapping problems identify divergent regions unusually high low risk disease Leave one cross validatory LOOCV model assessment gold standard estimating predictive p values can flag divergent regions However LOOCV time consuming introduce new method called integrated importance sampling iIS estimating LOOCV predictive p values Markov_chain samples drawn posterior based full data_set formula used iIS can proved equivalent LOOCV predictive p value compare iIS three existing_methods literature two disease mapping datasets empirical results show predictive p values estimated iIS almost identical predictive p values estimated actual LOOCV outperform given existing three methods namely posterior predictive checking ordinary importance sampling ghosting method Marshall Spiegelhalter
Systems differential equations often used model buffering processes modulate non smooth high energy input produce output smooth distributes energy load time space Handwriting buffered way show smooth complex script spells statistics Chinese can represented buffered version series equal interval step inputs buffer consists three undamped oscillating springs one orthogonal coordinate periods oscillation vary slightly three coordinate way reflects masses moved muscle activations use term dynamic smoothing estimation structured input functional object along buffer characteristics
major challenge inference spatio temporal dynamical systems high dimension simulation models used model integration forward simulation extremely computationally costly Ensemble based data assimilation emerged method choice state estimation environmental prediction based numerical models ocean climate atmosphere also interest parameter_estimation uncertainty quantification UQ problems talk_will discuss approximate approaches facilitate state parameter_estimation well UQ computationally costly systems overview will include ensemble Kalman filters various approximations particle filters use emulators also discussed approaches illustrated using data assimilation problems oceanography
Acute respiratory diseases ARI public health concern associated morbidity mortality especially children elderly nature individual interactions mean disease dynamics large population may described stochastic process model governed system stochastic differential equations SDE multiple ARI circulation dynamics complicated corresponding SDE can solved analytically analysis examine challenges inference models multiple pathogen dynamics including inability distinguish different pathogens based symptoms partial symmetries model discuss overcome resulting identifiability problems utilizing additional data sources modeling data multiple outbreak seasons simultaneously
average treatment effect ATE often primary interest causal_inference commonly estimated based inverse probability weighting IPW method validity however challenged presence error prone outcomes paper investigate various issues concerning IPW estimation ATE mismeasured outcome variables study impact measurement_error reveal important consequences naive analysis ignores measurement_error continuous outcome variable mismeasured additive measurement_error model naive analysis still yields consistent estimator outcome binary derive asymptotic bias closed form conduct valid inference develop estimation procedures practical scenarios either validation data replicates available Furthermore validation data propose efficient method substantially outperforms usual methods using validation data
estimation missing_data crucial step determine data missing completely random MCAR case complete case analysis suffice existing tests MCAR provide method subsequent estimation MCAR rejected setting estimating means response variables subject missingness propose unified approach testing MCAR subsequent estimation Upon rejecting MCAR set weights used testing can used estimation resulting estimators consistent missingness response_variable depends set fully observed auxiliary variables true outcome regression_model among user specified functions deriving weights proposed procedure based calibration idea survey sampling literature empirical_likelihood theory joint work Peisong Han Changbao Wu
dynamic treatment regime DTR set decision rules applied across multiple stages treatments decisions tailored individuals inputting individual s observed characteristics outputting treatment decision stage individual Dynamic weighted ordinary least squares dWOLS theoretically robust easily implemented method estimating optimal DTR Like many DTR estimators dWOLS estimators can non regular true treatment_effects zero small resulting invalid Wald type standard bootstrap confidence_intervals Inspired analysis effect diet infancy weight body size childhood investigate use m n bootstrap dWOLS valid inferences optimal DTR provide extensive simulation_study compare performance different choices resample size m situations treatment_effects non regular illustrate methodology study effect solid food intake infancy long term health outcomes
important goal estimating causal effect achieve balance covariates propose using kernel distance measure balance among different treatment groups propose new propensity score estimator setting kernel distance zero estimating_equations solved generalized method moments Simulation_studies conducted across different scenarios varying degree nonlinearity simulation_study shows proposed approaches produce smaller mean_squared errors estimating causal treatment_effects many existing approaches including well known covariate balance propensity score CBPS approach shown kernel distance one best bias indicators estimating causal effect compared balance measures absolute standardized mean difference ASMD KS statistic
meta analysis continuous outcome usually studies report medians excluded median spread used estimate mean standard deviation order estimate pooled effect across studies previous work proposed several methods directly meta analyze medians showed via simulation methods outperform methods estimate means standard deviations studies reporting medians provided guidelines data analysts improve performance meta analyzing studies report medians apply novel methods recently published meta analyses evaluate extent pooled estimates conclusions drawn meta analyses change using methods
segmented regression analysis approach useful method may used evaluate effects public health interventions approach may superior standard time_series analyses allows short long term effects evaluation adjusting potential confounders assessing study subject characteristics like socioeconomic status SES may influence effects used segmented regression approach study implementation pediatric diabetes network Ontario decreased SES disparities emergency ED visits used population based aggregate data annual crude rates level SES potential confounders geographic location sex age group determined population average adjusted ED visit rate ratios accounting number individuals aggregate using generalized estimating equation Poisson link observed network implementation ED visits decreased significantly lowest SES quintiles concluded segmented regression analysis approach may used analyse aggregate ecological data adjusting potential confounders effect modifiers
consider tail empirical process TEP related distribution regularly varying tail important tool used nonparametric estimation extremal quantities like Hill estimator index regular variation various risk_measures talk consider long memory stochastic volatility model interest finance first start investigating probabilistic properties model establish central non central limit theorems TEP apply results investigate asymptotic behaviour aforementioned extremal quantities theoretical results illustrated simulation_studies
paper present analysis recurrent_event data subject censoring using generalized estimating_equations conditional means variances gap times Censoring dealt imposing conditional independence assumptions censored gap times covariates censoring times Simulation results demonstrate relative robustness parametric estimates even assumption incorrect actually estimate censored gap time using observed data solving estimating_equations asymptotically unbiased prove strong consistency using modern analytical techniques illustrate results simulations
provide analytically simple computationally efficient solution discrete time queueing model GIX Geom Using imbedded Markov_chain method analysis carried late arrival system results early arrival system derived late arrival system probability distributions numbers system various epochs found terms roots underlying characteristic equation Numerical results also discussed
talk consider estimation problem concerning matrix correlation coefficient context high_dimensional data settings particular revisited results Li Rolsalsky Li D Rolsalsky strong limit theorems largest entries sample correlation matrices Annals Applied Probability extend one findings Li Rosalsky simplify remarkably proof one result quoted paper generalize theorem useful deriving existence pth moment well studying convergence rates law large numbers
rare event simulation plain Monte_Carlo estimator bound imprecise since samples underlying distribution low chance landing important region Importance sampling IS popular variance reduction technique situations IS draws samples proposal distribution constricted way places heavier weights important region design effective proposal distribution however generally hard many variables involved becomes harder characterize part domain corresponds important region Fortunately high_dimensional problems low dimensional structures many cases one dimensional projection single index input vector captures large majority overall variance IS technique exploits low dimensional structure problems constructs efficient proposal distribution
Mixture modelling increasingly used clinical epidemiological research clustering patients different developmental trajectories Many previous studies concerned continuous data whereas limited number studies focus evaluating appropriate approaches clustering binary data Using simulated_data study aims investigate several mixture modeling approaches namely standard latent class model growth mixture model K means mixture model Bayesian latent class model Various scenarios differing sample sizes number classes class sizes development trajectories shapes considered focus evaluating performance approaches recovering true class membership development trajectories longitudinal binary responses illustration also present case study using real_data
Quantile expectile regression models pertain estimation unknown quantiles expectiles cumulative distribution function dependent variable function set covariates vector regression coefficients approaches make assumption shape distribution response_variable allowing investigation comprehensive class covariate effects develop methods panel data within random_effects framework provide asymptotic properties underlying model parameter estimators suggest appropriate estimators variances covariances matrices performance suggested estimators evaluated simulation_studies methodology illustrated using real_data simulations show expectile regression comparable quantile_regression easily computable relevant statistical properties conclusion expectiles mean quantiles median used interpreted quantilized mean
propose functional semiparametric additive model incorporates effects functional covariates scalar covariates specifically effect functional covariate modeled nonparametrically linear form adopted model effects scalar covariates strategy can enhance flexibility modelling effect functional covariate maintaining interpretability effects scalar covariates Additionally estimation method developed smooth select non vanishing components estimating nonparametric components coefficients scalar covariates can obtained simultaneously proposed_method finite sample performance proposed model corresponding estimation method compared various methods using simulation_studies proposed_method demonstrated real application predicting protein content meat samples using Tecator absorbance spectra fat water contents meat samples
Recently two different copula based approaches proposed estimate conditional quantile function variable Y respect vector covariates X first estimator related quantile_regression weighted conditional copula density second estimator based inverse conditional distribution function written terms margins copula Using empirical processes show even two estimators look quite different converge limit also propose bootstrap procedure limiting process order able construct uniform confidence bands around conditional quantile function case study based hydro climatic data illustrates proposed methodology
Suboptimal gestational weight gain GWG linked increased risk adverse outcomes pregnant woman infant prevalent large cohort study Canadian pregnant women goals estimate individual weight growth trajectory using sparsely collected data identify factors affecting total GWG first goal achieved functional principal component analysis FPCA conditional expectation second goal used linear_regression total weight gain response_variable trajectory modeling FPCA significantly smaller mean square error improved adaptability traditional nonlinear mixed effect models demonstrating novel tool facilitate real time monitoring interventions GWG regression analysis showed prepregnancy body mass index BMI high predictive value total GWG agrees published weight gain guideline
Health utility HU data negatively skewed bounded observations lying close bound extremely low levels HU analyzed linear_regression OLS assumes normally distributed errors lacks restriction upper limit Ordinal Regression OR limited assumptions produces estimates bounded study compares performance OR OLS prediction HU given covariates using simulated real_data prostate cancer patients Using various sample sizes three different distributions HU data simulated analyzed OLS OR Models evaluated bias coverage probability estimated mean using Root Mean Square Error RMSE comparing simulated predicted HU values Results show OR provides accurate estimates regardless sample_size OLS bias increases HU approaches zero Coverage probability similar methods
theory statistical_inference along strategy divide combine large scale data analysis recently attracted considerable interest due popularity MapReduce scheme key development statistical_inference lies method combining results yielded separately mapped data batches consider general inferential methodology based estimating functions allows us perform regression analyses massive complex data via MapReduce scheme longitudinal data survival data quantile_regression can done using maximum_likelihood method proposed statistical_inference inherits many key large sample properties estimating functions Also show proposed_method closely connected generalized method moments GMM method provides unified framework many kinds statistical_models data types illustrated via numerical examples simulation_studies real world data analyses
Imprecise probability IP generalization de Finetti s approach probability formalized Williams extensively discussed monograph Walley presented ideas JRSS discussion paper methodology can interpreted Bayesian sensitivity analysis using set priors presentation Walley introduced practical form inference discrete data using family Dirichlet priors fixed concentration parameter proposed also used survival data discretization Coolen extended Walley s model allow censoring Coolen Yan proposed nonparametric predictive inference paradigm also led IP conclusions Bickis used IP concepts estimate hazard function reviewing methodology will presenting IP version log rank test will produce sequence imprecise posteriors updated time death
Clinical trial prescribed learning process Bayesian methods take learn go approach uniquely suitable learning recent years rapid advancements medicine demand innovative methods identify better therapies appropriate population timely efficient way will first illustrate concept Bayesian update Bayesian inference give overview Bayesian adaptive designs dose finding predictive probability multi arm platform design outcome adaptive randomization etc Applications including BATTLE trials lung cancer SPY trials breast_cancer will given Bayesian adaptive designs increase study efficiency allow flexible trial conduct treat patients effective treatments trial also possess desirable frequentist properties Perspectives will given future development trial design conduct evaluation streamline speed drug approval
Modern functional magnetic resonance imaging fMRI techniques provide measurements ongoing activity whole human brain unprecedented spatial mm temporal s resolution Unfortunately measurements corrupted substantial noise arising intermittent artifacts subject motion well non neural physiological processes Latter processes lead characteristic low frequency drifts times series data Noise processes also show characteristic spatial structure depends substantially underlying neuroanatomy become clear many current noise models implemented commonly_used fMRI analysis packages lead suboptimal estimation model parameters provide adequate statistical_inference will show developments lab improve spatio temporal noise models will demonstrate impact single voxel analysis multi variate pattern analysis whole brain functional connectivity analysis
Yield curves functions defined time maturity corresponding values equal yield interest bond typically standardized government issued instrument Yield curves commonly_used predict future states economy basis interest investors demand government debt various maturities curves form time_series functions one function per day talk_will discuss methods detecting change point mean function functional time_series reviewing recent research will present two methods one uses factor representation yield curves fully nonparametric method methods permit second order structure change independently changes mean structure Based asymptotic theory two numerical approaches implementation tests will presented compared Application US Federal Reserve yield curves will presented
propose new concepts order analyse model dependence_structure two time_series methods rely exclusively order structure data points Hence methods stable monotone transformations time_series robust small perturbations measurement errors Ordinal pattern dependence can characterised four parameters propose estimators parameters calculate asymptotic distributions Furthermore derive test structural breaks within dependence_structure results supplemented simulation_studies empirical examples
University British Columbia recently launched professional master s program Data Science first cohort students began studies September talk_will describe program challenges successes encountered thus far Points emphasis will include thinking choice topics include choice prerequisites choice delivery methods Since students will undertaking capstone projects throughout May June also plan give live update working
talk_will describe new undergraduate Masters Data Science programs University Waterloo will_discuss programs genesis motivation target audience well challenges encountered along way session will conclude discussion Data Science generally issues questions starting Data Science program
Observational databases events related health disease processes widely used study human health can pose challenges rigorous analysis due part selection effects related inclusion database missing information important factors addition times data observed individual may related outcomes covariates study selection observation processes usually include elements non ignorable failure adjust can produce biased analysis talk_will discuss likelihood weighted estimating function methods dealing non ignorable observation processes Applications analysis data psoriatic arthritis cohort will considered
population cancer survivors increasing rapidly result advances cancer treatment Consequently greatly increased interest evaluating late mortality morbidity survivors health care utilization talk presents strategies risk classification prediction counts physician visits associated costs cancer survivors administrative health databases illustrate methodology using physician claims associated subjects CAYACS survivor cohort BCCA Cancer Agency British Columbia Canada
Considerable investments made conduct large scale cohort studies involve routine followup participants Loss followup common particularly study duration long Standard likelihood methods ignoring missing_data yield consistent less efficient estimators Sequential Missing Random SMAR whereas bias arises assumption violated unless missing_data modelled correctly Tracing disease status lost followup help address missing_data problem lead potential efficiency gain however relatively little attention given design studies work proposes selection models tracing based history disease process models optimized achieve maximum efficiency gains fixed sample_size budget extension develop adaptive two phase tracing design will_also discussed loss followup non SMAR
handful results available tree based survival models investigate method perspective splitting rules log rank test statistics calculated compared splitting rule essentially treating resulting child nodes identically distributed However demonstrate approach affected censoring may lead inconsistency method Based observation develop adaptive concentration bound sense terminal node estimation centers around true within node average underlying survival function quantity may affected censoring distribution show consistency method can achieved splitting rule modified satisfy certain restrictions
recently developed partDSA multivariate method similarly CART utilizes loss functions select partition predictor variables build tree like regression_model given outcome However unlike CART partDSA permits conjunctions predictors elucidating interactions variables well independent contributions partDSA thus permits tremendous flexibility provides ideal foundation developing clinician friendly tool accurate risk prediction stratification interesting extension partDSA aggregate learner Olshen et_al talk describe approach combining predictive ability aggregrate learner clinical utility single tree method illustrated via data analysis cancer patients based various clinical biomarker covariates
Longitudinal monitoring biomarkers often helpful predicting disease poor clinical outcome Much research longitudinal biomarkers overall accuracy predicting outcome However good predictive accuracy may found subgroup population identification subgroups can guide design follow schemes future studies Fetal Growth Study consider prediction large small gestational age births using longitudinal ultrasound measurements attempt identify subgroups women prediction less accurate exist propose tree based approaches identifying subgroups pruning algorithm explicitly incorporates desired type error_rate allowing us control risk false discovery subgroups methods proposed applied data Scandinavian Fetal Growth Study evaluated via simulations
Small area estimation SAE become active area research statistics Many models studied SAE focus one variables interest single survey without paying close attention treatment covariates useful utilize idea borrowing strength covariates build model combines two multiple surveys many real applications also covariates measured errors talk study nested error linear_regression model combining two surveys multiple unit level error free covariates multiple area level covariates subject measurement_error particular derive empirical best predictor small_area mean corresponding mean_squared prediction error estimation performance proposed approach studied simulation_studies also real application
Follow nonrespondents business surveys time resource intensive activity Given decline response rates non response follow takes importance ensure continued quality estimates produced Given fixed budget follow non responding units best way select units non response follow business surveys non respondents followed just sample sample followed selected sample selected using simple random sampling SRS stratified SRS probability proportional size PPS sampling questions addressed two ways Firstly simulation_study compared Monte_Carlo biases mean square error using data existing survey Secondly exact follow sample_size follow using simple random sampling assuming uniform response rates developed
Variance estimation presents challenges context complex survey designs Conventional variance estimators rely second order inclusion probabilities can difficult compute sampling designs Based procedure proposed Ohlsson context sequential Poisson sampling suggest simplified variance estimator requires first order inclusion probabilities idea approximate estimation strategy consists sampling design point estimator equivalent strategy based Poisson sampling Proportional size sampling designs cluster designs will discussed Results simulation_study will presented
study Pareto optimal reinsurance policies insurer s reinsurer s risks returns considered assume insurer reinsurer measure risks distortion risk_measures possibly different distortion operators addition reinsurance premium determined distortion risk_measure analysis suppose reinsurance policy feasible resulting risk party pre determined values Methodologically show generalized Neyman Pearson method Lagrange multiplier method dynamic control methods can utilized solve optimization problem constraints Special cases parties risks measured Value Risk VaR Tail Value Risk TVaR studied great details numerical example provided illustrate practical implications results
talk first discuss conventional two step method constructing Global Minimum Variance Portfolio GMVP noted vast portfolio allocation problem conventional method fails give us portfolio minimum true risk due severe estimation error sample covariance_matrix show set conditions true covariance_matrix possible reduce true portfolio risk replacing sample covariance_matrix modified covariance_matrix constructed shrunk dominant eigenvalue first step Simulation empirical results used illustrate effect shrinkage method
World Health Organization many countries built pharmacovigilance databases detect potential adverse effects due marketed drugs Although number methods developed early detection adverse drug effects vast majority consider multiplicity arising testing thousands drug adverse event combinations first derive optimal statistic maximize power detection maintaining desired error_rate propose nonparametric empirical Bayes method estimate optimal statistic demonstrate superior performance simulation Finally proposed_method applied pharmacovigilance database United Kingdom
introduce mixture model clustering high_dimensional longitudinal data data containing measurements taken large number time points model uses extension mixture common factor analyzers model allow variability measurements explained measurements taken smaller number time points modified Cholesky decomposition latent covariance_matrix utilized take account longitudinal nature data
purpose work develop robust discriminant analysis procedures making use mixture_models work motivated recent model based clustering work includes work mixtures multivariate t distributions mixtures multivariate power exponential distributions mixtures contaminated Gaussian distributions Parameter estimation carried using expectation maximization algorithm number components per class selected using Bayesian information criterion proposed approaches illustrated via simulated well real_data
talk concerns estimation longitudinal linear mixed effects model bivariate responses mixed continuous binary variables presence measurement errors covariates consider continuous covariates well categorical covariates misclassification three categories simulation_studies investigate impact measurement errors covariates naive estimator parameter model parameters fixed
presence clustering data_sets can impede statistical_inference especially small clusters observations address complications clustered data research focused adjustments conventional distribution free approaches lieu likelihood based inference known sensitive small samples However advances likelihood theory offer alternative solution clustering problem via higher order approximations accurate can attractive working small samples demonstrate higher order likelihood methods perform comparably well current distribution free approaches terms attaining appropriate coverage can preferable terms power illustrative examples using Tennessee Student Teacher Achievement Ratio STAR dataset provided context comparison higher order likelihood distribution free approaches
Survival data often limited sample_size subject length biased sampling limited specific time age range Information sample similar cohorts observations longer observation time may obtained literature However may able access source data may trouble combining two samples introduce way incorporate information auxiliary sample creating pseudo datasets match form conditions observed sample eliminating length biased sampling problem improving efficiency will illustrate method real dataset
analysis past developments processes dynamic covariates useful understand present future processes generating recurrent events study consider two important features processes dynamic models features related monotonic trends clustering events recurrent_event data common medical studies discuss certain issues estimation features dynamic models based event counts unexplained heterogeneity present data Furthermore show violation strong assumption independent gap times may introduce substantial bias estimation features models based event counts address issues therefore apply copula based estimation method gap time models approach rely strong independent gap time assumption provides valid estimation model parameters
Analysis left truncated right censored LTRC survival data received extensive interest Many inference methods developed various survival models including Cox model additive hazards model methods however break many applications survival data error contaminated Although error prone survival data commonly arise practice little work available literature handling left truncated right censored survival data measurement_error paper explore important problem Cox model develop valid inference methods derive kernel smoothing estimators survival model parameters establish asymptotic results proposed estimators assess performance proposed methods using simulation_studies methods broad scope application including handling length biased sampling data
many scientific applications biological studies predictors covariates usually high_dimensional naturally grouped research consider Andersen Gill regression_model analysis recurrent_event data high_dimensional group covariates order study effects covariates occurrence recurrent events hierarchically penalized group selection method introduced address group selection problem Andersen Gill model also consider adaptive hierarchically penalized method selecting covariates efficiently especially identifying important covariates important groups asymptotic oracle properties methods investigated simulation_studies show proposed methods perform well selecting important groups important individual covariates groups simultaneously illustrate methods using real life data_sets medicine
Family based studies receiving renewed attention ability identify genetic susceptibility factors associated rare diseases studies power detect rare variants require smaller sample sizes can accurately detect sequencing errors case control studies However garnering enough families analysis rare disease require years effort making studies difficult replicate address shortcoming created R_package SimRVPedigree randomly simulate families ascertained contain multiple relatives affected rare disease package aims mimic process family development allowing users incorporate multiple facets family ascertainment illustrate approximate Bayesian computation SimRVPedigree may used estimate relative risk disease genetic cases sample ascertained families
Many methods proposed detect disease association sequence variants candidate genomic regions However literature lacks comparison methods terms ability localize fine map causal risk variants lying within candidate region extend previous comparison detection abilities methods comparison localization abilities contrast previous work cases controls sampled diploid e two parent rather haploid one parent population simulated sequencing datasets million base pair candidate genomic region cases controls Risk variants middle subregion present case study one simulated dataset illustrate methods describe simulation results score method best localizes risk subregion results lend support potential genealogy based methods genetic fine mapping disease
Existing allele based association tests examining association genetic markers complex human traits sensitive assumption Hardy Weinberg equilibrium HWE limited binary outcomes propose new regression framework individual allele response_variable show score test statistic derived regression_model contains correction factor explicitly adjusts departure HWE thus maintains type error control presence HW disequilibrium HWD absence HWD proposed_method comparable power genotype based association tests Using regression framework can study quantitative traits simultaneously test HWE assumption well handle complex data including correlated individuals families multiple populations uncertain genotype observations support analytical findings using evidence simulation application studies
Gene set enrichment methods aim discover gene sets associated phenotypes Although progress developing proper methods analysis high_dimensional microarray datasets cross sectional studies methods dealing longitudinal phenotypes still limited two step self contained gene set analysis method developed handle multiple longitudinal outcomes Analysis within subject variation first step followed examining subject variation utilizing Linear Combination Test LCT second step method also applicable analysis time course microarray data performance method evaluated simulation_study proposed_method efficient controlling Type error works well small sample_size large number genes missing_data
genetic studies many neurological psychological disorders quantitative phenotypic traits latent inferred ordinal assessment scores diagnostic questionnaires cases accurate representation dependence_structure multivariate ordinal data essential correct identification latent traits use genetic mapping work provides detailed comparison dependence measures multivariate ordinal data investigates robustness polychoric correlation estimation settings asymmetric dependence patterns varying degrees skewness marginal distributions alternative strategy quantify latent traits proposed using factor copula models compare performances approaches factor scores regression testing genetic associations
Recent technical advances genomics led abundance high_dimensional correlated data Dimension reduction methods typically rely matrix decompositions e g SVD EVD compute quantities needed analysis However high_dimensional setting decompositions must adapted cope singularity matrices involved illustrate can done context particular dimension reduction method Principal Component Explained Variance PCEV PCEV seeks linear combination outcomes maximising proportion variance explained covariates interest Using random matrix theory propose heuristic provides fast way compute valid p values test significance decomposition compare power approach common approaches high_dimensional data Finally illustrate method using methylation data collected small number individuals
study risk_measure derived ruin theory defined amount capital needed cope expectation first occurrence ruin event Specifically within compound Poisson model investigate properties risk_measure respect stochastic ordering claim severities Particular situations combining risks yield diversification benefits identified Closed form expressions upper bounds also provided certain claim severities extensions explored
paper show risk_measures can embedded within framework credibility theory specifically introduce new type risk_measures credible risk_measures order capture risk individual contract financial portfolio well industry risk consisting several similar identical contracts financial portfolios consider classical case well regression case Examples given based Fama French data
integrate single optimization problem risk_measure either arbitrage free real market quotations financial pricing rules generated arbitrage free stochastic pricing model call good deal GD sequence investment strategies couple expected return risk diverges infinity infinity existence sequence equivalent existence alternative sequence couple risk price goes infinity infinity Moreover appropriately adding riskless asset every GD may generate new one composed strategies priced show GDs exist practice study measure good deal size also provide minimum relative per dollar price modification prevents existence GDs crucial tool detect priced securities marketed claims Many classical actuarial financial optimization problems can generate wrong solutions used market quotations stochastic pricing models prevent existence GDs illustrate show GD indices help overcome caveat Numerical illustrations given
statistics education community increasingly focusing use simulation based methods including bootstrapping permutation tests illustrate core concepts statistical_inference within context overall research process new focus presents opportunity address documented shortcomings introductory intermediate level statistics courses talk_will discuss motivation rationale behind simulation based approach share concrete examples approach works can integrated existing courses present research evidence effectiveness impacting students conceptual understanding attitudes post course months following courses completion share wealth instructional resources available support instructors trying using approaches
use simulation based methods teaching statistical_inference received considerable attention recent years objective presentation share experience incorporating simulation based methods inference introductory_statistics course University Lethbridge change made first time academic year will_discuss motivations change consequences arose change will address available resources highlight student performance feedback pertains change discuss lessons learned next time
Misclassification frequent administrative data Using simulations compared two Bayesian methods reducing outcome misclassification bias logistic_regression Sensitivity specificity priors based external information method internal complementary data method Bias credible intervals CI coverage mean_squared error MSE used assess performance methods methods yielded estimates less bias na ve analysis scenarios explored Method performed well reasonable sensitivity specificity priors performance decreased decreased mean prior sensitivity CI coverage high methods Bayesian external validation method practical useful reduce outcome misclassification bias logistic_regression internal method may provide better adjustment disadvantage requiring additional individual data
Routinely collected health data RCD defined data collected without specific priori research questions developed prior utilization research Canada widely used population based RCD health administrative data examples include disease registries public health reporting electronic health records increased use RCD increased awareness methodological concerns REporting studies Conducted using Observational Routinely collected Data RECORD statement developed address limitations help meet requirement clear reporting research using RCD RECORD requires description validation studies codes algorithms used select population describe outcomes confounders effect modifiers will emphasize importance algorithm code validation prior embarking research using RCD present validation methods literature will encourage discussion new innovative methods validate administrative data use health research
Chronic disease surveillance information dependent quality EMR data quality case identification algorithms Data obtained Canadian Primary Care Sentinel Surveillance Network chart review conducted presence chronic conditions primary care patients results review will used training data classification models Features will selected billing codes medication prescriptions laboratory values encounter diagnoses health problem lists CART C CHAID decision tree algorithms will compared LASSO forward stepwise logistic_regression terms case definition development bootstrap validation technique will used select optimal complexity parameter value Validity measures will determined using fold cross validation RESULTS shown methods comparable committee created case definitions terms predictive accuracy
cluster randomized trial important tool study interventions group level interventions contamination may concern Hospital level program interventions particularly appropriate design Cluster randomized trials can however expensive time consuming trials often involve collection substantial data observational analyses follow data can provide useful information will_discuss analytic challenges related observational analyses cluster trials challenges include methods account clustering particular clustering affects missing_data measurement_error appropriate statement research question identification correct comparisons address question will illustrate problems analyses PROBIT study breastfeeding promotion intervention
Randomized trials may feature outcome measured longitudinally part usual care reduces trial costs special study visits required allowing exploration longer term outcomes However since timing visits specified protocol can lead outcome observation times varying among subjects potentially related outcomes may result bias unless appropriate analytic methods used talk_will explore two popular classes methods inverse intensity weighting semi parametric joint models can extended account within cluster correlation
Missing data common issue cluster randomized trials CRTs Choosing appropriate strategy handle missing binary outcome CRTs may challenging due great variability design implementation trials According published literature optimal methods handling missing outcome data CRTs infrequently used practice presentation will provide brief overview methods available literature handling missing binary outcomes CRTs illustrate choose implement appropriate missing_data strategy based design characteristics CRTs using real CRT simulated scenarios examples will_also address issues using population averaged cluster specific methods analyzing binary data CRTs missing_data present
Dynamic treatment regimes DTRs formalize personalized medicine tailoring treatment decisions individual patient characteristics G estimation DTR identification targets parameters structural nested mean model blip function optimal DTR derived Despite much work focusing deriving estimation methods little focus extending G estimation case non additive effects non continuous outcomes model selection demonstrate G estimation can widely applied use iteratively reweighted least squares procedures illustrate log linear models derive quasi likelihood_function G estimation within DTR framework show can used form information criterion blip model selection developments demonstrated simulation well application data Sequenced Treatment Alternatives Relieve Depression study
studied Bayesian doubly robust causal inferences framework causal contrast specified terms prediction problem hypothetical randomized experiment inverse probability treatment IPT weights introduced importance sampling weights Monte_Carlo integration posterior distribution produced sampling non parametric model parametric working models introduced smoothing purposes Marginal structural models can estimated similarly maximization parametric utility function However still unclear certain causal estimands treatment_effects among treated structural mean models estimation methods propensity score regression g estimation incorporated Bayesian semi parametric framework will review ideas behind Bayesian doubly robust estimation using IPT weights consider extending aforementioned directions
Intrinsic efficiency multiple robustness desirable properties missing_data analysis establish estimating mean response end longitudinal study drop idea calibrate estimated missingness probability visit using data past visits consider one working model missingness probability multiple working models data distribution Intrinsic efficiency guarantees missingness probability correctly modelled multiple data distribution models combined data prior end study optimally accommodated maximize efficiency Multiple robustness ensures estimation consistency missingness probability model misspecified one data distribution model correct proposed estimators convex combinations observed responses thus always fall within parameter space
will present glimpse Donsker type weak invariance principles established years self normalized partial sums processes domain attraction stable law index number interval Special attention will given case index number e DAN domain attraction normal law applications change point analysis domain summands possibly infinite variance Strassen type strong functional laws invariance principles DAN will_also explored latter case
will revisit Donsker type functional central limit theorems self normalized processes based independent random variables will_also present statistical applications theorems including ones linear errors variables regression models construction asymptotic confidence_intervals population mean modifications generalizations theorems will outlined
talk address applications self normalized partial sums randomly weighted data increasing accuracy CLT based confidence_intervals ii drawing inference based sub samples drawn large data_sets
Bayesian method proposed address misclassification errors independent dependent variables work motivated study women experienced new breast cancers two separate occasions Hormone receptors HRs important breast_cancer biology well recognized measurement HR_status subject errors discordance HR_status two primary breast cancers concern might important reason treatment failure consider logistic_regression model association HR_status two cancers introduce misclassification parameters accounting misclassification HR_status prior distribution sensitivity specificity based HR_status assessed laboratory procedures B spline terms used account nonlinear effect error free covariate findings indicate true concordance rate HR_status two primary cancers greater observed value
Population size estimation critical planning public health programs injection drug users Estimation difficult populations considered hidden hard reach accepted population size estimate Greater Victoria Canada individuals dated prior year likely underestimate will_discuss use closed open mark recapture models estimate population size using cross sectional survey data collected methods provided population size estimates higher currently accepted estimate open population estimate number injection drug users Greater Victoria relatively stable time fewer individuals year study Improved estimates population size dynamics will assist improving access harm reduction services may reduce higher risk drug use practices
become increasingly common biomedical research involve multiple fields e g neuroscience nutrition psychiatry pediatrics etc researchers sharing common overarching goals specific project aims study designs carried different teams Due nature linked often parallel research programs information regarding data structure integration steps essential considering statistical analyses Examples taken ongoing multiple platform studies will presented illustrate statistical applications selected statistical methods quality assurance dimension reduction visualization will discussed pros cons commonly_used statistical approaches multiple platform studies will illustrated real examples
goal research jointly model time spent duration days area burned size hectares ground attack final control fire bivariate survival outcome using either copula model framework connects two outcomes functionally joint modeling framework connects two outcomes shared random_effect challenges include specific framework employed longitudinal environmental variables e g precipitation drought indices incorporated complexity computation associated two outcomes considered jointly key aspect project knowledge translation collaboration fire scientists federal government implement optimal framework developed component fire prediction system concurrently development Natural Resources Canada direct comparison statistical properties two broad frameworks also interest
Redundancy important method improve reliability repairable system study consider clustering failures redundant systems due parallel type carryover effects event intensity recurrent_event process temporarily increased event occurrences processes main goal develop formal test procedures assessment effects redundant systems repairable components connected parallel subject recurrent failures therefore develop partial score tests testing presence parallel carryover effects recurrent_event settings Asymptotic properties test statistics discussed analytically well simulations data_set including failure times diesel power generators operating remote isolated communities analyzed illustrate methods developed
hear time time data surrounds us data scientist career future etc opportunity us statistician entrepreneurial interests need know taking leap talk ll share experiences building startup helps university coaches prepare next opponent leveraging modern statistical techniques play play data ll discuss lessons learned didn t get school product development business strategy user acquisition customer success ll briefly acknowledge successes focusing failures specific product developed marketed statistics product hopefully deriving useful general advice ll conclude useful resources hope provide information wish saw someone speak started endeavour encourages anyone interests starting venture
goal online display advertising entice users convert e take pre defined action making purchase clicking ad important measure value ad probability conversion focus project finding efficient accurate precise estimator conversion probability Challenges associated data delays observing conversions size data_set number observations number predictors Two models previously considered basis estimation logistic_regression model joint model conversion status delay times Fitting former simple ignoring delays conversion leads estimate conversion probability latter realistic computationally expensive fit proposed estimator compromise two estimators apply results data_set Criteo subset clicks occurred two month period along final conversion status
Atomic force microscopy AFM experiments provide nanoscopic measurements dynamic properties molecules atoms Parametric spectral density estimation features prominently analysis calibration experiments typically generate large amounts data end maximum_likelihood estimation can prohibitively expensive much faster least squares estimation method incurs considerable loss statistical precision Moreover methods highly sensitive systematic sources instrument vibration propose variance stabilizing procedure combine simplicity least squares efficiency maximum_likelihood de noising procedure based significance testing spectral periodicities Simulation experimental results indicate two ten fold reduction mean square error can expected applying methodology
Clustering functional data using mixture model based clustering considered Dimension reduction carried via functional PCA principal components clustered using Gaussian mixture model proposed_method uses MM algorithm calculate MLEs mixture model allows algorithm scale efficiently data dimensionality increases work motivated applied problem whose goal identify different physical activity patterns children s movement time movement measured wearable accelerometers measurements recorded every seconds course week
Hard partitional clustering methods Partitioning Around Medoids PAM algorithm give measure individual cluster membership certainties propose two posterior probability like measures individual cluster membership certainty can applied hard partition sample One measure extension individual silhouette widths based pairwise dissimilarities sample apply measures set dissimilarity matrices categorical data PAM algorithm evaluate performance individual ambiguous cluster membership simulated datasets benchmark also present results soft clustering algorithm two model based clustering methods proposed measures behave similarly posterior probabilities soft clustering model based clustering methods worth considering way augment hard partitional clustering methods
spot fire term used wildfire management indicate airborne ember ignited new fire certain meteorologic wildfire conditions spot fire can cross fuel break river road allow wildfire progress otherwise unreachable location model progression wildfire variety conditions associated generation airborne embers can result spot fire derive probability distributions time first spot fire occurring across fuel break wildfire simulator developed present procedures various types practical data estimate model parameters
Mixed models commonly_used analyze spatial data frequently occur practice health sciences life studies customary incorporate spatial random_effects model account spatial variation data particular Poisson mixed_models used analyze spatial count data assume observations area conditional spatial random_effects independent However may valid assumption practice E g multiple asthma visits patient clinics within year clearly independent observations address issue develop spatial models repeated events particular compound Poisson mixed_models introduced account repeated events well spatial variation data Performance proposed approach evaluated simulation_study application dataset patients asthma visits clinics province Manitoba Canada
microbiome collection microorganisms colonizing human body plays integral part human health growing trend microbiome analysis construct network estimate level co occurrence different taxa Current methods facilitate investigation networks change respect phenotype propose model estimate network changes respect binary phenotype counts individual taxa samples modeled Poisson distribution whose mean depends Gaussian random_effect term penalized precision matrix random_effect terms determines co occurrence network among taxa model fit obtained evaluated using Monte_Carlo methods Finally introduce means coding established biological information model performance model evaluated extensive simulation_study tested samples recent gut microbiome studies
Climate change now profound impact life systems scientists preoccupied phenology Usually two sources data used phenology studies first remote sensing data ground based data costly get ground based data new location reason interested building model using remote sensing data predict ground based data one location connecting multiple nearby locations predict ground based data across surface wider range data seasonal variable phase data need registered phase standardized year build model based FDA time warping function gives statistic model linking ground remote data Future work will include spatial aspects idea
objective study identify efficient statistically sound user friendly method analysis Expression quantitative trait loci eQTL studies study performed expression quantitative trait loci eQTL analysis using Matrix eQTL R_package technique implements matrix covariance calculation efficiently runs linear_regression analysis False discovery rate FDR used identify significant cis trans eQTL multiple testing corrections applied matrix eQTL real_data set consisting SNP RNA samples processing data gene SNPs associations can identified using ANOVA model study cis eQTL trans eQTL identified FDR less found matrix eQTL computationally efficient user friendly method analysis eQTL studies results provide insight genomic architecture gene regulation inflammatory bowel disease IBD
Modern multivariate statistical analysis flexible distributional assumptions can applied continuous discrete variables require classical Gaussian assumption popular methods based copulas flexible copula constructions high dimensions based graphical objects called vines Vine pair copula constructions including latent variables can considered Gaussian extensions correlation loading matrix parametrized terms correlations partial correlations algebraically independent will shown Gaussian models parsimonious dependence copula extensions can represented graphical models truncated vine also provides parsimonious dependence model multivariate Gaussian applications Concrete examples will used illustrate main ideas
Multi population mortality modelling plays crucial role securitization longevity risk example payoff Swiss Re s Kortis bond involves mortality improvements U S U K male populations Capturing mortality dependence two populations critical accurate longevity risk pricing Recently researchers investigated use copula modelling mortality dependence observed mortality dependence stronger mortality deteriorations mortality improvements order capture observed asymmetric dependence propose use multivariate regime switching copula study choice copula affects risk profile longevity security examine impact asymmetric dependence regime switching pricing security
talk propose hierarchical credibility approach modeling multi population mortality rates using data Human Mortality Database Hierarchical credibility used premium calculation grouped data different levels tree structure Traditional mortality models single population estimate model parameters mortality data grouped two levels age year group mortality data genders several developed countries four levels country gender age year three levels population age year apply hierarchical credibility level better reflect correlation mortality rates among populations genders countries compare forecasting performances model multi population mortality ones
Google Apple Adobe Intel settled class action lawsuit million suit alleged companies colluded major corporations Silicon Valley suppress employee wages agreeing poach workers cartel members build niche theory dynamic context analysis network manager job changes period cartel active investigate two related questions First cartel affect managerial labour flows second cartel affect firms competing talent labour market use temporal exponential graph model TERGM examine changes manager job transitions cartel s activities find firms part cartel lost relatively workers expected niches overlapped higher proportion cartel member firms work suggests paths future labour market policy making
introduce generative model based training deep architectures model consists K networks trained together learn underlying distribution given data_set process starts dividing input data K clusters feeding separate network use EM like algorithm train networks together update clusters data call model Mixture Networks provided model platform can used deep structure trained conventional objective function distribution modeling components model neural networks high capability characterizing complicated data distributions well clustering data apply algorithm MNIST handwritten digits Yale faces datasets model can learn distribution data_sets One can sample new data points distributions look like real handwritten digit real face also demonstrate clustering ability model using real world toy examples
Technology companies especially consumer facing ones seen explosion use experiments Many companies now consider statistical hypothesis testing AB testing appropriate solution many questions Outside well defined examples however lack statistical knowledge hampers efforts real world practitioners purpose talk document experiments explain failed will_also consider steps researchers academics practitioners can take avoid failures future
Students know adage Correlation Causation yet research shows even completing introductory_statistics course can fooled making causal conclusions relationships surface appear plausible perform poorly assessments purpose randomization study design Moreover research questions causal yet data science problems rely observational data Unfortunately research statistics education literature focused largely sample population inference rather experiment causation inference will talk problems observed may inhibiting students ability make appropriate causal inferences including ambiguity language lack facility multivariate thinking will consider might first often last non mathematical course statistics give students deeper practical understanding causal inferences
important decisions make individuals collectively based perceptions answers causal questions cacophony conflicting claims effects diets drugs social economic policies largely result fact causal questions observational evidence statisticians unique appreciation issues involved causal_inference can help improve public understanding causality helping students service courses develop judgment assess causal claims talk_will discuss attempts achieve
report ergodicity approach approximate conditional causation probabilities conditional cause failure probabilities via finite Markov chains significant provide additional important information failure distribution system especially provide information reliability aged system Assisted fast algorithm demonstrate readers obtain probabilities verify theoretical derivations paper interest broad range audiences including academic researchers reliability system engineers work continuation entitled Distributions causation probabilities multiple run rules applications system reliability quality control start tests already accepted publication
Control charts effective tools signal detection manufacturing processes service processes Much data service industries come processes exhibiting non normal unknown distributions commonly_used Shewhart variable control charts depend heavily normality assumption appropriately used paper thus proposes DS EWMA AV chart monitoring process variability explore sampling properties new monitoring statistics calculate average run lengths using proposed DS EWMA AV chart performance DS EWMA AV chart non parametric variance charts compared considering cases critical quality characteristic presents normal non normal distribution Comparison results show proposed chart always outperforms latter
flexible semi parametric regression_model autocorrelated count data proposed Unlike earlier models available literature model require construction likelihood_function entails specification first two conditional moments simple flexible combined estimating function approach makes efficient use information contained data adopted model absence likelihood_function Simulation_studies conducted study performance semi parametric method likelihood either correctly specified misspecified methodology illustrated using monthly polio counts dataset
Statistical methods adapt unknown population structures attractive due numerical theoretical advantages non adaptive counterparts contribute adaptive modelling functional regression challenges arise infinite dimensionality underlying spaces interested scenario functional predictor process lies nonlinear manifold intrinsically finite dimensional embedded infinite dimensional function space proposed novel approach built upon local linear manifold smoothing achieves polynomial convergence rate adapts contamination level intrinsic manifold dimension even functional data observed intermittently contaminated noise contrast logarithm rate nonparametric functional regression literature demonstrate proposal enjoys favourable finite sample performance relative commonly_used methods via simulated real examples
Functional principal component analysis FPCA popular approach functional data analysis explore major sources variation sample random curves major sources variation represented functional principal components FPCs existing FPCA approaches use set flexible basis functions B spline basis represent FPCs control smoothness FPCs adding roughness penalties However flexible representations pose difficulties interpretation consider variety applications FPCA find many situations shapes top FPCs simple enough approximated using simple parametric functions propose parametric approach estimate top FPCs enhance interpretability users parametric approach can circumvent smoothing parameter selecting process conventional nonparametric FPCAs simulation_study shows proposed parametric FPCA robust outlier curves exist parametric FPCA demonstrated several datasets variety settings
aim develop spatial quantile_regression framework accurately quantifying high_dimensional image data conditional scalar predictors new framework allows us delineate spatial quantile association neuroimaging data covariates explicitly modeling spatial dependence neuroimaging data Theoretically establish minimax rates convergence prediction risk fixed random designs develop efficient algorithms ADMM primal dual algorithm estimate varying coefficients method able estimate whole conditional distribution image response given scalar covariates Simulations real_data analysis conducted examine finite sample performance
Recurrent events data arises many biomedical longitudinal studies failure events can occur repeatedly subject follow time interested gap times recurrent events propose semiparametric accelerated transform gap time model based trend renewal process contains trend renewal component use Buckley James imputation approach deal censored transform gap times proposed estimators shown consistent asymptotically normal Model diagnostic plots residuals prediction method predicting number recurrent events given specified covariate follow time also presented Simulation_studies conducted assess finite sample performances proposed_method proposed technique demonstrated application two real_data sets
talk new class length biased Weibull mixtures will introduced review basic distributional properties will given generalization Erlang mixture distribution length biased Weibull mixture distribution increased flexibility fit various shapes data distributions including heavy tailed ones Methods statistical estimation model selection will discussed applications real catastrophic loss data_sets
develop multi parameter regression survival models interval censored survival data setting means multi parameter Weibull regression survival model wholly parametric non proportional hazards describe model develop interval censored likelihood without frailty involved conduct detailed simulation_study designed investigate effect sample_size proportion right censored observations analyse data population based study survival lung cancer conducted UK compare findings obtained analysis data using right censored multi parameter regression survival methods best knowledge first time effect interval censoring investigated MPR survival model setting
grouped relative risk model GRRM popular semi parametric model analyzing discrete survival time data maximum_likelihood estimators MLEs regression coefficients model often asymptotically efficient relative based restrictive parametric model However settings small number sampling units usual properties MLEs assured talk discuss computational issues can arise fitting GRRM small samples describe conditions MLEs can ill behaved find overall estimators based penalized score function behave substantially better MLEs setting particular can far efficient
Proportional hazard PH models can formulated without assuming distribution survival times former assumption leads parametric models whereas latter leads semi parametric Cox model far popular survival analysis However parametric model may lead efficient estimates Cox model certain conditions parametric models closed PH assumption common Weibull accommodates monotone hazard functions propose three parameter distribution closed PH assumption model flexible parsimonious sense accommodates four basic shapes hazard function increasing decreasing unimodal bathtub shape small cost estimating one additional parameter compared Weibull PH model model can used analyzing time event data recurrent_event data joint modeling time event longitudinal data Comparative studies based real simulated_data reveal model can valuable adequately describing different_types time event data
Survey data often collected complex sampling design finite population parameters typically defined solution called population census estimating_equations paper address inferential problems population estimating_equations also involve unknown nuisance function depend parameters Survey weighted estimating_equations semiparametric plug component constructed empirical_likelihood based inferences Maximum empirical_likelihood estimators introduced together set sufficient conditions ensure root n consistency asymptotic normality estimators Effects using plug estimator nuisance function empirical_likelihood inference finite population parameters examined proposed_method applicable wide range problems including generalized Lorenz curve Gini measure income inequalities Numerical results based simulated real_data provided
production official statistics can considerably hindered errors collected data process correcting errors referred statistical data editing traditionally expensive time consuming manual effort However becoming increasingly accepted practice correction small subset influential errors often sufficient guarantee good quality estimate therefore additional correction data past certain threshold yields minor improvements quality concept selective editing involves identifying significant errors estimating impact final estimate providing signal halt editing operations error tolerance reached work compare several methods based predicted values estimating reporting errors prioritizing units failed edit follow context Integrated Business Statistics Program Statistics Canada
Balanced sampling sampling method totals estimated Horvitz Thompson estimator close true population totals given set auxiliary variables survey frame auxiliary variables highly correlated variables interest variances estimators totals will small Statistics Canada s Job Vacancy Wage Survey JVWS quarterly sample business locations split three monthly subsamples data collection parallel locations enterprise must collected month presentation will show one can use balanced sampling method allocate sampling units enterprise level way keeps number employees number locations balanced province industry months allocation strategy implemented JVWS survey using Cube method proposed Deville Till
Statistical agencies facing challenge declining response rates increased demand national statistics must find innovative ways improve collection strategies maintaining high quality data One innovation use characteristics associated survey non response order segment population homogenous clusters Advertising campaigns communication tools can tailored specific clusters population emphasizing areas higher non response talk combine data various household surveys sociodemographic population characteristics Canadian Census order profile survey non response Logistic regressions identify characteristics associated survey non response cluster analysis creates clusters geographic areas similar prevalence characteristics associated non response
Reverse Record Check RRC Survey used estimate number scope persons missed Census Population RRC one important challenging surveys done Statistics Canada Successfully tracing interviewing persons selected RRC sample one biggest challenges even though several sources administrative data used provide information find persons RRC collection operations faced quite challenges survey including redesign survey contents multi mode collection methodology new Computer Assisted Telephone Interview CATI application new tracing application article describes RRC collection challenges strategies
Currently within household selection electronic questionnaire surveys consists sending paper invitation random sample households asking online roster occupants random selection person made within electronic questionnaire household one occupant might require two people connect application also risk second person want answer creates non response even first person completed part challenges roster method raised questions selected person instead determined instructions mailed invitation clear enough people know selected Will follow instructions answer questions last birthday method new version age order method tested compared roster method Response rates selection inaccuracy rates analyzed results will presented
medical studies different_types outcomes frequently collected time many subjects example CD cell counts viral loads patients usually observed longitudinally human immunodeficiency virus HIV research Therefore effectiveness treatments interventions can profiled based bivariate responses accounting association nature relationships two outcomes particular interest two outcomes analyzed jointly instead separately study propose model data mixed types jointly incorporating subject specific temporally dependent random_effects Tweedie models different index parameters optimal estimation model developed using orthodox best linear unbiased predictor random_effects analysis results rely distributional assumption random_effects
talk presents efficient algorithms computing projection median exactly R projection median first introduced Durocher Kirkpatrick robust non parametric descriptor multivariate location Computing projection median R d involves averaging projections infinite directions d dimensional unit sphere Previously approximation algorithms existed computing projection median d algorithm begins transforming problem computing average infinite directions computing median level arrangement finite set hyperplanes Next randomized algorithm Agarwal et_al calculating median level arrangement planes applied transforming solution back original setting Exact efficient computation opens door practical use statistic higher dimensions well study properties projection median
Multivariate modeling financial assets often plays critical role portfolio analysis risk management Stochastic Volatility SV models enjoyed enormous success univariate setting multivariate SV models challenging design implement owing large amount latent variables need correlations satisfy positive definiteness constraint propose common factor multivariate SV model naturally extending univariate case interpretable hierarchical correlation structure positive definiteness guaranteed show common factor can flexibly proxied observable volatilities resulting enormous computational savings parameter inference model used conduct multivariate SV analysis several major components S P
Stationary Gaussian processes popular models many areas statistical applications observations regularly spaced structure variance matrix admits fast likelihood evaluations via famous Durbin Levinson algorithm scaling O N number observations instead usual O N adapt lesser known Generalized Schur algorithm reduce cost O N log N superfast method extends gradient hessian calculations thus applicable many inference algorithms Bayesian Frequentist alike implementation available via R_package SuperGauss beating Durbin Levinson around N present application Gaussian process factor analysis
Diseases now thought result changes entire biological networks whose states affected complex interaction genetic environmental factors general power estimate interactions low number possible interactions enormous effects may non linear Existing approaches lasso might keep interaction remove main effect problematic interpretation develop model linear non linear interactions penalized regression models automatically enforces strong heredity property computationally efficient fitting algorithm combined non parametric screening approach scales high_dimensional datasets implemented R_package apply method identify gene prenatal maternal depression interactions negative emotionality mother infant dyads Maternal Adversity Vulnerability Neurodevelopment MAVAN cohort
Computational neuroscience studies brain information processing point view strong body literature suggests nervous system fundamental stochasticity making statistical_models attractive computational neuroscience said adage models wrong useful applies Among useful collection data driven models tailored understand neurophysiological phenomena process information coding brain talk reviews briefly common statistical methods recent advancements analysis neural spike trains Data visualization computational challenges biological justification performance models will_also discussed addition exciting opportunities future research
Malaria can diagnosed presence parasites symptoms usually fever However individual may fever attributable either malaria causes parasite level individual fever follows two component mixture distribution parasite levels nonmalaria individuals exactly zero distribution parasite levels nonmalaria individuals also follows mixture distribution propose maximum multinomial likelihood approach estimating proportion clinical malaria using parasite level data two groups individuals first collected endemic area whereas second community parasite levels nonmalaria individuals develop EM algorithm show convergence locally Simulations show proposed estimator efficient existing nonparametric estimators using frequencies zero nonzero data proposed_method used analyze data malaria survey carried Tanzania
systematic survey aimed review simulation_studies discrete choice experiments DCEs determine design features affect statistical efficiency Electronic searches conducted JSTOR Science Direct PubMed OVID Screening data extraction performed independently duplicate reporting quality simulation_studies also evaluated study potentially relevant studies proved eligible Studies showed statistical efficiency improved increasing number choice tasks alternatives decreasing number attributes attribute levels overlaps incorporating response behaviour heterogeneity correctly specifying Bayesian priors minimizing prior variances matching method research question Studies need improve reporting study objectives failures random number generators starting seeds software results may help inform investigators DCE design creation
recent years class complex statistical_models known individual level models ILMs effectively used model infectious_disease transmission discrete time models well developed assume probability disease transmission two individuals depends spatial network based separation spatial locations However spatially varying demographic environmental factors influence disease transmission Thus might useful incorporate effect spatial location ILMs study extend ILMs Geo dependent ILMs GD ILMs allow evaluation effect spatially varying social risk factors environmental factors well unobserved spatial structure upon transmission infectious_disease consider conditional autoregressive CAR model capture effects unobserved spatially structured latent covariates measurement_error show GD ILMs can fitted data within Bayesian statistical framework using Markov_chain Monte_Carlo MCMC methods
Influenza infectious_disease periodic patterns commonly modeled focusing yearly cycle However considering four pandemics occurred past century poses question whether hidden patterns Using times series analysis multitaper spectral analysis techniques investigation mortality due Influenza occurred United States shown interesting features Periodic signals one year period found significant well signals low frequency range near years analysis also revealed significant frequencies cycles per year possible frequencies biological connection influenza artifact created fact yearly cycle absolutely identical year follows might complex periodic structure influenza mortality data
Health utilities represent single global measure Health Related Quality Life necessary economic evaluation health interventions data distribution semi continuous skewed leptokurtic upper bound probability mass bound Linear regression appropriate analysis data since many assumptions linearity homoscedasticity normality violated investigate benefits applying mathematical transformation response_variable fitting linear_regression model use simulated real health utility data compare number transformations untransformed model using several measures model accuracy goodness fit investigation identifies benefits transformations context offering suggestions future analysis modeling health utilities
consider competing risks progressively type II censored sample competing risks follow linear exponential law develop likelihood inference known number competing risks demonstrate performance maximum_likelihood estimates consider case two competing risks carry extensive simulation_study also apply inferential method real_data set
High dimensional data collected daily many fields work DNA analysis social media data analysis presence clusters data_set can crucial discovering new trends patterns e g disease subtypes Therefore necessary reliable quick techniques can detect define clusters Model based clustering popular method uses mixture distributions component density corresponds cluster common model based clustering techniques based using mixture multivariate normal distributions method called high_dimensional data clustering HDDC given rise computationally efficient family Gaussian mixture_models high_dimensional data HDDC based idea high_dimensional data can represented much lower dimensional subspaces HDDC family models gained vast attention due superior performance compared families mixture_models propose t analogue family call tHDDC family tHDDC family extends high_dimensional data clustering models include multivariate t distribution
stop signal reaction time SSRT measure latency stop signal process theoretically formulated using horse race model go stop signal processes American scientist Gordon Logan Current analysis general population sample children age Toronto demonstrated significantly higher probability successful inhibit stop trials following stop trial compared following go trial suggesting need modified weighted mixture approach estimate SSRT Results showed Significantly reduced go response times stop trials versus correct go trials mean go response times assumed follow Ex Gaussian distribution Mean Difference ms p value ii Significantly larger estimates SSRT using pooled estimate weighted trial type versus original Logan method Ex Gaussian distribution assumption Mean Difference ms p value
automated valuation model AVM mathematical program estimate market value real estates based real estate data Currently boosting popular predictive approach AVM Geoffrey Hinton ignited popularity neural networks showing substantially better performance deep neural network However application deep learning algorithm fully explored AVM market paper investigated advanced statistical learning approaches AVM linear_regression model reference study investigated accuracy advanced statistical learning approaches boosting random forest support vector regression models real estate data Moreover constructing deep belief networks DBN regression considered application ensemble approach integrate deep learning method statistical learning methods AVM also investigated research
Type X colorectal cancer CRC occurs individuals approximately age typically least two affected relatives major gene found far explain clustering CRC families will investigate several genetic models familial Type X CRC phenotype carrying complex segregation analysis Type X pedigree data will provide evidence favour major gene polygenic environmental component Furthermore will incorporate polygenic threshold model genetic risk model estimate probability individual carrier disease causing genetic component predict individuals family history Type X CRC will increased probability carrying genetic component thus higher chance diagnosed colorectal cancer Ultimately research will applications genetic counselling cancer prevention strategies screening guidelines
paper propose novel look CDC Centers Disease Control Prevention USA surveillance data yearly influenza outbreaks First apply flexible model based non homogeneous birth death process allows estimate lower bound basic reproduction ratio indicator transmission potential epidemic explore various ways comparing data across years regions resorting conditional model allows circumvent drawbacks original data particular apply standard procedures mixture modeling order assess whether two outbreaks similar example attain peak time
Infectious disease assays can imperfect estimating disease prevalence imperfection accounted incorporating assay sensitivity specificity point variance estimates Unfortunately accuracy measures often treated fixed constants rather taking account actually estimates assay validation process purpose presentation show detrimental effect taking account sampling variability samples obtained group testing k pooled testing show confidence interval coverage can dramatically decline sample_size increases main sample interest remedy problem propose new confidence interval takes account extra sampling variability new interval shown obtain coverage near nominal level
Experimental observational studies shown bumblebees return faithfully particular forage locations can significant effect pollination services provide research project develops stochastic models bumblebees without spatial memory investigate resulting pollination services topics memory movement models pollination services previously researched separately yet combined work movement modelled combination correlated uncorrelated random walks agent based model inspired partial differential equations informed recent bumblebee radar tracking studies Pollination services quantified flower visitation rates tracked bumblebee moves throughout landscape results work will inform future modelling work landscape requirements optimal pollination blueberry crops bumblebees
Low density lipoprotein LDL dyslipidemia LDL geq mmol L one leading factors contributing cardiovascular diseases Previous studies suggest regional variations LDL dyslipidemia however random_effect due geography cause overdispersion binary responses investigate variability spatial random_effect factors impact binary response fitted spatial mixed model LDL dyslipidemia binary response individuals local regions n Newfoundland Labrador NL Using data Laboratory Information System among adults LDL test N LDL dyslipidemia Using method moments assessing variability random component responses individuals given geographical region correlated common random_effect shared individuals Spatial mixed model useful obtaining consistent efficient estimates parameters model
additive structure multivariate GARCH type model proposed describe dynamic common driven process stocks indices observable sequence divided two parts common risk term individual risk term following GARCH type structure conditional volatilities stocks can increase dramatically together sudden peak common volatility provide sufficient conditions strict stationarity ergodicity model parameters model identifiable terms conditional second moments mild assumptions general assumptions proposed without specifying distribution innovation different initial values lead estimates asymptotically Sufficient conditions strong consistency asymptotic normality quasi maximum_likelihood estimator QMLE proposed
common practice students towards assessments quizzes exams look grade file away case learning potential assessments fully reached Assessment corrections allow students second chance improve performance protect GPA thus reducing D Fail Withdraw DFW rates actually encourage students identify knowledge gaps try fill gaps expected corrections students gain deeper understanding course materials also accumulate confidence learning skills main goal poster presentation disseminate research findings scholarship teaching learning SoTL project aimed examine impact midterm corrections student learning large service course setup
Various approaches discriminant analysis longitudinal data investigated focus model based approaches latter typically based modified Cholesky decomposition covariance_matrix Gaussian mixture however non Gaussian mixtures also considered applicable Bayesian information criterion used select number components per class various approaches demonstrated real simulated_data
Von Bertalanffy growth function VoB specifies length fish function age However practice age measured error study structural errors variables SEV approach account ageing error Recent research proposed approach fish growth data assumed distribution true unobserved age Gamma distribution showed particular SEV approach provided improved regression parameter inferences compared standard nonlinear estimation approach account covariate measurement_error ME presentation investigate whether SEV VoB parameter estimators robust misspecification gamma true age distribution robust mean large sample bias outline numerical method evaluating large sample bias SEV VoB parameter estimators simulation results demonstrate SEV VoB using gamma distribution true age robust ME age small misspecification bias low However large ME bias estimators may large
performing statistical_inference regression_model parameters need know whether parameters interest estimable identifiable model said identifiable unknown parameters model can estimated uniquely provided data consider different interaction models without measurement_error will look different measurement_error models Berkson classic models apply remedies non identifiability instrumental variables well replicated validated data
exists need development models able account discreteness data along time_series properties correlation review application thinning operators adapt ARMA recursion integer valued case first discussed class integer valued ARMA INARMA models arises application focus falls INteger valued AutoRegressive INAR type models INAR type models can used conjunction existing model based clustering techniques cluster discrete valued time_series data approach illustrated addition autocorrelations use finite mixture model several existing techniques selection number clusters estimation using expectation maximization model selection applicable proposed model demonstrated real_data illustrate clustering applications
Surplus production models provide simple analytical methods assessing fish populations taking annual biomass growth rate carrying capacity account However simple models may adequately reflect fish stock dynamics can substantially complex age length specific birth growth death processes play account process errors can included production model state space modelling framework paper compare sensitivity estimators state space conventional nonlinear production models without process errors using local influence analysis method introduced R D Cook apply diagnostics different fish stocks assess estimated parameters respond small perturbations data diagnostics reveal conventional model parameter estimators sensitive small changes input data compared state space model estimators
analysis existence form time trends repairable systems important issue reliability studies Hence many trend tests proposed studied literature recent interest developing robust trend tests based robust estimating function approach tests appealing powerful range settings study consider monotone time trends recurrent_event data repairable systems develop robust trend test based rate functions power law process main goal discuss power robust trend tests well compare power well known trend tests various settings conduct extensive Monte_Carlo simulations compute compare power tests various cases Finally analyze data_set industry illustrate methodology
Cluster analysis commonly described classification unlabeled observations groups similar one another observations groups Model based clustering assumes data arise statistical mixture model clustering applications common fit several models family report results best model chosen using selection criterion often Bayesian information criterion Recent interest placed selecting subset solutions close best model averaging create weighted averaging clustering results Two averaging approaches explored use Occam s window select models sense close best one one methods posteriori probabilities averaged method models parameters averaged generate single interpretable model efficacy model based averaging approaches demonstrated family skew t mixture_models using real simulated_data
Central designing delivering individual crop insurance program historical individual farm level yields serve foundation setting coverage levels rates However data scarcity credibility particularly lack farm level yield data make difficult calculate individual expected losses result aggregate county level data often used establish base premium rate may contribute adverse selection thus program losses develop new relational model predict farm level crop yield distributions absence farm level yield data improve accuracy computing crop insurance premium rates relational model developed defines similarity measure based Euclidean distance metric select optimal county reference country farm level yield data borrowed empirical analysis shows relational model achieves lower mean standard deviation prediction errors compared benchmark model able recover actual premium rates closely
talk_will introduce flexible new bivariate copulas R_package CopulaOne applications data analytics insurance finance Popular multivariate copulas vine copulas factor copulas constructed based bivariate copulas ideal bivariate copula following features First upper lower tails able explain full range tail dependence dependence tail can range among quadrant tail independence intermediate tail dependence usual tail dependence Second can capture upper lower tail dependence patterns either different talk_will discuss general approach constructing copulas features promising parametric copula families presented ideal features computational speeds considered constructing copulas Finally applications using copulas demonstrated
two sided exit problem subject risk management analysis used better understand dynamic various insurance risk processes two sided exit setting discounted aggregate claims investigated dependent renewal process also known dependent Sparre Andersen risk process Utilizing Lundberg s generalized equation Laplace transform identify fundamental solutions given integral equation will shown play role similar scale matrix spectrally negative Markov additive processes Explicit expressions recursions identified two sided probabilities moments aggregate claims respectively numerical example two sided exit probabilities involving Farlie Gumbel Morgenstern FGM copula provided
Market Risk usually assessed means point estimates risk_measures exposed randomness data misspecification underlying model assumptions paper extent view risk measurement distribution estimators risk_measure forecasts account possible model risk addition analyze degree unexpected economic losses may reduced using information risk_measure estimator distribution combine future market expectations idea behind approach conservative risk_measure forecasts point estimate estimator s distribution used market expectations point times financial turmoil
Current banking regulations require banks use called risk weight function determine amount capital held buffer extraordinary losses function based simple intuitive model developed Vasicek makes several strong simplifying assumptions retains popularity due intuitive appeal workarounds shortcomings caused assumptions proposed literature adopted practice One assumptions correlations vary business cycle stark contrast empirical evidence talk generalize Vasicek model allow correlations vary systematically state economy model includes parameter allows user control degree correlations rise adverse economic scenarios demonstrate parameter profound impact regulatory capital calculations
talk_will present novel methods small_area estimation outcome variable discrete methods developed framework generalized linear mixed_models clustered correlated data will robust slight deviations underlying distributions outcome auxiliary variables empirical properties proposed estimators will studied using Monte_Carlo simulations application will_also provided using actual survey data
Complex statistical_models multiple sources dependencies variability observations primary importance studying data multiple disciplines include spatial temporal spatio temporal various mixed effects statistical_models special importance among models useful studying problems limited directly observed data example small_area models talk present new resampling based method can used simultaneous variable selection inference several complex models including small_area mixed effects models Theoretical results justifying proposed resampling schemes will presented followed simulations real_data examples talk based research involving several students collaborators multiple institutions
National statistical offices mandated produce reliable statistics important characteristics many sub populations small areas defined geography demography Model based methods borrow strength areas variables extensively used generate reliable statistics Standard model based small_area estimates perform poorly presence outliers Sinha Rao proposed robust frequentist approach handle outliers propose robust hierarchical Bayes HB method handle outliers unit level data consider two component scale mixture normal distributions unit error model outliers produce noninformative HB predictors small_area means example extensive simulations convincingly show robustness HB predictors Simulation evaluation two procedures shows superiority M quantile small_area estimators HB procedure enjoys dual Bayes frequentist dominance
Registry based randomized controlled trials RRCTs defined pragmatic trials use registries platform case records data collection randomization follow Recently application RRCTs attracted increasing attention health research address comparative effectiveness research questions real world settings mainly due low cost enhanced generalizability findings rapid consecutive enrollment potential completeness follow reference population compared conventional randomized effectiveness trials However several challenges RRCTs taken consideration including registry data quality ethical issues methodological challenges talk_will summarize advantages challenges areas future research related RRCTs
Randomized controlled trials RCTs broadly categorized either pragmatic explanatory attitude Pragmatic trials designed evaluate effectiveness interventions real life routine practice conditions whereas explanatory trials aim test whether intervention works optimal situations talk_will discuss development Pragmatic Explanatory Continuum Indicator Summary PRECIS classifying trials application registry based trials
Randomized controlled trials RCTs treatments interventions typically described either explanatory pragmatic Meta analysis RCT studies typically pools evidence treatment_effects included studies regardless classification pragmatic explanatory trials Given treatment_effects explanatory trials may greater obtained pragmatic trials conventional meta analytic approaches may accurately account heterogeneity among studies Stratified meta analysis systematically review studies treatment_effects explanatory trials meta analyzed reported separately pragmatic trials generally used systematic reviews study investigate variety meta analytic approaches synthesizing evidence pragmatic explanatory trials Discussions key statistical design considerations pooling evidence types trial designs provided
Latent Gaussian Model LDM used various fields ecology study cancer stock market model usually used high_dimensional data Integrated Laplace Approximation INLA advanced approach inference LDM Compare traditional approach Monte_Carlo Markov Chain INLA much better performance terms running speed error_rate However one main assumption using INLA data come Gaussian Markov random fields GMRF paper present method diagnose normality assumption LDM high dimensions method uses Ordinary Differential Equations ODE Bayesian approach
Human cancer cell line experiments valuable tool investigating drug sensitivity biomarkers number biomarkers measured experiments typically order several thousand whereas number samples often limited one three replicates experimental condition developed innovative Bayesian approach efficiently identifies clusters markers similar patterns define prior distributions cluster parameters allow us obtain biologically meaningful clusters better discriminate Motivated availability ion mobility mass spectrometry data cell line experiments myelodysplastic syndromes acute myeloid leukemia Moon Shots Program MD Anderson Cancer Center methodology can identify protein markers follow biologically meaningful trends
use Bayesian statistical methods handle missing_data biomedical studies become popular recent years paper propose novel Bayesian sensitivity analysis BSA model accounts influences missing outcome data estimation treatment_effects randomized trials non ignorable missing_data implement method using probabilistic programming language STAN apply data Vancouver Home VAH Study randomized control trial provided housing homeless people mental illness compare results BSA existing Bayesian longitudinal model ignores missingness outcome Furthermore demonstrate simulation_study BSA credible intervals greater length higher coverage rate target parameters existing_methods
statistical modelling infectious_disease spread population generally requires use non standard statistical_models primarily due fact infection events depend upon infection status members population hence can assume independence infection events complication added fact often complex heterogeneities population wish account since example populations tend mix homogeneously Sometimes may wish account heterogeneities using spatial mechanisms assume transmission events likely occur individuals close together space individuals apart Sometimes natural model heterogeneities using contact networks represent example sharing supplier companies farms Typically statistical_inference models e g parameter_estimation done Bayesian context using computational techniques Markov_chain Monte_Carlo MCMC examine main characteristics individual level infectious_disease models fit data within Bayesian statistical framework using R_package EpiILM
show locally powerful tests uniformity circle given Beran optimality properties just parametric alternatives described Beran also many non parametric alternatives describe local asymptotic versions optimality
Jaundice meters intended use screening device newborns Meters need validated putting clinical use Transcutaneous bilirubin readings compared clinical laboratory standard serum bilirubin obtained blood samples Meter reliability evaluated regression_model errors variables since error associated jaundice meter method also clinical standard Bayesian approach discussed independent noninformative priors Since reliability varies meter meter number clustering techniques examined develop statistical quality assurance program management multiple meters using reliability index simulation_study carried see robustness developed program various conditions including unbalanced samples size meters ranged outlying observations error ratio factors can controlled practice
Quantile regression wide applications many fields Studies heavy tailed distributions rapidly developed multivariate heavy tailed distributions estimation conditional quantiles high low tails interest numerous applications presentation proposes new weighted quantile_regression method high quantile_regression Monte_Carlo simulations show good efficiency proposed weighted estimator relative regular quantile_regression estimator presentation also investigates real world example using proposed weighted method Comparisons proposed_method existing_methods given
choice model framework regression setting depends nature data focus study changepoint data exhibiting three phases incoming outgoing linear joined curved transition Bent cable regression appealing statistical tool characterize trajectories quantifying nature transition two linear phases modeling transition quadratic phase unknown width demonstrate quadratic function may appropriate adequately describe many changepoint data propose generalization bent cable model relaxing assumption quadratic bend properties generalized model discussed Bayesian approach inference proposed study suggests proposed generalization can produce fit either comparable superior quadratic bent cable model
standard expectation maximization approach parameter_estimation Gaussian mixture_models modified within cycle incorporation nonparametric bootstrap order prevent overfitting model based clustering applications Benefits include reasonable parameter estimates including estimates standard error realistic clustering results increased avoidance local maxima addition find better efficiency versus traditional nonparametric bootstrap approach EM mixture_models Proposals issues arise lack monotonicity convergence discussed method applied real simulated_data
paper study quantile_regression nominated samples Nominated samples wide range applications medical ecological environmental research shown perform better SRS estimating several population parameters propose new objective function takes account ranking information estimate unknown model parameters based maxima minima nomination sampling designs compare mean_squared error proposed quantile_regression estimates using maxima minima nomination sampling design observe provides higher relative efficiency compared counterparts SRS design analyzing upper lower tails distribution response_variable also evaluate performance proposed methods ranking done error
consider estimation problem weighted least absolute deviation WLAD regression parameter vector outliers response leverage points predictors propose pretest shrinkage WLAD estimators parameters may subject certain restrictions derive asymptotic risk pretest shrinkage WLAD estimators show asymptotic risk shrinkage estimator strictly less unrestricted WLAD estimator hand risk pretest WLAD estimator depends validity restrictions Furthermore study WLAD absolute shrinkage selection operator compare relative performance pretest shrinkage WLAD estimators simulation_study conducted evaluate performance proposed estimators relative unrestricted WLAD estimator real life data example used illustrate performance proposed estimators
Nonparametric regression models allowing locally stationary covariates received increasing interest catch dynamic nature regression function adopt flexible structural nonparametric model classical varying coefficient model VC additive model AM named varying coefficient additive model VCAM model propose three step spline estimation method varying coefficient additive component functions show consistency rate convergence Furthermore develop two stage penalty procedure identify varying coefficient additive terms VCAM demonstrate proposed identification procedure consistent e probability approaching one correctly select varying coefficient additive terms respectively Simulation_studies explore finite sample performance validate asymptotic theories Two real_data applications illustrating methodologies presented well
wildfires occur protected area Canada year Approximately fires exceed ha size account suppression costs greatest threat communities Although statistical approaches fire occurrence prediction FOP evolved past years FOP implemented national scale Canada develop big data supervised machine learning based statistical modeling approach predict severe large wildfire occurrences Canada using set spatially gridded meteorological topographic demographic covariates modeling framework focused allowing better preparedness effective decisions share resources among provincial territorial federal agencies enhance Canada s resilience large wildfires talk_will focus large wildfire prediction British Columbia Canada showcase methodology
relationships ecological variables usually estimated fitting statistical_models go conditional means variables example piecewise linear_regression model goes conditional mean response_variable given predictor s used obtain relationships breakpoints variables answer relevant ecological questions contrast piecewise linear quantile_regression model goes quantiles conditional distribution response_variable given predictor s provides much richer information terms estimating relationships breakpoints confidence bands methods illustrated two examples ecological literature relating index wetlands fish community health amount human activity wetlands adjacent watersheds relating biomass Cyanobacteria Total Phosphorus concentration Canadian lakes
Preferential sampling geostatistics occurs process determines sampling locations may depend spatial process modeled case inference can generally performed conditionally locations revisit seminal work Diggle et_al JRSS C show certain conditions satisfied commonly_used geostatistical models previously proposed Monte_Carlo estimates likelihood_function may behave expected also show alternative numerical methods approximate likelihood_function yield noticeably better results Monte_Carlo based methods Although illustrate results relatively simple models Diggle et_al advantage approach particularly important complex preferential sampling models
Spatial temporal data abundant many scientific fields examples include temperature readings multiple weather stations spread infectious_disease particular region taken time many instances spatial data accompanied mathematical models expressed terms partial differential equations PDEs PDE determines theoretical aspects behaviour physical chemical biological phenomena considered parameters PDE typically unknown must inferred expert knowledge phenomena considered will introduce methodology attaining data driven estimates PDE parameters extending profiling parameter cascading procedure outlined Ramsay et_al also incorporate splines triangulations Lai Schumaker account attributes geometry problem irregular shaped domains internal boundary features
Testing stability linear factor models become important topic statistics econometrics research communities develop general test structural stability linear factor models based monitoring changes largest eigenvalue sample covariance_matrix asymptotic distribution proposed test statistic established null hypothesis mean covariance structure cross sectional units remain stable observation period show test consistent assuming common breaks mean factor loadings results investigated means Monte_Carlo simulation_study usefulness demonstrated application U S treasury yield curve data interesting features subprime crisis illuminated
Longitudinal data occur frequently practice medical studies life sciences Generalized linear mixed_models GLMMs commonly_used analyze data typically assumed random_effects covariance_matrix constant across subject among subjects models many situations however correlation structure may differ among subjects ignoring heterogeneity can cause biased estimate model parameters Covariates measured error also happen frequently longitudinal data_set Ignoring issue data may also produce bias model parameters estimate lead wrong conclusions work propose approach properly model random_effects covariance_matrix based covariates class GLMMs covariates subject measurement_error resulting parameters decomposition random_effects covariance_matrix sensible interpretation can easily modeled without concern positive definiteness resulting estimator Performance proposed approach evaluated simulation_studies well real longitudinal data application Manitoba Follow study
years financial time_series become available increasingly higher frequencies well known time_series sampled different frequencies carry information relevant different components original time_series e g fast slow moving components interested research question take advantage samples multiple frequencies order improve statistical_inference presence multiscale phenomenon Multiscale phenomenon financial time_series reported researches wide range fields ranging physicists applied mathematicians econometricians talk introduce empirical_likelihood EL based test model scaling property using multiple frequency samples method used test whether given model can consistently used across different time scales application apply models capturing multiscale phenomenon financial asset s volatility
Traditional methods detecting periodicity time_series depend assumption identical independent Gaussian noises sensitive outliers propose innovative nonparametric robust periodicity test highly resistant outliers distribution free also works second moment error finite traditional methods fail derive asymptotic distribution test statistic show method efficient assumption Gaussian noises response surface regression approach also implemented approximate finite distribution test statistic addition implementation approach dramatically sped utilizing parallel computation Shared Hierarchical Academic Research Computer Network
Regression models zero inflated count data often need accommodate within subject correlation individual heterogeneity frequently random_effects models utilized incorporating complex correlation structures cases several longitudinal zero heavy count outcomes jointly considered excess zeros may arise several distinct sources case study patterns mechanisms process desistance criminal activity Methodological challenges analysis longitudinal criminal behaviour data include need develop methods multivariate longitudinal discrete data incorporating modulating exposure variables several possible sources zero inflation additional complications outcomes prohibited time secure facility well intervention carry effects outcomes underlying process generating events may resolve individuals
cancer clinical trials medical studies longitudinal measurements data time event survival time often collected patients Joint analyses data improve efficiency statistical inferences propose new joint model longitudinal proportional measurements restricted finite interval survival times potential cure fraction penalized joint likelihood derived based Laplace approximation semiparametric procedure based likelihood developed estimate parameters joint model simulation_study performed evaluate statistical properties proposed procedures proposed model applied data clinical trial early breast_cancer
cystic fibrosis chronic Pseudomonas aeruginosa Pa infection associated worse clinical outcomes including frequent pulmonary exacerbations PE PE leading cause morbidity CF important endpoints CF clinical trials talk discusses joint models address challenge understanding co dependence progression Pa infection recurrent PE time Using data ongoing Early Pseudomonas Infection Control study propose joint model built within frameworks hidden Markov_chain models model progression Pa dynamic recurrent_event models model recurrence PE address additional challenges motivating study including missing misalignment covariates dynamic aspect recurrence PE latent aspect different Pa states infection
Let isotropic sub gaussian m n matrix give simple sufficient condition random sub Gaussian matrix well conditioned restricted subset R n also prove local version theorem allows unbounded sets theorems various applications general theory compressed sensing
Global quadratic hedging problem finding trading strategy minimizes expected squared error value portfolio value contingent claim conditions value portfolio given stochastic integral value contingent claim random variable context problem global quadratic hedging can seen projection L random variable linear space stochastic integrals respect semimartingale recently incorporate effect illiquidity stock prices given semimartingales space parameters case value portfolio given stochastic integral linear respect integrand talk_will look conditions allow find L approximation random variables nonlinear stochastic integrals
talk briefly discuss different_types symmetries probability including stationarity stationarity increments isotropy self similarity exchangeability combinations symmetries naturally related operator path space sense symmetry can expressed invariance distribution stochastic processes respect corresponding operator particular consider random locations stochastic processes hitting times location path supremum fixed interval one hand see probabilistic symmetries can imply properties distributions random locations hand also discuss distributions random locations can used characterize probabilistic symmetries
technological development genomic sequencing enabled researchers unveil wide variability bacteria presented within different locations body necessary better understand environmental host genetic factors impact composition microbiome improve disease management Several analytic approaches introduced summarize assess single multiple OTUs using different computational algorithms Motivated multivariate nature microbiome data hierarchical taxonomic clusters counts often skewed zero inflated propose Bayesian latent variable methodology jointly model multiple operational taxonomic units within single taxonomic cluster novel method can incorporate negative binomial zero inflated negative binomial responses can account serial familial correlations method can help discover genetic environmental factors influence microbiome
Genotype imputation technique extensively used genome wide association studies GWAS well meta analysis talk outline strategies account imputation accuracy including imputation based GWAS results meta analysis considering fixed effect random_effects models adding studies meta analysis typically boosts power detect genetic associations inclusion imputation based studies may adversely affect power due increased genotype uncertainty address trade via reweighing scheme controls contribution imputation based studies meta analyses proposed_method achieves better detection power relative traditional approaches improves validity reliability imputation based meta analysis results
Conventional methods adjusting p values multiple comparisons control family wise error_rate FWER genome wise error_rate recognition lead excessive false negative rates genomics applications led widespread use false discovery rates FDRs place conventional adjustments improvement way FDRs used analysis genomics data leads opposite problem excessive false positive rates sense FDR overcorrects excessive conservatism bias toward false negatives FWER controlling methods adjusting p values Estimators local FDR LFDR much less biased widely adopted due high variance lack availability software address issues propose estimating LFDR correcting estimated FDR level FDR controlled Preprint http hdl handle net
study explores online beer reviews identify types flavours beer popular among consumers different geographic regions exploration beer reviews carried Latent Dirichlet Allocation Natural Language Processing approach cluster text beer reviews flavour sentiment topics Combining text topic data reviews numerical ratings beers categories flavour scent colour can determine flavour preferences matching breweries geographical locations explore beer flavour type preference trends across country analysis used recommendation system predicting regional beer success flavour beer type combinations
Motivated emerging craft brewery scene Vancouver B C set goal design beer tour visit amazing hotspots town Using Hadley Wickham s tidyverse package automate data extraction pinpoint exact locations breweries proceed extract distances travelling times GoogleMaps API travelling salesman algorithm applied optimize best tour path can tailored individual s preferences improve usability tour planning extend workflow interactive customizable web application built Shiny app shows location craft breweries Vancouver respective menus average rating scores menu options Users can simply point click express preferences obtain personalized path utility chart derived user inputs displays marginal utility gained stop also provided
William Gosset Student published Probable Error Mean Despite seminal nature modern day statistics textbooks give article short shrift today s students teachers aware z statistic whose sampling distribution actually derived mathematical derivation simulations check work material used simulations table produced one line missing proof supplied year old Fisher still student subsequent switch collaboration Fisher z t statistic remind readers hope next generation statisticians come know man simply worked Guinness brewery appreciate statistical distributions derived single pass Students well use paper model writing first statistical article Extra material www epi mcgill ca hanley Student
standard challenges faced survey design completeness sample frame sample selection interviewer quality etc can manifest unique ways developing world Sample frames incomplete non existent Interviewer education capacity limited Field conditions often challenging possibility probability unforeseen events must built survey designs presentation covers set illustrative recent case studies challenges faced sub Saharan Africa Topics include using satellite imagery sampling frame Kinshasa proposed alternative random walk second stage selection conflict areas Mogadishu application random geographic cluster sample methodology survey pastoralist nomadic populations eastern Ethiopia implementation high frequency cell phone survey measure socioeconomic impacts Ebola Sierra Leone Liberia Focus will technical implementation challenges faced addressed maintaining generalizable probability design lessons learned future applications
Measuring cash holdings usage particularly difficult due anonymous nature cash Bank Canada undertaken surveys consumer merchants address issue However several challenges involved recruitment respondents low response rates discuss methodology employed address concerns
New Zealand Health Survey monitors health nation s adult child populations produces consistent core set indicators must also reflect emerging policy priorities provide regular information national demographic ethnic regional subgroups achieve goals survey continuously field reports annually continuous nature allows set core questions combined flexible programme modules Data pooled different periods balance timeliness precision sample design oversamples key subpopulations using novel method reflects strengths weaknesses data available design Using mechanisms survey supports policy directions including Smokefree goal Childhood Obesity Plan Rheumatic Fever Prevention Programme continuous improvement responsiveness equity health services
Foldover follow technique used design experiments Traditional foldover designs obtained changing signs columns initial design consider performing column permutation investigate column permutation results combined foldover design better G aberration corresponding traditional combined foldover design Properties foldover designs studied
Routinely collected administrative clinical data increasingly utilized comparing quality care outcomes hospitals problem can considered causal_inference framework comparisons adjusted hospital specific patient case mix can done using either outcome assignment model subject misspecification often interest compare performance hospitals average level care health care system using indirectly standardized mortality ratios doubly robust estimator makes use outcome assignment model case mix adjustment requiring one correctly specified valid inferences present causal estimand indirect standardization terms potential outcome variables propose doubly robust estimator study properties
propensity score PS tool eliminate imbalance distribution confounding variables treatment groups Previous work Pirracchio et_al suggested Super Learner SL ensemble method outperforms logistic_regression LR non linear settings investigated wider range settings varying complexities compare performances logistic_regression generalized boosted models GBM SL estimated average treatment effect using PS regression PS matching inverse probability treatment weighting IPTW found SL LR comparable terms bias covariate balance mean_squared error outperform GBM however SL computationally expensive also found PS regression superior either IPTW matching used two real_data examples demonstrate performances estimators support findings find LR provides best balance two empirical analyses
Missing data often handled using multiple imputation parameters estimated using Rubin s Rules model selection performed multiple imputation propose bootstrap imputation followed model selection model averaging bootstrap distribution estimated variance efficiently computed bootstrap sample Efron confidence_intervals computed using normal distribution call bootstrap imputation Efron s Rules compare Efron s Rules estimator Rubin s Rules estimator also compare single imputation followed model selection Impute Select estimator bootstrap percentile confidence_intervals Simulation_studies show model averaged estimators perform better Impute Select confidence_intervals based Rubin s Rules can severe undercoverage Efron s Rules estimator improves performance normal theory confidence_intervals
aid discrimination two possibly nonlinear regression models study construction experimental designs Considering two models might approximately specified robust maximin designs proposed rough idea follows impose neighbourhood structures regression response describe uncertainty specifications true underlying models determine least favourable terms Kullback Leibler divergence members neighbourhoods Optimal designs maximizing minimum divergence Sequential adaptive approaches maximization studied Asymptotic optimality established
Bradley Terry model paired comparisons broadly applied many areas statistics sports machine learning work determine maximum_likelihood estimates parameters latent variable models Bradley Terry model using theory optimal design consider parameters Bradley Terry model terms set another parameters consider weights optimal design weights positive sum one order solve problem first consider likelihood_function criterion function determine optimality conditions terms point point directional derivatives likelihood_function determine maximum_likelihood estimates using class algorithms indexed function depends derivatives likelihood_function Finally apply problem data_set American League Baseball Teams
Multi label classification supervised learning problem observation may associated multiple binary outcome labels simultaneously give overview common approaches multi label classification also introduce new approach extension nearest neighbor principle Experiments benchmark multi label data_sets show proposed_method average outperforms commonly_used approaches terms classification performance
study problems maximum_likelihood estimation classification based different collections order statistics finite mixture_models consider problems labeled unlabeled collections order statistics New missing mechanisms expectation maximization algorithms developed exploit structure observed data estimation procedures Various model based classification criteria developed observed unobserved data underlying FMM simulation_studies evaluate performance estimation classification methodologies Finally proposed methods used real_data analysis
Classification regression trees CART machine learning methods constructing prediction models recursively partitioning data space fitting simple model within partition Two innovative applications CART proposed discretization interaction detection fitting regression models Discretization convert continuous variable categorical necessary statistical methods analysis variance ANOVA chi square tests fitting multinomial model etc Simulation_studies medical applications used illustrate effectiveness classification regression trees transferring continuous data categorical extracting interactions structure explanatory variables
Classification molecular subtypes breast_cancer using deep learning poses exceptional challenge train deep neural network DNN hundreds thousands parameters using small dataset thousands samples tens thousands features expected integration knowledge multiple data sources measured individuals can overcome challenge paper proposed number DNN models integrate different omics data_sets collected set breast_cancer patients predicting molecular subtypes compared results DNNs traditional machine learning models Support Vector Machine SVM Random Forest RF demonstrated DNN models superior SVM RF
Minimum Hellinger Distance estimator obtained minimizing Hellinger distance assumed parametric model nonparametric estimator model density method receives increasing attention past decades due asymptotic efficiency excellent robustness small deviation model work propose use Minimum Profile Hellinger Distance estimation MPHD parameters two class symmetric location models Asymptotic normality property robustness estimator discussed comparison MLE carried Monte_Carlo simulation_studies estimation applied breast_cancer dataset
Classification models based multivariate outcomes commonly_used discriminating groups repeated measures RM designs However majority models rely assumption multivariate normality rarely incorporate covariate information routinely collected RM designs Using Monte_Carlo simulation methods investigate impact time varying time invariant covariates misclassification error_rate classical linear discriminant analysis LDA LDA based linear mixed effect regression Bayes classifier logistic_regression quadratic inference function classifiers provide recommendations using classifiers RM designs multiple covariates
Life insurers exposed deflation risk falling prices lead insufficient investment returns inflation indexed protections make insurers vulnerable deflation spirit study proposes market based methodology measuring deflation risk based discrete framework latter accounts real interest rate inflation index level conditional variance expected inflation rate US inflation data used estimate model show importance deflation risk Specifically distribution fictitious life insurer s future payments investigated find proposed inflation model yields higher risk_measures ones obtained using competing models stressing need dynamic market consistent inflation modelling life insurance industry
Bank Canada conducted Retailer Survey Cost Payment Methods two reasons estimate total private social costs Canadian retailers cash cards analyse efficiency payment methods focus specifically private costs large businesses presence outliers sample suggests need robust estimation method Three robust versions Royall s best linear unbiased predictor BLUP computed estimators naive conditional bias Beaumont Haziza Ruiz Gazen BHR Chambers BHR propose adaptive method choosing tuning constant derive tuning constant Chambers estimator minimising parametric bootstrap estimator mean_squared error MSE results total private costs retailers cash cards compared robust Chambers estimator conditional bias estimator also compared
consider stochastic model target benefit pension fund continuous time plan member s contributions set advance pension payments depend financial situation plan implying risk sharing different generations pension fund invested risk free asset risky asset particular stochastic salary correlation salary movement financial market fluctuation considered Using stochastic optimal control approach derive closed form solutions optimal investment strategies well optimal benefit payment adjustments minimize combination benefit risk deviating target benefit inter generational transfers Numerical analysis presented illustrate sensitivity optimal strategies parameters financial market salary rates also consider optimal benefit changes respect different target levels
talk_will study specific stochastic control problem known mean variance investment problem portfolio strategy subject short selling constraint specifically Black Scholes type financial market modelization stock s volatility follows Heston diffusion process will construct optimal portfolio carefully chosen adjoint processes backward stochastic differential equations
paper consider computation risk_measures VaR TVaR portfolio exchangeable risks assuming marginal distributions known dependence_structure partially known approach compute lower upper bounds risk_measures VaR TVaR aggregate risk portfolio approach based known moments stochastic orders Numerical examples provided illustrate proposed approach
paper studies moments distribution aggregate discounted claims Markovian environment claim arrivals claim amounts forces interest discounting influenced underlying Markov process representing external environment changes Specifically assume claims occur according Markovian arrival process MAP recursive formula derived moments aggregate discounted claims occurred certain states also study two types covariances covariance aggregate discounted claims occurred two subsets state space covariance aggregate discounted claims occurred two different times distribution aggregate discounted claims occurred certain states specific time also investigated two state Markov modulated model numerical results presented
many scientific studies different statistical tests proposed competing claims method performance Discussions optimality rely assumptions optimal test often impossible multi dimensional parameter space example studying association multiple genetic variants complex human trait show existing_methods belong linear quadratic class tests powerful sub spaces another location scale test preferred depending presence main interaction effect cases show two classes tests asymptotically independent global null hypothesis Thus can use Fisher s method derive new class robust tests obtain asymptotic distribution desirable analyzing big data talk based work Derkach Lawless Sun Statistical Science Soave et_al AJHG Soave Sun Biometrics
will share career highlights embarrassments will tell ROC papers arose ask disseminated widely will describe issues arising extra mural consultations including rule thumb logistic_regression requires outcome events parameter work revise decision limits used World Anti Doping Agency Court Arbitration Sport case involving cyclist tested ve communicating non statisticians will end year mission help cancer screeners replace proportional hazards PH model prevailing way measure mortality reductions produced cancer screening reading paradigm changing paper never asked characteristics disease process targeted countermeasures prerequisites PH model answers look data formal tests non proportionality logic e bio logic reasoning
Participating contracts popular insurance policies payoff policyholder linked performance portfolio managed insurer consider portfolio selection problem insurer offers participating contracts S shaped utility function Applying martingale approach closed form solutions obtained resulting optimal strategies compared portfolio insurance hedging strategies CPPI OBPI also study numerical solutions portfolio selection problem constraints portfolio weights
Inspired Marshall Olkin s approach multivariate distributions can constructed use exponential mixtures paper propose alternative hierarchical Archimedean copula obtained multivariate survival functions multivariate mixed exponential distributions key element construction latter defined vector mixing random variables follows multivariate gamma distribution Kibble s bivariate gamma distribution presenting construction technique properties new family copulas investigated simulation algorithms provides illustrative examples given Risk aggregation capital allocation newly proposed dependence_structure also examined
prevalence chronic diseases account significant portion deaths new approach life insurance emerged address issue new approach integrates health rewards programs life insurance products insured classified fitness statuses according level participation get premium reductions superior statuses introduce Markov_chain process model dynamic transition fitness statuses linked corresponding levels mortality risks reduction embed transition process stochastic multi state model describe new life insurance product Formulas given calculating benefit premium reserve surplus results compared traditional life insurance Numerical examples given illustration
consider total dividends paid prior ruin threshold strategy investigate class Sparre Andersen risk processes associated class delayed risk processes rational distributed inter claim times presence constant dividend barrier present structure integro differential equation applicable numerous quantities Gerber Shiu function maximum surplus etc Finally study fraction time dividends payment prior ruin interest investor wants know often expect dividends
lab component provides intimate stimulating setting student interact lectured content lab designed supplement lecture material mature student comprehension However often labs mutated rarely attended communal homework periods actually diminish every pedagogical underpinning associated higher education focus present findings dynamic statistics lab experience designed refresh re establish noble venture supplemental learning multi staged weekly seminar compels preparation requires participation enforces comprehension evaluates retention approach utilizes small step learning replace anxiety cumulative lab examination will_also display open source software integration WebWork R Project Statistical Programming alleviate additional resource overhead
Educreations free downloadable app iPad allows instructors make narrated videos write problems stylus pen iPad screen created videos can uploaded student access website ability students access videos time review questions notes can valuable instructional tool distance education regular class students session will talking ways program can used effectively inside outside classroom basics creating videos make accessible students Though necessary feel free bring along iPad like follow along instructions
talk_will show one can use R simulate personalized assignments automatically grade particularly useful one wants make sure students large introductory_statistics courses homework individually successfully implemented strategy introductory_statistics course engineers taken students course students assigned two homework analyze individual randomly generated datasets R scripts generate datasets take care grading
Predictive mean matching imputation popular handling item nonresponse survey sampling study asymptotic properties predictive mean matching mean estimation Moreover conventional bootstrap inference matching estimators fixed matches shown invalid due nonsmoothess nature matching estimator propose asymptotically valid replication variance estimation predictive mean matching estimators key strategy construct replicates estimator directly based linear terms estimator instead individual records variables Extension nearest neighbor imputation also discussed simulation_study confirms new procedure provides confidence_intervals correct coverage properties
Monotone patterns non response may occur longitudinal studies measured variables dependent beneficial use joint statistical model impute missing values propose use vine copulas factorize density observed variables cascade bivariate copulas yield flexible model joint distribution structure vine depends non response pattern build work Aas et_al propose method select model estimate parameters bivariate copulas selected model impute using constructed model imputed values drawn conditional distribution missing values given observed data example using United Kingdom Labour Force Survey quaterly data illustrates proposed methodology
Swiss cheese nonresponse refers case variables can contain missing values general pattern even wording abuse since Swiss cheeses holes However case Swiss cheese nonresponse possible consider variables known every units can thus used auxiliary variables propose new technique random donor imputation method based establishment consistency principles nonrespondent donor close donor must used missing variables unit Moreover impose imputing known variables nonrespondent values donor totals variables must remain procedure based calculation imputation probabilities Next donors selected randomly way constraints satisfied
Personalized medicine optimizes patient outcome tailoring treatments patient level characteristics approach formalized dynamic treatment regimes DTRs decision rules take patient information input output recommended treatment decisions DTR literature seen development increasingly sophisticated causal_inference techniques attempt address limitations typically observational datasets note however practice patients receive optimal near optimal treatment outcome used part typical DTR analysis provide much extra information light propose reward ignorant modelling ignoring outcome eliciting optimal DTR regressing observed treatment relevant covariates standard analysis present early results investigating concept analysis data International Warfarin Pharmacogenetics Consortium
Informative designs increasingly common experiments sequential accrual subjects observed responses used estimate optimal design next cohort making early stopping decisions safety efficacy re estimating sample_size Typically estimates following informative designs normally distributed asymptotically even assuming normal model unknown mean Instead ignoring ancillary processes inference can based fully unconditional probabilities conditional ancillary statistic Indeed norming parameter estimates measures information Fisher information will often produce asymptotic normal random variables present alternatives explicitly two stage adaptive optimal design data accumulated first stage used select design second stage Lane Flournoy JPS Lane Yao Flournoy JSPI Lane Wang Flournoy mODa
Generalized linear mixed_models GLMMs commonly_used analyzing clustered correlated data non continuous responses including longitudinal data repeated measurements paper discuss impact different_types model misspecification estimation regression coefficients GLMM explore construction methods model robust designs GLMMs possible misspecification first derive asymptotic distribution maximum_likelihood estimators fixed effect parameters imprecision appears assumed linear predictor investigate techniques sequentially designing experiments values explanatory variables GLMM can chosen optimal robust way Design problems general setting GLMMs addressed yet emphasis application longitudinal data Finally performance proposed designs assessed simulation_studies
paper summarize latest results field causal_inference related big data particular address problem data fusion piecing together multiple datasets collected heterogeneous conditions e different populations regimes sampling methods obtain valid answers queries interest availability multiple heterogeneous datasets presents new opportunities big data analysts since knowledge can acquired combined data possible individual source alone However biases emerge heterogeneous environments require new analytical tools biases including confounding sampling selection cross population biases addressed isolation largely restricted parametric models present general non parametric framework handling biases ultimately theoretical solution problem data fusion causal_inference tasks
analysis multi arm randomized trials methods pooling data across trials belong one two broad classes first class methods consists contrast based estimators estimate contrast treatment effect pair treatment levels pool across estimated contrasts second class encompasses arm based methods contrasts marginal estimates treatment arm Leading researchers assailed arm based methods broad criticism breaking randomization implying biased estimation causal effects treatment However one established formal causal definition breaking randomization critical examination amount bias result talk characterize conditions arm based methods biased population causal effects discuss advantages arm based methods contrast based methods regards precision
evaluating accuracy diagnostic tests three designs commonly_used crossover design randomized design non comparative design Existing methods meta analysis diagnostic tests mainly consider simple cases reference test none studies can considered gold standard test studies use either randomized non comparative design Yet proliferation diagnostic instruments diversity study designs used boosted demand develop general methods extend Bayesian hierarchical summary receiver operating characteristic model network meta analysis diagnostic tests simultaneously compare multiple tests missing_data framework model accounts potential correlations multiple tests within study heterogeneity across studies also allows different studies perform different subsets diagnostic tests model evaluated simulations illustrated using real_data deep vein thrombosis tests
N trials single patient multiple crossover studies determining relative effectiveness treatments individual participant series N trials assessing scientific question may combined make inferences average efficacy treatment well borrow strength across series make improved inferences individuals Series include two treatments may enable network model can simultaneously estimate compare different treatments models complex trial contributes data form time_series changing treatments data therefore highly correlated potentially contaminated carryover will use data series N trials ongoing study assessing different treatments chronic pain illustrate different models may used represent data
new hidden Markov model HMM based method proposed identify differentially methylated CpG sites regions data arising genome sequencing conjunction bisulphite treatment BS seq Features BS Seq data include variable read depth unevenly distributed sites non smooth patterns correlations sites Shortcomings available methods include inability compare two groups examine associations continuous covariates difficulties handling missing low read depth data proposed_method flexible choosing HMM order uses weights account read depth introduces noise state allow sites follow general HMM pattern comprehensive comparison new method several existing_methods will presented based real simulated_data
Recently several region based multivariate tests proposed identify rare variant RV associations multiple correlated continuous phenotypes observed However methods assume multivariate normality MN distribution phenotypes MN assumption violated modelling phenotypic dependence using Pearson correlation may suitable can lead inflated type error address issue using copulas model phenotypic dependence bivariate case allows us assume marginal phenotypic distribution exponential family derive copula based variance component score test association RVs bivariate phenotypes provide analytic test p value calculation illustrate performance proposed methodology using simulation_studies analyzing real_data PETALE project Sainte Justine Hospital research center patients survived childhood leukemia
Testing association DNA methylation health phenotypes important area research Illumina arrays exploit reisistance methylated DNA effects bisulfite treatment allow genome wide inference methylation phenotype relationship method distinguish two methylations types mC hmC latter form shown abundant human brain may important neurological phenotypes new method called oxidative bisulfite treatment allows isolation mC methylation theory can find hmC methylation signal taking difference signals two parallel runs one using regular bisulfite treatment one using oxidative bisulfite treated DNA measures noisy difference often negative examine statistical methods evaluate amount hmC methylation signal two parallel runs association hmC phenotype
Recent advances technology laboratory protocols allowed generation high throughput sequencing data single cell including DNA methylation data One main goals field research cluster data different cells together according DNA methylation profiles amount DNA material per cell limited generated single cell data usually sparse e contain large amount missing_data address problem propose probabilistic based algorithm infer missing_data cluster cells simultaneously borrowing statistical strength across cells neighbouring sites investigate properties proposed_method different scenarios via simulation_studies apply algorithm publicly available single cell DNA methylation data_sets
Recent technological advances allowed real time tracking immune cells cytotoxic T cells search environment pathogens Understanding way cells move interest cell biologists aim improve search efficiency via treatment literature several different_types stochastic models considered T cell movements including Levy flight Brownian motion Persistent random walks data consider based observing motion cytotoxic T cells synthetic collagen matrix Based observations cells often switch several modes motion apparently affected attendance cells nearby collagen therefore propose hidden Markov model separate correlated random_effects different Cartesian axes increments cells trajectory assess performance proposed model using simulation_studies
Quantile regression QR models attractive several fields due capability provide rich description set predictor effects outcome without making assumptions shape outcome distribution work consider problem selecting grouped variables linear quantile_regression models introduce Group variable Selection framework Quantile Regression GSQR appealing group penalties group lasso penalty group non convex penalties MCP SCAD local approximation non convex penalties sparse group lasso penalty propose smooth block wise descent algorithm approximates QR check function smooth pseudo quantile check function differentiable zero using maximization minimization trick update group predictors derive simple efficient group wise descent algorithm illustrate GSQR approach performance using simulations high_dimensional settings analyze breast_cancer gene expression dataset
Multitaper spectrum estimation techniques can used test presence modes space physics dataset model assumes presence random amplitude periodic components Random modulators account single form non stationary process forms will sought reside signal Accounting processes reduces effects test bias due departures model assumption stationarity detect identify non stationary processes standard use map two frequency coherence process manifest discrete set high coherence curves However dynamic range two frequency spectrum high coherence statistic subject severe bias contamination proposed alternative test statistic makes use circularity coefficients set coefficients contains effects data tapers coherence frequency bias high
new algorithm developed Box Cox estimation forecasting simulation time_series models algorithm implemented R illustrative applications presented
study high_dimensional covariance_matrix estimation assumption low rank diagonal matrix decomposition covariance_matrix estimator decomposed low rank matrix L diagonal matrix D rank L either fixed small suppressed penalty function moderate conditions population covariance_matrix penalty function estimator enjoys consistency results algorithm iteratively updates L D applied solve estimator simulations real_data analysis presented show performance finite sample_size
Infection disease transmission individuals heterogeneous population often best modelled contact network However contact network data often unobserved form missing_data can accounted Bayesian data augmented framework using Markov_chain Monte_Carlo MCMC However fitting models MCMC framework can highly computationally intensive investigate fitting network based infectious_disease models completely unknown contact networks using full Bayesian MCMC framework approximate Bayesian computation population Monte_Carlo ABC PMC methods done context simulated_data data UK foot mouth disease epidemic show ABC PMC able obtain reasonable approximations underlying infectious_disease model huge savings computation time
Conventional phylogenetic clustering approaches rely arbitrary cutpoints applied posteriori phylogenetic estimates Although practice Bayesian bootstrap based clustering tend lead similar estimates often produce conflicting measures confidence clusters bootstrap support phylogenetic clusters benefit straightforward interpretation current study proposes new Bayesian phylogenetic clustering algorithm refer DM PhyClus identifies sets sequences resulting quick transmission chains thus yielding easily interpretable clusters without using ad hoc distance confidence requirement Simulations reveal DM PhyClus can outperform conventional clustering methods terms mean cluster recovery apply DM PhyClus sample real HIV sequences revealing set clusters whose inference line conclusions previous thorough analysis
many applications covariates subject measurement_error vast literature measurement_error problems regression settings little known impact covariate measurement_error dependence parameter_estimation multivariate models address latter problem using conditional copula model show dependence parameter estimates can significantly biased covariate measurement_error ignored analysis identify underlying bias pattern direction magnitude marginal effect sizes introduce exact bias correction method special case Gaussian copula general conditional copula models likelihood based correction method proposed likelihood_function computed via Monte_Carlo integration consistency asymptotic normality bias corrected estimators established Numerical studies confirm proposed bias correction methods yield target estimation demonstrate proposed correction methods subset SWAN Study Women s Health Across Nation data
challenging accurately assess volatility financial assets high frequency data Heteroscedasticity realized volatility one factors causes difficulty study propose localized quantile_regression approach proposed approach sequentially identifies homogeneous intervals applies quantile_regression model homogeneous interval Hence quantile_regression model time dependent coefficients quantile_regression model require distributional assumptions Direct interpretation results selected quantiles might interests practitioners area finance simulation_study shows localized quantile_regression model fits realized volatility closely also predictive
Bivariate time_series counts excessive zeros frequently occur biological social environmental sciences deal data propose model includes serial correlated random_effect series shared responses use compound Poisson distribution capture excessive zeros will_discuss parameter_estimation model implement model analysis musculoskeletal workplace injuries
Due close connection extreme value copulas Pickands dependence function useful tool study dependence extremes function must time convex satisfy boundary constraints proposing bona fide estimator remains difficult task work interpret Pickands dependence function kind regression function adapt least squares type convex regression algorithm problem Despite nonparametric nature problem algorithm require select smoothing parameter Moreover can implemented using convex quadratic programming Besides consistency results function well derivative may derived results weighted empirical processes
talk apply realized volatility forecasting Extreme Value Theory EVT propose two step approach returns first pre whitened high frequency based volatility model EVT based model fitted tails standardized residuals realized EVT approach compared conditional EVT McNeil Frey assess approaches ability filter dependence extremes produce stable sample VaR ES estimates one day ten day time horizons main finding GARCH type models perform well filtering dependence realized EVT approach seems preferable forecasting especially longer time horizons
appealing use marginal models analysis recurrent events randomized trials several types events arise interest may lie nature dependence_structure adopt multivariate random_effect models dependence type specific random_effects accommodated Gaussian copula models retain simple interpretation marginal treatment_effects separately reflect heterogeneity risk type event provide insight dependence different_types events Inference proposed based composite likelihood avoid high_dimensional integration relative efficiency estimators obtained simultaneous two stage estimation examined application study nutritional supplements malnourished children given goal evaluate reduction rate several types infection Extensions accommodate interval censoring described
Accelerometer data possess unique challenges Multiple Imputation MI require careful consideration selecting imputation models methods challenges become daunting imputation performed epoch level Yet MI appealing setting accelerometers consistently worn study participants missing epochs data create issues commonly employed methods accelerometer data analysis may introduce bias physical activity measures Zero inflated Poisson log normal imputation models contrasted simpler forms imputation models evaluated based epoch level imputation accuracy well ability recover common physical activity summary measures interest accelerometer data used investigation Active Play Study ongoing physical activity study involving children youth Kingston Ontario
analysis lifetime data interest often lie modeling time particular event occurrence certain event identifying associated risk factors evaluating effects survival time event occurrence may encounter heterogeneous group event interest may occur cease occur period time individuals Mover stayer models developed using binary variable indicate whether underlying process resolved addition data measurement_error often occurs medical research public health problems associated well known yet often ignored due technical difficulties propose expectation maximization EM algorithm based method estimate parameters adjust mismeasurement present covariates advantages na ve analysis illustrated simulation_studies motivating study breast_cancer examined illustrate proposed_method
Missing observations responses covariates commonly observed longitudinal studies missing_data missing random inferences likelihood framework often require joint modelling response covariate processes well missing_data processes associated incompleteness responses covariates Specification four joint distributions difficult modelling computation discuss three specific missing_data mechanisms simplified pairwise likelihood formulations likelihood functions lead consistent estimators enjoy better robustness computational convenience proposed_method evaluated empirically simulation_studies applied real world dataset
longitudinal observational studies marginal structural models MSM can used account time dependent confounding Many estimation approaches Longitudinal Targeted Maximum Likelihood Estimation LTMLE require finite number time points variables measured contrast electronic health data EHD produced mechanisms collect health system user information real time common practice longitudinal analysis preceded arbitrary discretization talk describe causal_inference problem operating data defined coarsening discretization observed data propose novel selection procedure uses cross validation LTMLE loss function select optimal discretization estimation data adaptive MSM parameter interest present simulation_study apply approach study using EHD evaluate relative impact asthma medications pregnancy
talk_will present overview history current advances predictive analytics world business applications drawn diverse fields retail utilities manufacturing public service financial services insurance sports Methods will include discussion topics ensemble models gradient boosting partial least squares support vector machines text mining link analysis association rule mining
describe new residual general regression models defined pr Y y pr Y y y observed outcome Y random variable fitted distribution probability scale residual PSR can written E sign y Y whereas popular observed minus expected residual can thought E y Y Therefore PSR useful settings differences meaningful expectation fitted distribution can calculated present several desirable properties PSR make useful diagnostics measuring residual rank correlation demonstrate utility continuous ordered discrete censored outcomes various models including Cox regression quantile_regression ordinal cumulative probability models fully specified distributions desirable needed cases suitable residuals available residual illustrated simulated real_data
traditional robust statistics generally assumed majority observations data free contamination minority observations contaminated contaminated observations flagged outliers weighted even single component contaminated observations may fully depart bulk data situation usually refers casewise outliers However observations can partially contaminated type contamination often appears single outlying cells data matrix therefore usually refers cellwise contamination cellwise contamination lot information lost weighting whole observation especially high_dimensional data Recent work shown procedures proceed way robust talk_will sketch proposal estimate multivariate location scatter cell casewise contamination
Diet compositions marine predators often interest marine ecologists trophic structure studies non lethal sampling created need non invasive diet estimation techniques Methods using fatty acids developed obtain dietary estimates previously difficult acquire Building existing quantitative fatty acid signature analysis constructed maximum_likelihood approach estimating dietary proportions includes standard errors allows potential inclusion covariates model model assessed using simulated well real life data results compared current approach
propose new class mixed effects model compositional data based Kent distribution directional data random_effects also Kent distributions One useful property new directional mixed model marginal mean direction closed form interpretable random_effects enter model multiplicative way via product set rotation matrices conditional mean direction random rotation marginal mean direction estimation apply quasi likelihood method results solving new set generalised estimating_equations shown low bias typical situations inference use nonparametric bootstrap method clustered data rely estimates shape parameters shape parameters difficult estimate Kent models new approach shown tractable traditional approach based logratio transformation
Compositional data met many different fields economics archaeometry ecology geology political sciences Regression dependent variable composition usually carried via log ratio transformation composition via Dirichlet distribution However zero values data two ways readily applicable Suggestions problem exist rely substituting zero values paper adjust Dirichlet distribution covariates present order allow zero values present data without modifying values modify log likelihood Dirichlet distribution account zero values Examples simulation_studies exhibit performance zero adjusted Dirichlet regression
huge advances measurement computing information technology years since University Manitoba Department Statistics founded transformed ways statistics applied talk_will review history statistical applications science technology period will_discuss current activities important issues including use big data role statistics within data science distinction scientific learning technological problems involving decision making
one speakers session celebrating th Anniversary Department Statistics University Manitoba sponsored Business Industrial Statistics Section asked reflect earlier interactions Department Statistics industrial partners will provide examples interactions describe interactions arose will_also discuss benefits academic statisticians involved interactions well challenges lessons learned
First dependence properties family chi square copulas presented family attractive generalizes Gaussian copula allows flexible modeling high_dimensional random vectors class dependence structures applied spatial statistics predict value non Gaussian stationary random field position observed interpolation method compared kriging method real example via simulation_study
Regression analysis one oldest topics statistics explanatory variables response_variable interest simultaneously observed fitting joint multivariate density variables enable prediction via conditional distributions vine copulas proven flexible tool high_dimensional dependence modeling introduce new regular vine copula based regression_model avoids issues variable selection variable transformation heteroscedasticity uses general regular vines developed efficient algorithm compute conditional distribution model able handle mix continuous discrete variables predictors response_variable can either continuous discrete thus unified model regression classification problems demonstrate predictive performance proposed_method using real_data set
use concepts copula modeling statistical learning improve estimation Kendall s tau correlation matrix set variables interested recovering structural properties variables joint distribution Relaxing full exchangeability assumption assume existence exchangeable subsets variables without explicitly specifying block exchangeable dependence structures corresponding tau matrix contains many identical entries iteratively modify usual estimator tau introducing constraints corresponding structural properties discovered thus producing sparser representation dependence first algorithm used produce sequence decreasingly complex models among final model selected Final selection provided second algorithm procedure yields improved estimation tau matrix better representation dependence among variables approach applied data US stock market
talk develop general method estimating vector parameters arbitrary dimension multivariate copula models proposed estimator based first p moments multivariate probability integral transformation one can associate given parametric copula model unbiased estimator p moments first described method moments estimator unknown vector parameters defined order method applicable even explicit expressions theoretical moments available simulated version estimator developed well Interestingly latter can performed long one able simulate given copula model consistency asymptotic normality estimators formally established standard mild conditions performance estimators terms bias mean_squared errors investigated extensive simulation_study
propose novel linear discriminant analysis approach classification high_dimensional matrix valued data commonly arise imaging studies Motivated equivalence conventional linear discriminant analysis ordinary least squares consider efficient nuclear norm penalized regression encourages low rank structure Theoretical properties including non asymptotic risk bound rank consistency result established Simulation_studies application electroencephalography data show superior performance proposed_method existing approaches
Multiple sclerosis MS autoimmune disease attacks central nervous system Magnetic resonance imaging MRI plays central role diagnosis management MS patients damage myelin visible MRI research question interest whether MRI images can predict MS subtype answer question propose Bayesian scalar image regression_model scalar outcome MS subtype binary image MRI covariates Parameters covariates spatially varying fitted using Gaussian random fields proposed model fitted real_data set consisting MS patients MS subtypes Hamiltonian Monte_Carlo HMC algorithm proposed implement full Bayesian statistical_inference reduce computational burden code problem run parallel graphical processing unit GPU
Much work recent years shown associations schizophrenia abnormal brain functional connectivity represents pattern interaction different brain regions functional connectivity network can derived functional MRI fMRI data provides indirect measure brain neural activity rest task reasonable spatial temporal resolution Despite much work reliable statistical biomarker yet identified can used diagnose schizophrenia guide treatment work aims finding reliable neuroimaging based sets features biomarkers statistically significant also predictive disease novel subject stable across different data subsets Results applying approach functional connectivity network features extracted fMRI data patients diagnosed schizophrenia healthy controls multi site fMRI dataset will presented
classical notion measurement_error typically relies mean zero assumption expectation errors conditional data However many applied problems medical social ecological sciences assumption often unreasonable talk_will define notion weakly calibrated measurement unobservable true quantity based upon weaker mean zero assumption accurately reflects structure applied problems disciplines will explore certain attractive features measurement_error formulation present traditional model Finally will indicate theoretical considerations can lead practical inferential gains context real problems medical social ecological sciences
statistical association studies outcomes determined combinations error prone accurately measured variables interact techniques dealing interaction terms error prone variables involved quite challenging practice often see use additive models ignore interaction effects However consequence erroneously omitting interactions models recognized one main threats efficiency studies fact presence error prone variables detecting interaction term challenging either individual effects order improve accuracy precision assessment factors one needs take account errors hand including interaction terms often raises issue non identifiability presentation will_discuss association studies primary focus interaction terms subject misclassification remedies deal issue non identifiable parameters
sample obtained intersecting two independent samples variance estimator variance expanded estimator can decomposed two different ways according sample used conditional expectations Even methods give result show one decomposition generally practical compute variance one convenient estimate variance differences decompositions due simplifications joint inclusion probabilities sheds light two particular cases reverse approach used nonresponse case estimation variance two stage sampling designs
Multiple samples often available different settings efficient inferences particular population may obtained suitable use information samples talk describe use empirical_likelihood Density Ratio Model DRM effectively combine information multiple samples show samples outcompetes fewer samples dominance estimation efficiency long DRM assumption satisfied presentation focuses estimation DRM parameters demonstrates efficiency gain parameter_estimation including samples DRM
studies may relatively affordable measure response_variable covariate might expensive obtain situation cost efficient response dependent two phase sampling designs considered phase easily measured variables including response_variable individuals cohort large random sample population phase obtain expensive variables subset individuals selected according response_variable obtained phase consider likelihood pseudo likelihood based methods incomplete data analysis estimate regression parameters extend estimation methods setting second phase sampling depends multiple response variables objective compare efficiency estimators develop efficient sampling model specification address change efficiency compared univariate response dependent sampling
practice number responses known prior data analysis responses also need variable selection research found paper address response_variable selection approach propose novelty response best subset selection RBS model provide estimation procedure perform response best subset selection regression coefficient estimation via penalizing unselected responses variables estimations enjoy oracle property model consistency asymptotic normality regression coefficient estimators corresponding selected response variables proposed model procedure can extended situation response variables group effects finite sample sized sample simulation_studies demonstrate proposed model procedure efficient completive apply promising approach study real_data set results shed new light selection response variables
Students will greatly assisted learning statistics feel useful relevant needs Ideas science studies can used provide possible ways motivating student interest statistics also can used suggest students good mathematical computational skills may completely unprepared content introductory_statistics course
flipped classroom model gaining popularity s focus hands learning practical exercise makes sense useful model teaching statistics Course notes read prior live class practice problems completed lab setting small groups smaller class size lectures just couple ways using flipped classroom model teach Introductory level statistics online model appeals learning styles also students want self directed challenge becomes making sure prepared tests exams purpose presentation share tools working course see others different things worked classrooms
discussion recently within statistical community whether rule thumb n really ensures central limit theorem holds Needless say conclusion date gross oversimplification holds relatively well behaved data impact teaching introductory_statistics courses want students critical basic techniques applied want feel confidence results techniques used appropriately will_discuss simulation_study typical procedures taught introductory_statistics examining can judged reliable
Two key components successful learning engagement material practice introductory Statistics course attracts large number students wide range abilities interests Twelve WeBWorK homework sets assigned students receive instant feedback can make infinite attempts achieve correct answer deadline added gamification feature exist WeBWorK designed increase amount time students want spend homework gamification techniques made use leveling achievement created five levels Novice Junior Statistician five challenge questions twelve various achievement badges general achievement badges constructed reward students practicing good homework habits encourage solve every problem will share result discuss effectiveness gamification features
talk_will review current state undergraduate Statistics curricula universities across Canada will present quantitative qualitative information structure composition Major programs Statistics specifically will look number type course requirements program learning outcomes serve topics skills develop well relevant information talk intends give overview collectively educate Statisticians ultimate goal helping identify directions future curricular development
