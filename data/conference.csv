Abstract,Abstract Description,Name/Institution/Workplace,Time Slot,Student Presenter
The Challenge of Creating Data Collection Methods that are Neither Too Far Ahead nor Behind our Survey Respondents,"The last ten years have witnessed enormous increases in computer memory, connectivity, storage and sensor capabilities, making it possible to obtain and send information from nearly anywhere to everywhere, and do it on pocket-size devices as easy to carry as one’s wallet. The long-term implications of these developments for conducting surveys are likely to be enormous, but will happen at a more measured pace than the technology allows. In this presentation I will discuss the transition away from long established survey data collection methods towards newer ones, and the challenge data collection methodologists face in being neither too far ahead or too far behind where people are at this time of rapidly accelerating change. In particular, I will discuss the recent development of web-push data collection methods, i.e. contacting sample units by one mode, usually postal mail, to request a response over the Internet, while withholding other response options until later contacts.",Don A. Dillman Washington State University,Monday 08:40-09:45,N
The Analysis of Longitudinal Multivariate (Discrete or Continuous) Traits under Irregular Time Measurements,"There are well known methods to analyse longitudinal continuous variables with regularly spaced measurements and some of them (in particular, mixed models) have been adapted to deal with longitudinal quantitative traits. Nonetheless, the problem is far from being solved. The modelling and analysis of longitudinal multivariate discrete and or continuous traits poses various challenges to accommodate the family structure and irregular time measurements. In this presentation we discuss these challenges and propose some ways of addressing the problems.",J. Concepcion Loredo-Osti Memorial University of Newfoundland,Monday 10:20-10:50,N
An Ancestral Tree-Based Approach to Detect Rare and Common Variants,"For detecting genetic variants associated with a disease or trait, it is useful to consider the ancestral trees that gave rise to the sample's genetic variability. For both rare and common disease or trait influencing genetic variants, we expect to see haplotypes from individuals with similar values of the disease or trait clustered together in the ancestral tree corresponding to the genomic location of the variant. In this presentation, we describe how tree-based statistics can be used for detecting both rare and common genetic variants associated with either continuous or dichotomous outcomes. We summarize the performance of these statistics on simulated data having known and missing tree structures and we compare results to those obtained using conventional approaches to detect genetic association. Finally, application of the tree-based method to real data is also discussed.",Kelly M. Burkett University of Ottawa,Monday 10:50-11:20,N
"Mapping Complex Traits, Rare Variants and Interaction via the Coalescent Process with Recombination ","Genetic population data is the result of the evolution of chromosomes on this population; thus, it has been found natural to try to analyze such data by modeling the unknown history of this population. By combining an approach based on the coalescent process with recombination and importance sampling techniques, we can show how this stochastic process can be used to map genes influencing a disease. We address issues pertaining to complex diseases, whether this complexity is due to incomplete penetrance or phenocopy, to multiple rare variants, or to the interaction between genes. We introduce the progress made in building such histories and outline the remaining challenges. ",Fabrice Larribe Université du Québec à Montréal,Monday 11:20-11:50,N
How Best to Bridge the Gap between Statistics and Medical Practice,"One of the interesting questions in statistics is how to handle the duality of developing methodology that is sound and also easily interpretable in an applied space. Particularly in medical research there may be an understanding that a problem has been simplified for easy testing or power. These simplified solutions become the accepted standard of practice. As methodologies and capacity grow it is worth revisiting these simplifications to develop the methodology to better align with the goal. For the statistician there is an opportunity to either aim for a robust approach and present a new method, or new measurement, that addresses these simplifications or work to an incremental approach that steps the applied user closer to the goal over time. We will work through an example from the composite endpoints used in cardiovascular clinical trials. In these trials fatal and non-fatal events are combined into a single analysis for regulatory approval.",Jeffrey A. Bakal Alberta Health Services,Monday 10:20-11:05,N
Some Experiences From an Industry Collaboration,"A successful industry collaboration, which has spanned around eight years and received support from NSERC, OCE, and MITACS, is described. Details of many aspects of the collaboration are discussed, including funding opportunities that were exploited. Special attention is paid to some of research that took place as part of the collaboration and tangentially to it.",Paul D. McNicholas McMaster University,Monday 11:05-11:50,N
 Statistical Disclosure Control for Statistical Outputs: Where Do We Go From Here? ,"I provide an overview of types of disclosure risks in traditional forms of data dissemination by statistical agencies, how the risk and information loss can be quantified and some recommended applications of disclosure control techniques. Statistical agencies, however, are making more use of restricting and licensing data as a statistical disclosure control approach given the difficulties in producing safe data. We look at new strategies for data dissemination and discuss their potential and limitations. These include web-based applications where the outputs are protected without the need for human intervention to check for disclosure risks, on-site data enclaves and its extension where researchers can access confidential data on their personal PCs through a remote server and the production and release of ‘synthetic’ data based on advanced statistical models. The link to the Computer Science definition of differential privacy will be discussed.",Natalie Shlomo University of Manchester,Monday 10:20-10:50,N
What is Synthetic about Synthetic Datasets?,"When synthetic datasets were proposed as a way to protect the confidentiality of respondents, the reasoning was that since they are not about anyone in the real dataset, they do not pose any risk. Our understanding of the risk of these datasets has however evolved, and we now know that they are not risk free. Nevertheless, synthetic datasets can be a useful tool for privacy protection and there is now a growing literature on the best techniques for creating and analysing synthetic datasets. But this has lead to the term synthetic dataset being used in a lot of different contexts, and it is not clear anymore what the essence of a synthetic dataset is. What really differentiates a synthetic dataset from a perturbed real dataset? This talk explores the main attributes of synthetic datasets and offers some thoughts for thinking and talking about them.",Anne-Sophie Charest Université Laval,Monday 10:50-11:20,N
Disclosure Control and Random Tabular Adjustment ,"Statistical agencies are interested in publishing useful statistical data but doing so may lead to the disclosure of individuals' private data. This is a problem, as it leads to a trade-off between the utility of the published data and the risk of disclosure of confidential data. Disclosure control can be seen as the use of methods to deal with this problem by assessing and controlling the risk of disclosing confidential data while also providing researchers with useful statistical data. This paper describes a disclosure control model based largely on Bayesian decision theory. This model allows for the description of the concepts of disclosure control in terms of familiar statistical concepts such as expectation and variance. A method of disclosure control, called random tabular adjustment (RTA), is described. This method controls the risk of disclosure by randomly adjusting the data instead of suppressing cells. It fits naturally into the disclosure control model described.",Mark Stinner Statistics Canada,Monday 11:20-11:50,N
A Potts Mixture Spatiotemporal Joint Model for Combined MEG and EEG Data,"We consider the ill-posed inverse problem arising when magnetoencephalography (MEG) and/or electroencephalography (EEG) are used to measure electromagnetic brain activity over an array of sensors at the scalp and it is of interest to map these data back to the sources of neural activity within the brain. We review existing approaches to solving this inverse problem and discuss the mesostate-space model (MSM) proposed by Daunizeau and Friston (Neuroimage, 2007). We then propose a new model that builds on the MSM and incorporates three major extensions: (i) We combine EEG and MEG data together and formulate a joint model for source reconstruction; (ii) we incorporate the Potts model to represent the spatial dependence in an allocation process that partitions the cortical surface into a small number of latent mesostates; (iii) we formulate the mesostate dynamics in a more flexible manner so that the model can characterize the functional connectivity between mesosources. We formulate the model, discuss computational implementation, and make comparisons to existing methods.","Farouk Nathoo University of Victoria, Yin Song University of Victoria",Monday 10:20-10:42,N
Spatially Structured Sparseness in Bayesian Spatial Health Modeling,Often geospatial disease outcomes are characterized by sparseness when viewed as a count distribution. This is sometimes called zero-inflation. Classically the models for zero inflation are of two types: zero class modelled as ‘structural’ or ‘Poisson’ or as hurdle models where the zeroes are treated completely separately from the truncated Poisson positive counts. Often the idea of structural zeroes is not made specific in that no attempt is made to assess which zeroes are in that class. In this talk I describe an approach which models the spatial structure of the structural zero class and provides estimates of the spatial distribution of the probability of being structural or not. This is applied in a spatio-temporal context. Different assumptions about the spatial prior distribution of the structural probability are compared using a Bayesian formulation. An example of spatio-temporal modeling of sudden infant death in the counties of Georgia USA will be presented.,"Andrew Lawson Medical University of South Carolina, Ray Boaz Medical University of South Carolina",Monday 10:42-11:04,N
Continuous Models for Aggregated Spatio-Temporal Data in Practice,"Dr. Brown and his collaborators have presented the local-EM algorithm for smoothing aggregated data a number of times at past SSC conferences, although the methodology has been too computationally intensive (and labour intensive) to attract a following. This year's talk will change the situation with the presentation of an R package and a description of the methodological and computational improvements which have made the method useful in practice. Efficient use of R's spatial data structures and parallel processing has decreased to the computational burden of the method to a manageable level. Re-casting the algorithm as modelling a latent-Gaussian process rather than a kernel smoother has allowed for a sparse matrix Markov random field approximation for the covariance matrix. An R package includes several examples using real datasets. ","Patrick E. Brown St. Michael's Hospital, Ofir Hariri University of Toronto, Alisha Albert-Green University of Toronto, Jamie Stafford University of Toronto, Paul Nguyen Queen's University, Lennon Li Public Health Ontario",Monday 11:04-11:26,N
Estimating Cross-validatory Predictive P-values with Integrated Importance Sampling for Disease Mapping Models,"An important statistical task in disease mapping problems is to identify divergent regions with unusually high or low risk of disease. Leave-one-out cross-validatory (LOOCV) model assessment is the gold standard for estimating predictive p-values that can flag such divergent regions. However, LOOCV is time-consuming. We introduce a new method, called integrated importance sampling (iIS), for estimating LOOCV predictive p-values with only Markov chain samples drawn from the posterior based on a full data set. The formula used by iIS can be proved to be equivalent to the LOOCV predictive p-value. We compare iIS and three other existing methods in the literature with two disease mapping datasets. Our empirical results show that the predictive p-values estimated with iIS are almost identical to the predictive p-values estimated with actual LOOCV, and outperform those given by the existing three methods, namely, the posterior predictive checking, the ordinary importance sampling, and the ghosting method by Marshall and Spiegelhalter (2003).","Longhai Li University of Saskatchewan, Xin (Cindy) Feng University of Saskatchewan, Shi Qiu International Road Dynamics",Monday 11:26-11:50,N
From Brain to Hand to Statistics with Dynamic Smoothing,"Systems of differential equations are often used to model buffering processes that modulate a non-smooth high-energy input so as to produce an output that is smooth and that distributes the energy load over time and space. Handwriting is buffered in this way. We show that the smooth complex script that spells ``statistics"" in Chinese can be represented as buffered version of a series of 46 equal-interval step inputs. The buffer consists of three undamped oscillating springs, one for each orthogonal coordinate. The periods of oscillation vary slightly over the three coordinate in a way that reflects the masses that are moved by muscle activations. We use the term ``dynamic smoothing"" for the estimation of a structured input functional object along with the buffer characteristics.","James O. Ramsay McGill University, Michelle Carey University College Dublin, Juan Li McGill University",Monday 10:20-10:50,N
Approximations to Facilitate Data Assimilation for High Dimensional Dynamical Systems,"A major challenge for inference in spatio-temporal dynamical systems is the high dimension of the simulation models used; model integration, or forward simulation, is extremely computationally costly. Ensemble–based data assimilation has emerged as a method of choice for state estimation for environmental prediction based on numerical models of the ocean, climate, and atmosphere. There is also interest in parameter estimation and uncertainty quantification (UQ) for these problems. In this talk, I will discuss approximate approaches to facilitate state and parameter estimation, as well as UQ, for these computationally costly systems. This overview will include ensemble Kalman filters, and various approximations for particle filters. The use of emulators is also discussed. The approaches are illustrated using data assimilation problems in oceanography.",Mike Dowd Dalhousie University,Monday 10:50-11:20,N
"Challenges and Strategies for Inference on a Stochastic Multi-Pathogen Model of Disease Dynamics in San Luis Potosi, Mexico ","Acute respiratory diseases (ARI) are a public health concern associated with morbidity and mortality, especially in children and the elderly. The nature of individual interactions mean that disease dynamics in a large population may be described by a stochastic process model governed by a system of stochastic differential equations (SDE). When multiple ARI are in circulation, the dynamics are further complicated, and the corresponding SDE cannot be solved analytically. In this analysis we examine the challenges of inference for models of multiple pathogen dynamics, including the inability to distinguish different pathogens based on symptoms and the partial symmetries in the model. We discuss how to overcome resulting identifiability problems by utilizing additional data sources and modeling data from multiple outbreak seasons simultaneously.","Oksana Chkrebtii Ohio State University, Yury E. Garcia Centro de Investigacion en Matematicas, Marcos A. Capistran Centro de Investigacion en Matematicas",Monday 11:20-11:50,N
Causal Inference with Measurement Error in Outcomes: Bias Analysis and Estimation Methods,"The average treatment effect (ATE) is often of primary interest in causal inference, and it is commonly estimated based on the inverse probability weighting (IPW) method. Its validity, however, is challenged by the presence of error-prone outcomes. In this paper we investigate various issues concerning IPW estimation of ATE with mismeasured outcome variables. We study the impact of measurement error and reveal important consequences of the naive analysis which ignores measurement error. When a continuous outcome variable is mismeasured under an additive measurement error model, the naive analysis still yields a consistent estimator; when the outcome is binary, we derive the asymptotic bias in a closed-form. To conduct valid inference, we develop estimation procedures for practical scenarios where either validation data or replicates are available. Furthermore, with validation data we propose an efficient method which substantially outperforms usual methods of using validation data.","Di Shu University of Waterloo, Grace Yi University of Waterloo",Monday 10:20-10:35,N
A Unified Empirical Likelihood Approach to Testing MCAR and Subsequent Estimation,"For estimation with missing data, a crucial step is to determine if the data are missing completely at random (MCAR), in which case a complete-case analysis would suffice. Most existing tests for MCAR do not provide a method for subsequent estimation once the MCAR is rejected. In the setting of estimating the means of some response variables that are subject to missingness, we propose a unified approach to testing MCAR and the subsequent estimation. Upon rejecting MCAR, the same set of weights used for testing can then be used for estimation. The resulting estimators are consistent if the missingness of each response variable depends on a set of fully observed auxiliary variables only and the true outcome regression model is among the user-specified functions for deriving the weights. The proposed procedure is based on the calibration idea from survey sampling literature and the empirical likelihood theory. This is joint work with Peisong Han and Changbao Wu.","Shixiao Zhang University of Waterloo, Peisong Han University of Waterloo, Changbao Wu University of Waterloo",Monday 10:35-10:50,N
Non-regular Inference for Dynamic Weighted Ordinary Least Squares: Understanding the Impact of Solid Food Intake in Infancy on Childhood Weight,"A dynamic treatment regime (DTR) is a set of decision rules to be applied across multiple stages of treatments. The decisions are tailored to individuals, inputting an individual's observed characteristics and outputting a treatment decision at each stage for that individual. Dynamic weighted ordinary least squares (dWOLS) is a theoretically robust and easily implemented method for estimating an optimal DTR. Like many DTR estimators, the dWOLS estimators can be non-regular when true treatment effects are zero or very small, resulting in invalid Wald-type or standard bootstrap confidence intervals. Inspired by an analysis of the effect of diet in infancy on weight and body size in childhood, we investigate the use of the m-out-of-n bootstrap with dWOLS for valid inferences of optimal DTR. We provide an extensive simulation study to compare the performance of different choices of resample size m in situations where the treatment effects are non-regular. We illustrate the methodology to study the effect of solid food intake in infancy on long-term health outcomes.","Gabrielle Simoneau McGill University, Erica E.M. Moodie McGill University, Robert W. Platt McGill University, Bibhas Chakraborty Duke-NUS Medical School, National University of Singapore",Monday 10:50-11:05,N
Kernel-Based Causal Inference,"An important goal in estimating the causal effect is to achieve balance in the covariates. We propose using kernel distance to measure balance among different treatment groups and propose a new propensity score estimator by setting the kernel distance to be zero. The estimating equations are solved by generalized method of moments. Simulation studies are conducted across different scenarios varying in the degree of nonlinearity. The simulation study shows that the proposed approaches produce smaller mean squared errors in estimating causal treatment effects than many existing approaches including the well-known covariate balance propensity score (CBPS) approach. Further, it is shown that the kernel distance is one of the best bias indicators in estimating the causal effect compared to other balance measures, such as absolute standardized mean difference (ASMD) and KS statistic. ","Yuying Xie University of Waterloo, Yeying Zhu University of Waterloo, Cecilia Cotton University of Waterloo",Monday 11:05-11:20,N
Incorporating Medians in Meta-Analysis,"In meta-analysis of a continuous outcome, usually studies that report medians are excluded or the median and spread are used to estimate a mean and standard deviation in order to estimate a pooled effect across studies. In our previous work, we proposed several methods to directly meta-analyze medians. We showed via simulation that these methods outperform methods that estimate means and standard deviations from studies reporting medians, and we provided guidelines for data analysts to improve the performance of meta-analyzing studies that report medians. We apply these novel methods to recently published meta-analyses and evaluate the extent to which the pooled estimates and conclusions drawn from these meta-analyses change when using these methods.","Sean McGrath McGill University, Russell Steele McGill University, Andrea Benedetti McGill University",Monday 11:20-11:35,N
Use of the Segmented Regression Analysis Approach to Adjust for Potential Confounders and Effect Modifiers when Evaluating Public Health Interventions,"The segmented regression analysis approach is a useful method that may be used to evaluate effects of public health interventions. This approach may be superior to standard time-series analyses because it allows for short and long-term effects evaluation, while adjusting for potential confounders and assessing study subject characteristics (like socioeconomic status [SES]) that may influence these effects. We used the segmented regression approach to study if implementation of a pediatric diabetes network in Ontario decreased SES-disparities in emergency (ED) visits. We used population-based aggregate data of annual crude rates for each level of SES and potential confounders: geographic location, sex and age group. We determined population average adjusted ED-visit rate ratios by accounting for the number of individuals in each aggregate using generalized estimating equation with a Poisson link. We observed that, after network implementation, ED-visits decreased significantly in the lowest SES quintiles. We concluded that the segmented regression analysis approach may be used to analyse aggregate ecological data while adjusting for potential confounders and effect modifiers.","Marc Simard Institut national de santé publique du Quebec, Nathalie Vandal Institut National de Santé Publique du Québec, Meranda Nakhla Research Institute of the McGill University Health Centre and McGill University, Elham Rahme Research Institute of the McGill University Health Centre, Astrid Guttmann Hospital for Sick Children, University of Toronto, Institute of Health Policy, Management and Evaluation, and Institute for Clinical Evaluative Sciences",Monday 11:35-11:50,N
Tail Empirical Processes for Stochastic Volatility Models,"We consider the tail empirical process (TEP) related to a distribution with a regularly varying tail. This is an important tool used in nonparametric estimation of extremal quantities, like the Hill estimator of the index of regular variation, or various risk measures. In this talk, we consider a long memory stochastic volatility model of interest in finance. We first start by investigating some probabilistic properties of this model. We establish central and non-central limit theorems for the TEP and apply these results to investigate the asymptotic behaviour of the aforementioned extremal quantities. Our theoretical results are illustrated by simulation studies. ","Clemonell Bilayi-Biakana University of Ottawa, Rafal Kulik University of Ottawa, Gail Ivanoff University of Ottawa",Monday 10:20-10:35,N
Modeling Recurrent Gap times Through Conditional GEE,"In this paper, we present the analysis of recurrent event data subject to censoring using generalized estimating equations for the conditional means and variances of the gap times. Censoring is dealt with by imposing conditional independence assumptions on the censored gap times, the covariates and the censoring times. Simulation results demonstrate the relative robustness of parametric estimates even when this assumption is incorrect. We actually estimate the censored gap time using only the observed data, by solving estimating equations that are asymptotically unbiased. We prove the strong consistency using modern analytical techniques and illustrate our results through simulations. ","Hai Yan Liu University of Ottawa, Ioana Schiopu-Kratina University of Ottawa, Pierre-Jerome Bergeron University of Ottawa, Mayer Alvo University of Ottawa",Monday 10:35-10:50,N
New approach to GIX/Geom/1 Queues involving Heavy-Tailed distributions,"We provide an analytically simple and computationally efficient solution to the discrete-time queueing model GIX /Geom/1. Using the imbedded Markov chain method, the analysis has been carried out for the late-arrival system and the results for the early-arrival system have been derived from those of the late-arrival system. The probability distributions of the numbers in the system at various epochs are all found in terms of roots of the underlying characteristic equation. Numerical results are also discussed. ","James Kim Royal Canadian Air Forces, Mohan Chaudhry Royal Military College of Canada",Monday 10:50-11:05,N
On Convergence of Sample Correlation Matrices in High-Dimensional Data,"In this talk, we consider an estimation problem concerning the matrix of correlation coefficient in context of high dimensional data settings. In particular, we revisited some results in Li and Rolsalsky [Li, D. and Rolsalsky, A. (2006). Some strong limit theorems for the largest entries of sample correlation matrices, The Annals of Applied Probability, 16, 1, 423-447]. We extend one of the findings in Li and Rosalsky (2006) and, we simplify remarkably the proof of one of the result of the quoted paper. Further, we generalize a theorem which is useful in deriving the existence of the pth moment as well as in studying the convergence rates in law of large numbers.","Yueleng Wang University of Windsor, Sévérien Nkurunziza University of Windsor",Monday 11:05-11:20,N
Importance Sampling Techniques for Single Index Models,"In rare-event simulation, a plain Monte Carlo estimator is bound to be imprecise since the samples from the underlying distribution has a low chance of landing on the important region. Importance sampling (IS) is a popular variance reduction technique in these situations. IS draws samples from a proposal distribution which is constricted in such a way that it places heavier weights on the important region. The design of the effective proposal distribution, however, is generally hard when many variables are involved as it becomes harder to characterize which part of the domain corresponds to the important region. Fortunately, any high-dimensional problems have a low-dimensional structures. In many cases, one-dimensional projection (single index) of the input vector captures a large majority of overall variance. Our IS technique exploits such a low-dimensional structure of those problems and constructs an efficient proposal distribution. ","Yoshihiro Taniguchi University of Waterloo, Christiane Lemieux University of Waterloo",Monday 11:20-11:35,N
A Comparison of Mixture Modeling Approaches for Clustering Longitudinal Binary Data,"Mixture modelling has been increasingly used in clinical and epidemiological research for clustering patients into different developmental trajectories. Many previous studies have been concerned with continuous data, whereas only a limited number of studies focus on evaluating appropriate approaches for clustering binary data. Using simulated data, this study aims to investigate several mixture modeling approaches, namely the standard latent class model, the growth mixture model, the K-means mixture model, and a Bayesian latent class model. Various scenarios with differing sample sizes, number of classes, class sizes, and development trajectories shapes are considered. We focus on evaluating the performance of these approaches in recovering the true class membership and development trajectories for longitudinal binary responses. For illustration, we also present a case study using real data. ",Zihang Lu University of Toronto,Monday 10:20-10:35,N
Quantile and Expectile Regression for Random Effects Models,"Quantile and expectile regression models pertain to the estimation of unknown quantiles/expectiles of the cumulative distribution function of a dependent variable as a function of a set of covariates and a vector of regression coefficients. Both approaches make no assumption on the shape of the distribution of the response variable, allowing for investigation of a comprehensive class of covariate effects. We develop methods for panel data within a random effects framework. We provide asymptotic properties of the underlying model parameter estimators and suggest appropriate estimators of their variances-covariances matrices. The performance of the suggested estimators is evaluated through simulation studies and the methodology is illustrated using real data. The simulations show that expectile regression is comparable to quantile regression, easily computable and has relevant statistical properties. In conclusion, expectiles are to the mean what quantiles are to the median, and should be used and interpreted as quantilized mean.","Amadou Diogo Barry Université du Québec à Montréal, Karim Oualkacha Université du Québec à Montréal, Arthur Charpentier Université de Rennes",Monday 10:35-10:50,N
Component Selection and Estimation for Functional Semiparametric Additive Model,"We propose a functional semiparametric additive model which incorporates the effects of functional covariates and scalar covariates. More specifically, the effect of a functional covariate is modeled nonparametrically while a linear form is adopted to model the effects of the scalar covariates. This strategy can enhance flexibility in modelling the effect of the functional covariate while maintaining interpretability for the effects of scalar covariates. Additionally, an estimation method is developed to smooth and select non-vanishing components when estimating the nonparametric components. The coefficients for the scalar covariates can be obtained simultaneously by our proposed method. The finite sample performance of our proposed model and the corresponding estimation method are compared with various methods using simulation studies. Our proposed method is demonstrated by a real application of predicting the protein content in meat samples using Tecator absorbance spectra and fat and water contents of the meat samples.","Peijun Sang Simon Fraser University, Jiguo Cao Simon Fraser University, Richard Lockhart Simon Fraser University",Monday 10:50-11:05,N
On Copula-Based Conditional Quantile Estimators,"Recently, two different copula-based approaches have been proposed to estimate the conditional quantile function of a variable Y with respect to a vector of covariates X: the first estimator is related to quantile regression weighted by the conditional copula density, while the second estimator is based on the inverse of the conditional distribution function written in terms of margins and the copula. Using empirical processes, we show that even if the two estimators look quite different, they converge to the same limit. We also propose a bootstrap procedure for the limiting process in order to be able to construct uniform confidence bands around the conditional quantile function. A case study based on hydro-climatic data illustrates the proposed methodology.","Bouchra Nasri HEC Montreal, Bruno Rémillard HEC Montréal, Taoufik Bouezmarni Université de Sherbrooke",Monday 11:05-11:20,N
Trajectory Modeling of Gestational Weight: a Functional Principal Component Analysis Approach,"Suboptimal gestational weight gain (GWG), which is linked to increased risk of adverse outcomes for a pregnant woman and her infant, is prevalent. In a large cohort study of Canadian pregnant women, our goals are to estimate the individual weight growth trajectory using sparsely collected data, and to identify the factors affecting the total GWG. The first goal was achieved through functional principal component analysis (FPCA) by conditional expectation. For the second goal, we used linear regression with the total weight gain as the response variable. The trajectory modeling through FPCA had significantly smaller mean square error and improved adaptability than the traditional nonlinear mixed-effect models, demonstrating a novel tool to facilitate real-time monitoring and interventions of GWG. Our regression analysis showed that prepregnancy body mass index (BMI) had a high predictive value for the total GWG, which agrees with the published weight gain guideline.","Menglu Che University of Waterloo, Linglong Kong University of Alberta, Rhonda Bell University of Alberta, Yan Yuan University of Alberta",Monday 11:20-11:35,N
Ordinal Regression for the Analysis of Health Utility Data,"Health utility (HU) data are negatively skewed and bounded by 1, with most observations lying close to that bound and some with extremely low levels. HU has been analyzed by linear regression (OLS) which assumes normally distributed errors and lacks a restriction at the upper limit; Ordinal Regression (OR) is not limited by such assumptions and it produces estimates bounded at 1. This study compares the performance of OR to OLS for the prediction of HU given covariates, using simulated and real data of prostate cancer patients. Using various sample sizes and three different distributions, HU data were simulated and analyzed by OLS and OR. Models were evaluated by the bias and coverage probability of the estimated mean, and by using the Root Mean Square Error (RMSE) comparing simulated and predicted HU values. Results show that OR provides more accurate estimates regardless of sample size, while OLS bias increases as HU approaches zero. Coverage probability is similar in both methods.","Myrtha E. Reyna Vargas University of Toronto, Nicholas Mitsakakis University of Toronto",Monday 11:35-11:50,N
Confidence Estimating Functions for Data Integration,"The theory of statistical inference along with the strategy of divide-and-combine for large-scale data analysis has recently attracted considerable interest due to the popularity of the MapReduce scheme. The key to the development of statistical inference lies in the method of combining results yielded from separately mapped data batches. We consider a general inferential methodology based on estimating functions, which allows us to perform regression analyses of massive complex data via the MapReduce scheme, such as longitudinal data, survival data and quantile regression, which cannot be done using the maximum likelihood method. The proposed statistical inference inherits many key large-sample properties of estimating functions. Also we show that the proposed method is closely connected to the generalized method of moments (GMM). Our method provides a unified framework for many kinds of statistical models and data types, which is illustrated via numerical examples in both simulation studies and real-world data analyses. ","Peter X. Song University of Michigan, Ling Zhou University of Michigan",Monday 13:30-14:00,N
Inference from Survival Data using Imprecise Probabilities,"Imprecise probability (IP) is a generalization de Finetti's approach to probability, formalized by Williams (1975) and extensively discussed in a monograph by Walley (1991), who presented these ideas in a JRSS discussion paper (1996). The methodology can be interpreted as Bayesian sensitivity analysis using a set of priors. In his 1996 presentation, Walley introduced a practical form for this inference on discrete data by using a family of Dirichlet priors with a fixed concentration parameter. He proposed that it could also be used for survival data by discretization. Coolen (1997) extended Walley's model to allow for censoring, and Coolen and Yan (2004) proposed a nonparametric predictive inference paradigm which also led to IP conclusions. Bickis (2009) used IP concepts to estimate the hazard function. After reviewing the methodology, we will be presenting an IP version of the log-rank test, which will produce a sequence of imprecise posteriors updated at the time of each death.","Mikelis Guntars Bickis University of Saskatchewan, Naeima Ashleik University of Saskatchewan, Juxin Liu University of Saskatchewan",Monday 14:00-14:30,N
Bayesian Adaptive Designs for Efficient Drug Development and Precision Oncology,"Clinical trial is a prescribed learning process. Bayesian methods take the “learn as we go” approach and are uniquely suitable for such learning. In recent years, rapid advancements in medicine demand innovative methods to identify better therapies and the most appropriate population in a timely and efficient way. I will first illustrate the concept of Bayesian update and Bayesian inference, then, give an overview of Bayesian adaptive designs in dose finding, predictive probability, multi-arm platform design, and outcome adaptive randomization, etc. Applications including BATTLE trials in lung cancer and I-SPY trials in breast cancer will be given. Bayesian adaptive designs increase the study efficiency, allow more flexible trial conduct, and treat more patients with more effective treatments in the trial but also possess desirable frequentist properties. Perspectives will be given on future development in trial design, conduct, and evaluation to streamline and speed up drug approval.",J. Jack Lee University of Texas MD Anderson Cancer Center,Monday 14:30-15:00,N
Spatio-Temporal Noise Models for the Analysis of Functional Magnetic Resonance Imaging Data,"Modern functional magnetic resonance imaging (fMRI) techniques provide measurements of ongoing activity in the whole human brain at an unprecedented spatial (1mm) and temporal (1s) resolution. Unfortunately, the measurements are corrupted by substantial noise, arising both from intermittent artifacts such as subject motion, as well as from non-neural physiological processes. Latter processes lead to characteristic low-frequency drifts in the times-series data. Noise processes also show a characteristic spatial structure that depends substantially on the underlying neuroanatomy. It has become clear that many current noise models implemented in commonly used fMRI analysis packages lead to suboptimal estimation of model parameters and do not provide adequate statistical inference. I will show some of the developments in our lab to improve spatio-temporal noise models and will demonstrate their impact on single-voxel analysis, multi-variate pattern analysis, and whole-brain functional connectivity analysis.",Jörn Diedrichsen University of Western Ontario,Monday 13:30-14:00,N
Change Point Detection in Functional Time Series Models for Yield Curves,"Yield curves are functions defined on time to maturity with corresponding values equal to yield (interest) on a bond, typically a standardized government issued instrument. Yield curves are commonly used to predict future states of the economy on the basis of the interest investors demand for government debt of various maturities. These curves form a time series of functions, one function per day. The talk will discuss methods of detecting a change point in the mean function of such a functional time series. After reviewing recent research, we will present two methods: one which uses a factor representation of the yield curves, the other a fully nonparametric method. Both methods permit the second order structure to change independently of the changes in the mean structure. Based on the asymptotic theory, two numerical approaches to the implementation of the tests will be presented and compared. Application to US Federal Reserve yield curves will be presented.",Piotr Kokoszka Colorado State University,Monday 14:00-14:30,N
Change-Point Detection in Time Series using Ordinal Patterns ,"We propose new concepts in order to analyse and model the dependence structure between two time series. Our methods rely exclusively on the order structure of the data points. Hence, the methods are stable under monotone transformations of the time series and robust against small perturbations or measurement errors. Ordinal pattern dependence can be characterised by four parameters. We propose estimators for these parameters, and we calculate their asymptotic distributions. Furthermore, we derive a test for structural breaks within the dependence structure. All results are supplemented by simulation studies and empirical examples. ","Herold Dehling Ruhr University, Alexander Schnurr University of Siegen",Monday 14:30-15:00,N
Designing and Launching a Professional Master’s Program in Data Science: Tales from the UBC Experience,"The University of British Columbia recently launched a professional master’s program in Data Science. The first cohort of students began their studies in September 2016. In this talk I will describe this program, and some of the challenges and successes encountered thus far. Points of emphasis will include our thinking on the choice of topics to include, the choice of prerequisites, and the choice of delivery methods. Since our students will be undertaking capstone projects throughout May and June 2017, I also plan to give a ``live update’’ on how this is working.","Paul Gustafson University of British Columbia, Jenny Bryan University of British Columbia, Giuseppe Carenini University of British Columbia, Vincenzo Coia University of British Columbia, Giulio V. Dalla Riva University of British Columbia, Michael A. Gelbart University of British Columbia, Gail C. Murphy University of British Columbia, Raymond T.  Ng University of British Columbia, Tiffany A. Timbers University of British Columbia",Monday 13:30-14:15,N
Data Science Programs at the University of Waterloo,"In this talk I will describe the new undergraduate and Masters' Data Science programs at the University of Waterloo. I will discuss both programs’ genesis, motivation, target audience as well as challenges encountered along the way. This session will conclude with discussion of Data Science generally and the issues and questions in starting a Data Science program. ",Stefan Steiner University of Waterloo,Monday 14:15-15:00,N
Event History Analysis of Data from Non-Ignorable Observation Schemes,"Observational databases on events related to health and disease processes are widely used to study human health, but can pose challenges for rigorous analysis. These are due in part to selection effects related to inclusion in a database and to missing information on important factors. In addition, the times at which data are observed on an individual may be related to outcomes and covariates under study. Such selection and observation processes usually include elements that are non-ignorable, and failure to adjust for them can produce biased analysis. This talk will discuss likelihood and weighted estimating function methods for dealing with non-ignorable observation processes. Applications to the analysis of data from a psoriatic arthritis cohort will be considered. ","Yayuan Zhu University of Texas MD Anderson Cancer Center, Jerald F. Lawless University of Waterloo",Monday 13:30-14:00,N
Risk Classification and Prediction with Administrative Health Data,"The population of cancer survivors has been increasing rapidly as a result of advances in cancer treatment. Consequently there has been greatly increased interest in evaluating late mortality and morbidity of survivors and their health care utilization. This talk presents strategies for risk classification and prediction with the counts of physician visits and the associated costs of cancer survivors from administrative health databases. We illustrate the methodology using the physician claims associated with the subjects in the CAYACS survivor cohort with the BCCA (Cancer Agency of British Columbia, Canada). ","X. Joan Hu Simon Fraser University, Huijing Wang Apple Inc., John Spinelli BC Cancer Agency",Monday 14:00-14:30,N
Design and Relative Efficiency of Tracing Studies in Cohorts with Loss to Followup,"Considerable investments are being made to conduct large-scale cohort studies which involve routine followup of participants. Loss-to-followup is common particularly when the study duration is long. Standard likelihood methods ignoring the missing data yield consistent but less efficient estimators under the Sequential Missing at Random (SMAR), whereas bias arises when this assumption is violated unless the missing data are modelled correctly. Tracing the disease status of those lost to followup help to address the missing data problem and lead to potential efficiency gain, however relatively little attention has been given to the design of such studies. This work proposes selection models for tracing based on the history of the disease process, and such models are optimized to achieve the maximum efficiency gains under a fixed sample size or budget. An extension to develop an adaptive two-phase tracing design will also be discussed for the loss-to-followup under non-SMAR.","Leilei Zeng University of Waterloo, Nathalie Moon University of Waterloo, Richard J. Cook University of Waterloo",Monday 14:30-15:00,N
Some Asymptotic Results on Tree-based Survival Models,"There are only a handful of results available for tree-based survival models. We investigate the method from the perspective of splitting rules, where the log-rank test statistics is calculated and compared. The splitting rule is essentially treating both resulting child nodes as being identically distributed. However, we demonstrate that this approach is affected by censoring, which may lead to inconsistency of the method. Based on this observation, we develop an adaptive concentration bound in the sense that for each terminal node, the estimation centers around the true within-node average of the underlying survival function, while this quantity may again be affected by the censoring distribution. We then show that the consistency of the method by can be achieved if the splitting rule is modified to satisfy certain restrictions.","Ruoqing Zhu University of North Carolina at Chapel Hill, Yifan Cui University of North Carolina at Chapel Hill, Mai Zhou University of Kentucky, Michael Kosorok University of North Carolina at Chapel Hill",Monday 13:30-14:00,N
Expanding the Predictive Ability of an Interpretable Tree,"We recently developed partDSA, a multivariate method that, similarly to CART, utilizes loss functions to select and partition predictor variables to build a tree-like regression model for a given outcome. However, unlike CART, partDSA permits both 'and' and 'or' conjunctions of predictors, elucidating interactions between variables as well as their independent contributions. partDSA thus permits tremendous flexibility and provides an ideal foundation for developing a clinician-friendly tool for accurate risk prediction and stratification. An interesting extension of partDSA is as an aggregate learner (Olshen et al. (2017)). In this talk, we describe an approach to combining the predictive ability of an aggregrate learner with the clinical utility of a single tree. The method is illustrated via a data analysis for cancer patients based on various clinical and biomarker covariates. ","Annette M. Molinaro University of California, San Francisco, Adam Olshen UCSF, Robert Strawderman University of Rochester",Monday 14:00-14:30,N
Tree-Based Approaches for Personalized Risk Prediction with Longitudinal Biomarkers: An Application to Fetal Growth,"Longitudinal monitoring of biomarkers is often helpful for predicting disease or a poor clinical outcome. Much research on longitudinal biomarkers has been on the overall accuracy in predicting an outcome. However, while good predictive accuracy may be found only in a subgroup of the population, identification of such subgroups can guide the design of follow-up schemes in future studies. In the Fetal Growth Study, we consider the prediction of both large and small for gestational age births by using longitudinal ultrasound measurements, and we attempt to identify subgroups of women for whom prediction is more (or less) accurate, should they exist. We propose tree-based approaches to identifying such subgroups, and a pruning algorithm that explicitly incorporates a desired type I error rate, allowing us to control the risk of false discovery of subgroups. The methods proposed are applied to data from the Scandinavian Fetal Growth Study and are evaluated via simulations.","Danping Liu National Institutes of Health, Jared C. Foster Eunice Kennedy Shriver National Institute of Child Health and Human Development and Mayo Clinic, Paul S. Albert Eunice Kennedy Shriver National Institute of Child Health and Human Development and National Cancer Institute, Aiyi Liu Eunice Kennedy Shriver National Institute of Child Health and Human Development",Monday 14:30-15:00,N
Small Area Estimation with Covariate Measurement Error,"Small area estimation (SAE) has become a very active area of research in statistics. Many models studied in SAE focus on one or few variables of interest from a single survey without paying close attention to the treatment of covariates. It is useful to utilize the idea of “borrowing strength” from covariates to build a model which combines two (or multiple) surveys. In many real applications, there are also covariates measured with errors. In this talk, we study a nested error linear regression model with combining two surveys which has multiple unit level error-free covariates and multiple area level covariates subject to measurement error. In particular, we derive an empirical best predictor of a small area mean with corresponding mean squared prediction error estimation. The performance of the proposed approach is studied through simulation studies and also by a real application. ","Mahmoud Torabi University of Manitoba, Gauri Datta University of Georgia, J.N.K. Rao Carleton University",Monday 13:30-14:00,N
Non-response Follow-up for Business Surveys,"Follow-up of nonrespondents in business surveys is a time and resource intensive activity. Given the decline in response rates, non-response follow-up takes on more and more importance to ensure continued quality of estimates produced. Given a fixed budget to follow-up non-responding units, what is the best way to select units for non-response follow-up in business surveys? Should all non-respondents be followed up or just a sample of them? If a sample is followed-up, how should it be selected? Should the sample be selected using simple random sampling (SRS), stratified SRS or probability proportional to size (PPS) sampling? These questions were addressed in two ways. Firstly, a simulation study compared the Monte Carlo biases and mean square error using data from an existing survey. Secondly, the exact follow-up sample size for a follow-up using simple random sampling and assuming uniform response rates was developed. ","Wesley Yung Statistics Canada, Mike Hidiroglou Statistics Canada, Elisabeth Neusy Statistics Canada",Monday 14:00-14:30,N
Simplified Variance Estimation in Surveys,"Variance estimation presents challenges in the context of complex survey designs. Conventional variance estimators rely on second-order inclusion probabilities that can be difficult to compute for some sampling designs. Based on a procedure proposed by Ohlsson (1998) in the context of sequential Poisson sampling, we suggest a simplified variance estimator that requires only the first-order inclusion probabilities. The idea is to approximate an estimation strategy (which consists of a sampling design and a point estimator) by an equivalent strategy based on Poisson sampling. Proportional-to-size sampling designs and cluster designs will discussed. Results of a simulation study will be presented. ","David Haziza Université de Montréal, Jean-François Beaumont Statistics Canada, Isabelle Lefebvre Université de Monréal",Monday 14:30-15:00,N
Pareto-optimal Reinsurance with Two Constraints under Distortion Risk Measure,"We study the Pareto-optimal reinsurance policies, where both the insurer's and the reinsurer's risks and returns are considered. We assume that the insurer and the reinsurer measure their risks by some distortion risk measures with possibly different distortion operators. In addition, the reinsurance premium is determined by a distortion risk measure. In our analysis, we suppose that a reinsurance policy is feasible only if the resulting risk of each party is below some pre-determined values. Methodologically, we show that the generalized Neyman-Pearson method, the Lagrange multiplier method, and the dynamic control methods can be utilized to solve the optimization problem with constraints. Special cases when both parties' risks are measured by Value-at-Risk (VaR) and Tail Value-at-Risk (TVaR) are studied in great details. An numerical example is provided to illustrate practical implications of the results.","Wenjun Jiang University of Western Ontario, Jiandong Ren University of Western Ontario, Hanping Hong University of Western Ontario",Monday 13:30-13:45,N
Improved Global Minimum Variance Portfolio via a Dominant Eigenvalue Shrinkage,"In this talk, we first discuss the conventional two-step method for constructing a Global Minimum Variance Portfolio (GMVP). It is noted that in a vast portfolio allocation problem, the conventional method fails to give us a portfolio with a minimum true risk due to a severe estimation error in the sample covariance matrix. Then we show that under a set of conditions for the true covariance matrix, it is possible to reduce the true portfolio risk by replacing the sample covariance matrix with a modified covariance matrix constructed from a shrunk dominant eigenvalue in the first step. Simulation and empirical results are used to illustrate the effect of this shrinkage method.","Danqiao Guo University of Waterloo, Tony Wirjanto University of Waterloo, Chengguo Weng University of Waterloo",Monday 13:45-14:00,N
Detecting Adverse Drug Effects from Pharmacovigilance Databases,"The World Health Organization and many countries have built pharmacovigilance databases to detect potential adverse effects due to marketed drugs. Although a number of methods have been developed for early detection of adverse drug effects, the vast majority of them do not consider the multiplicity arising from testing thousands of drug and adverse event combinations. We first derive the optimal statistic to maximize the power of detection while maintaining the desired error rate. We then propose a nonparametric empirical Bayes method to estimate the optimal statistic and demonstrate its superior performance through simulation. Finally, the proposed method is applied to the pharmacovigilance database in the United Kingdom.",Yu Gao University of Waterloo,Monday 14:00-14:15,N
A Latent Gaussian Mixture Model for Longitudinal Data,"We introduce a mixture model for clustering high-dimensional longitudinal data; that is, data containing measurements taken at a large number of time points. The model uses an extension of the mixture of common factor analyzers model to allow the variability in such measurements to be explained by measurements taken at a smaller number of time points. A modified Cholesky decomposition on the latent covariance matrix is utilized to take into account the longitudinal nature of the data.","Vanessa Bierling McMaster University, Paul D. McNicholas McMaster University",Monday 14:15-14:30,N
Robust Mixture Discriminant Analysis,"The purpose of this work is to develop robust discriminant analysis procedures making use of mixture models. This work is motivated by recent model-based clustering work. This includes work on mixtures of multivariate t-distributions, mixtures of multivariate power exponential distributions, and mixtures of contaminated Gaussian distributions. Parameter estimation is carried out using the expectation-maximization algorithm, and the number of components per class is selected using the Bayesian information criterion. The proposed approaches are illustrated via simulated as well as real data.","Nkumbuludzi Ndwapi McMaster University, Paul D. McNicholas McMaster University",Monday 14:45-15:00,N
A Longitudinal Linear Mixed Effect Model for Bivariate Responses with Measurement Errors and Misclassification in Covariates,"This talk concerns estimation of a longitudinal linear mixed effects model with bivariate responses mixed with continuous and binary variables in the presence of measurement errors in the covariates. We consider both continuous covariates as well as categorical covariates with misclassification up to three categories. Through simulation studies, we investigate the impact of measurement errors on the covariates on the naive estimator of each parameter in the model when the other parameters are fixed.","Jia Li University of Calgary, Alexander R. de Leon University of Calgary, Haocheng Li University of Calgary",Monday 13:30-13:45,N
Accurate Confidence Intervals for Clustered Data and Small Samples,"The presence of clustering in data sets can impede statistical inference, especially for too few or small clusters of observations. To address the complications of clustered data, research has focused on adjustments to conventional distribution-free approaches in lieu of likelihood-based inference, which has been known to be sensitive for small samples. However, advances in likelihood theory offer an alternative solution to the clustering problem via higher-order approximations that are accurate, and can be attractive when working with small samples. I demonstrate that higher-order likelihood methods perform comparably well to some current distribution-free approaches in terms of attaining the appropriate coverage and can be preferable in terms of power. Some illustrative examples using the Tennessee Student-Teacher Achievement Ratio (STAR) dataset are provided as a context for comparison of higher-order likelihood and distribution-free approaches.",Harris Quach Pennsylvania State University,Monday 13:45-14:00,N
Incorporating Auxiliary Information by Joint Modelling of Pseudo Data and Length Biased Data,"Survival data are often limited in sample size, subject to length-biased sampling, or limited to a specific time/age range. Information for a sample from the same or similar cohorts, having more observations or a longer observation time, may be obtained from the literature. However, we may not be able to access the source data, or may have trouble combining the two samples. We introduce a way to incorporate the information from the auxiliary sample by creating pseudo datasets which match the form and conditions of the observed sample, eliminating the length-biased sampling problem and improving efficiency. We will illustrate the method with a real dataset.","Yidan Shi University of Waterloo, Leilei Zeng University of Waterloo, Mary E. Thompson University of Waterloo, Suzanne Tyas University of Waterloo",Monday 14:00-14:15,N
Analysis of Recurrent Events with Dynamic Models for Dependent Gap Times: A Copula Approach,"The analysis of past developments of processes through dynamic covariates is useful to understand the present and the future of processes generating recurrent events. In this study, we consider two important features of such processes through dynamic models. These features are related to monotonic trends and clustering of events in recurrent event data and are common in medical studies. We discuss certain issues in the estimation of these features through dynamic models based on event counts when unexplained heterogeneity is present in the data. Furthermore, we show that the violation of the strong assumption of independent gap times may introduce substantial bias in the estimation of these features with models based on event counts. To address these issues, we therefore apply a copula-based estimation method for the gap time models. This approach does not rely on the strong independent gap time assumption and provides valid estimation of model parameters.","Kunasekaran Nirmalkanna Memorial University of Newfoundland, Candemir Cigsar Memorial University of Newfoundland",Monday 14:15-14:30,N
Left-Truncated and Right-Censored Survival Data Analysis with Covariate Measurement Error,"Analysis of left-truncated and right-censored (LTRC) survival data has received extensive interest. Many inference methods have been developed for various survival models, including the Cox model and the additive hazards model. These methods, however, break down for many applications where survival data are error-contaminated. Although error-prone survival data commonly arise in practice, little work has been available in the literature for handling left-truncated and right-censored survival data with measurement error. In this paper, we explore this important problem under the Cox model. We develop valid inference methods and derive kernel smoothing estimators for the survival model parameters. We establish asymptotic results for the proposed estimators and assess the performance of our proposed methods using simulation studies. Our methods have a broad scope of application, including handling length-biased sampling data.",Li-Pang Chen University of Waterloo,Monday 14:30-14:45,N
Group Variable Selection for Recurrent Event Data,"In many scientific applications, such as biological studies, the predictors or covariates are usually high dimensional and naturally grouped. In this research, we consider the Andersen-Gill regression model for the analysis of recurrent event data with high dimensional group covariates. In order to study the effects of the covariates on the occurrence of recurrent events, a hierarchically penalized group selection method is introduced to address group selection problem under the Andersen-Gill model. We also consider an adaptive hierarchically penalized method for selecting covariates more efficiently, especially for identifying the important covariates in important groups. The asymptotic oracle properties of these methods are investigated. Our simulation studies show that the proposed methods perform well in selecting important groups and important individual covariates in these groups simultaneously. We illustrate these methods using some real life data sets from medicine.","Kaida Cai University of Calgary, Hua Shen University of Calgary, Xuewen Lu University of Calgary",Monday 14:45-15:00,N
SimRVPedigree: An R Package to Simulate Pedigrees Ascertained for Multiple Relatives Affected by a Rare Disease,"Family-based studies are receiving renewed attention because of their ability to identify genetic susceptibility factors associated with rare diseases. These studies have more power to detect rare variants, require smaller sample sizes, and can more accurately detect sequencing errors than case-control studies. However, garnering enough families for analysis of a rare disease could require years of effort, making these studies difficult to replicate. To address this shortcoming we have created an R package, SimRVPedigree, to randomly simulate families ascertained to contain multiple relatives affected by a rare disease. The package aims to mimic the process of family development, while allowing users to incorporate multiple facets of family ascertainment. We illustrate how approximate Bayesian computation with SimRVPedigree may be used to estimate the relative risk of disease for genetic cases in a sample of ascertained families.","Christina M. Nieuwoudt Simon Fraser University, Jinko Graham Simon Fraser University",Monday 13:30-13:45,N
A Comparison of Association Methods for Fine-Mapping from Sequence Data in Diploid Populations using the Case-Control Study Design,"Many methods have been proposed to detect disease association with sequence variants in candidate genomic regions. However, the literature lacks a comparison of these methods in terms of their ability to localize or fine-map the causal risk variants lying within the candidate region. We extend a previous comparison of the detection abilities of these methods to a comparison of their localization abilities. In contrast to previous work, cases and controls are sampled from a diploid (i.e., two-parent) rather than a haploid (one-parent) population. We simulated 200 sequencing datasets of a 2-million base-pair candidate genomic region for 50 cases and 50 controls. Risk variants were in a middle subregion. We present a case study of one simulated dataset to illustrate the methods and describe simulation results to score which method best localizes the risk subregion. Our results lend support to the potential of genealogy-based methods for genetic fine-mapping of disease. ","Charith Bhagya Karunarathna Simon Fraser University, Jinko Graham Simon Fraser University",Monday 13:45-14:00,N
A Robust Allele-based Regression Framework for Testing Association and Hardy-Weinberg Equilibrium,"Existing allele-based association tests, examining association between genetic markers and complex human traits, are sensitive to the assumption of Hardy-Weinberg equilibrium (HWE) and limited to binary outcomes. We propose a new regression framework with individual allele as the response variable. We show that the score test statistic derived from this regression model contains a correction factor that explicitly adjusts for the departure from HWE, thus it maintains type 1 error control in the presence of HW disequilibrium (HWD). In the absence of HWD, the proposed method has comparable power as genotype-based association tests. Using this regression framework, we can then study quantitative traits, simultaneously test the HWE assumption, as well as handle more complex data, including correlated individuals from families, multiple populations and uncertain genotype observations. We support our analytical findings using evidence from both simulation and application studies.","Lin Zhang University of Toronto, Lei Sun University of Toronto",Monday 14:00-14:15,N
Gene Set Enrichment Analysis in Longitudinal Studies,"Gene-set enrichment methods aim to discover gene-sets associated with phenotypes. Although there has been progress in developing proper methods for analysis of high dimensional microarray datasets in cross-sectional studies, methods for dealing with longitudinal phenotypes are still limited. A two-step self-contained gene-set analysis method is developed to handle multiple longitudinal outcomes. Analysis of within-subject variation in the first step is followed by examining the between-subject variation utilizing Linear Combination Test (LCT) in the second step. This method is also applicable in analysis of time-course microarray data. The performance of the method is evaluated in a simulation study. The proposed method is very efficient in controlling Type I error and works well with small sample size, large number of genes and missing data. ","Elham Khodayari Moez University of Alberta, Irina Dinu School of Public Health, University of Alberta",Monday 14:15-14:30,N
Factor Copula Analysis for Multivariate Ordinal Data ,"In genetic studies of many neurological and psychological disorders, quantitative phenotypic traits are latent and inferred from the ordinal assessment scores in diagnostic questionnaires. In such cases, an accurate representation of the dependence structure in multivariate ordinal data is essential for correct identification of these latent traits and their use in genetic mapping. This work provides a detailed comparison of dependence measures for multivariate ordinal data and investigates the robustness of polychoric correlation estimation under settings with asymmetric dependence patterns and varying degrees of skewness in marginal distributions. An alternative strategy to quantify the latent traits is proposed using factor copula models. We compare the performances of these approaches in factor scores regression for testing genetic associations. ","Agnes Nessie Amu University of Manitoba, Elif Fidan Acar University of Manitoba",Monday 14:30-14:45,N
Reduced-Rank Singular Value Decomposition for Dimension Reduction with High-Dimensional Data,"Recent technical advances in genomics have led to an abundance of high-dimensional and correlated data. Dimension reduction methods typically rely on matrix decompositions (e.g. SVD and EVD) to compute the quantities needed for further analysis. However, in a high-dimensional setting, these decompositions must be adapted to cope with the singularity of the matrices involved. We illustrate how this can be done in the context of a particular dimension reduction method, Principal Component of Explained Variance (PCEV). PCEV seeks a linear combination of outcomes by maximising the proportion of variance explained by the covariates of interest. Using random matrix theory, we propose a heuristic that provides a fast way to compute valid p-values to test the significance of the decomposition. We compare the power of this approach with that of other common approaches to high-dimensional data. Finally, we illustrate our method using methylation data collected on a small number of individuals.","Maxime Turgeon McGill University, Stepan Grinek BC Cancer Agency, Celia M.T. Greenwood Lady Davis Institute, Jewish General Hospital, Montreal and McGill University, Aurélie Labbe HEC Montréal",Monday 14:45-15:00,N
"Properties of Risk Measures Inspired from the Ruin Probability, the Deficit at Ruin, and the Time of Ruin","We study a risk measure derived from ruin theory defined as the amount of capital needed to cope in expectation with the first occurrence of a ruin event. Specifically, within the compound Poisson model, we investigate some properties of this risk measure with respect to the stochastic ordering of claim severities. Particular situations where combining risks yield diversification benefits are identified. Closed form expressions and upper bounds are also provided for certain claim severities. Further extensions are explored.","Ilie Radu Mitric Université Laval, Julien Trufin Université Libre de Bruxelles, Amine Mohamed Lkabous Université du Québec à Montréal",Monday 15:30-16:00,N
Risk Measures Related to both the Information of an Individual Risk and Industry Risk,"In this paper we show how risk measures can be embedded within the framework of credibility theory. More specifically, we introduce a new type of risk measures, the credible risk measures, in order to capture the risk of an individual contract (or financial portfolio) as well as the industry risk consisting of several similar, but not identical, contracts (or financial portfolios). We consider the classical case, as well as the regression case. Examples are given based on the Fama/French data.",Georgios Pitselis University of Piraeus,Monday 16:00-16:30,N
Good Deal Indices in Asset Pricing: Actuarial and Financial Implications,"We integrate into a single optimization problem a risk measure and, either arbitrage free real market quotations, or financial pricing rules generated by an arbitrage free stochastic pricing model. We call a good deal (GD) a sequence of investment strategies such that the couple (expected-return, risk) diverges to (+infinity;-infinity). The existence of such a sequence is equivalent to the existence of an alternative sequence such that the couple (risk, price) goes to (-infinity; - infinity). Moreover, by appropriately adding the riskless asset, every GD may generate a new one composed only of strategies priced at 1. We show how GDs exist in practice, and study how to measure a good deal size. We also provide the minimum relative (per dollar) price modification that prevents the existence of GDs. This is a crucial tool to detect over/under-priced securities or marketed claims. Many classical actuarial and financial optimization problems can generate wrong solutions if the used market quotations or stochastic pricing models do not prevent the existence of GDs. We illustrate this and show how GD indices help overcome this caveat. Numerical illustrations are given. ","Jose Garrido Concordia University, Alejandro Balbás University Carlos III de Madrid, Spain, Ramin Okhrati United of Southampton, United Kingdom",Monday 16:30-17:00,N
Teaching the Statistical Investigation Process with Simulation-Based Inference to Improve Statistical Thinking ,"The statistics education community is increasingly focusing on the use of simulation-based methods, including bootstrapping and permutation tests, to illustrate core concepts of statistical inference within the context of the overall research process. This new focus presents an opportunity to address documented shortcomings in introductory and intermediate level statistics courses. In this talk I will (1) discuss the motivation and rationale behind the simulation-based approach, (2) share some concrete examples of how the approach works and can be integrated into existing courses, (3) present research evidence of its effectiveness at impacting students conceptual understanding and attitudes post-course and in the months following the courses completion and, (4) share a wealth of instructional resources available to support instructors trying out and using these approaches.",Nathan Tintle Dordt College,Monday 15:30-16:15,N
Incorporating Simulation-Based Inference in an Introductory Statistics Course,"The use of simulation-based methods in the teaching of statistical inference has received considerable attention in recent years. The objective of this presentation is to share my experience incorporating simulation-based methods of inference in the introductory statistics course at the University of Lethbridge. This change was made for the first time in the 2016/17 academic year. I will discuss motivations for the change, and some of the consequences that arose from the change. I will address available resources, highlight student performance and feedback, as it pertains to the change, and discuss lessons learned for next time.",John Sheriff University of Lethbridge,Monday 16:15-17:00,N
Comparing Bayesian External and Internal Validation Methods in Correcting Outcome Misclassification Bias in Logistic Regression,"Misclassification is frequent in administrative data. Using simulations, we compared two Bayesian methods in reducing outcome misclassification bias in logistic regression. Sensitivity and specificity priors were based on external information in method 1 and on internal complementary data in method 2. Bias, 95\% credible intervals (CI) coverage and mean squared error (MSE) were used to assess the performance of these methods. Both methods yielded estimates with less bias (-0.113 - 0.047) than the naïve analysis (-0.452 - 0.258) in all scenarios explored. Method1 performed well with reasonable sensitivity and specificity priors; performance decreased with decreased mean of prior sensitivity. CI coverage was high for both methods (95-100\%). The Bayesian external validation method is practical and useful to reduce outcome misclassification bias in logistic regression. While the internal method may provide better adjustment, it has the disadvantage of requiring additional individual data.","Elham Rahme McGill University, Jiayi Ni Research Institute of the McGill University Health Center, Kaberi Dasgupta McGill University, Denis Talbot Université Laval, Geneviève Lefebvre Université du Québec à Montréal, Lisa M. Lix University of Manitoba, Lucie Blais Université de Montréal",Monday 15:30-16:00,N
Validation of Codes and Algorithms are an Essential Part of Research Using Routinely Collected Health Data,"Routinely collected health data (RCD) are defined as data collected without specific a priori research questions developed prior to utilization for research. In Canada, the most widely used population-based RCD are health administrative data, other examples include disease registries, public health reporting, and electronic health records. With the increased use of RCD, there is increased awareness of methodological concerns. The REporting of studies Conducted using Observational Routinely-collected Data (RECORD) statement was developed to address these limitations and to help meet the requirement of clear reporting of research using RCD. RECORD requires the description of validation studies of the codes/algorithms used to select the population, and to describe outcomes, confounders, or effect modifiers. I will emphasize the importance of algorithm and code validation prior to embarking on research using RCD, and present validation methods from the literature. We will encourage discussion on new and innovative methods to validate administrative data for use in health research.",Eric Benchimol University of Ottawa,Monday 16:00-16:30,N
Feature Selection Methods for the Development of Case Definitions for Common Chronic Conditions in Primary Care Electronic Medical Record Data,"Chronic disease surveillance information is dependent on the quality of the EMR data, and the quality of the case identification algorithms. Data were obtained from the Canadian Primary Care Sentinel Surveillance Network. A chart review was conducted for the presence of 8 chronic conditions in 1920 primary care patients. The results of this review will be used as training data for classification models. Features will be selected from billing codes, medication prescriptions, laboratory values, encounter diagnoses and health-problem lists. CART, C5.0, and CHAID decision tree algorithms will be compared with LASSO and forward stepwise logistic regression in terms of case definition development. A bootstrap validation technique will be used to select optimal complexity parameter value. Validity measures will be determined using 10-fold cross validation. RESULTS: It has been shown that these methods are comparable with committee created case definitions in terms of predictive accuracy. ","Tyler Williamson University of Calgary, Brendan Cord Lethebe University of Calgary, Colin Weaver University of Calgary, Tolulope T. Sajobi University of Calgary, Hude Quan University of Calgary",Monday 16:30-17:00,N
Observational Analyses of Cluster-Randomized Trials,"The cluster-randomized trial is an important tool for the study of interventions at a group level, and for interventions in which contamination may be a concern. Hospital-level program interventions are particularly appropriate for this design. Cluster-randomized trials can, however, be very expensive and time-consuming. Because these trials often involve collection of substantial data, observational analyses of follow-up data can provide useful information. I will discuss some analytic challenges related to observational analyses of cluster trials. Such challenges include methods to account for clustering, in particular how clustering affects missing data and measurement error, and appropriate statement of the research question and identification of correct comparisons to address the question. I will illustrate these problems through analyses of the PROBIT study of a breastfeeding promotion intervention.",Robert W. Platt McGill University,Monday 15:30-16:00,N
Analysis of Cluster-Randomized Trials with a Longitudinal Outcome Subject to Irregular Observation,"Randomized trials may feature an outcome measured longitudinally as part of usual care. This reduces trial costs, as no special study visits are required, while allowing for an exploration of longer term outcomes. However, since the timing of the visits is not specified by protocol, this can lead to outcome observation times varying among subjects, and potentially related to the outcomes themselves. This may result in bias unless appropriate analytic methods are used. This talk will explore how two popular classes of methods (inverse-intensity weighting and semi-parametric joint models) can be extended to account for within-cluster correlation.",Eleanor M. Pullenayegum The Hospital for Sick Children,Monday 16:00-16:30,N
Strategies to Handle Missing Binary Outcomes in Cluster Randomized Trials,"Missing data is a common issue in cluster randomized trials (CRTs). Choosing the most appropriate strategy to handle a missing binary outcome in CRTs may be very challenging due to great variability in the design and implementation of such trials. According to published literature, the optimal methods for handling missing outcome data in CRTs are infrequently used in practice. In my presentation, I will provide a brief overview of the methods available in the literature for handling missing binary outcomes in CRTs, and illustrate how to choose and implement the most appropriate missing data strategy based on the design characteristics of the CRTs using both a real CRT and simulated scenarios as examples. I will also address the issues using the population-averaged and cluster-specific methods in analyzing the binary data in CRTs when missing data present.","Jinhui Ma Children's Hospital of Eastern Ontario Research Institute, Monica Taljaard Ottawa Hospital Research Institute, Lisa Dolovich McMaster University, Janusz Kaczorowski University of Montreal, Larry Chambers University of Ottawa, Lehana Thabane McMaster University",Monday 16:30-17:00,N
G-estimation and Model Selection for Dynamic Treatment Regimes,"Dynamic treatment regimes (DTRs) formalize personalized medicine by tailoring treatment decisions to individual patient characteristics. G-estimation for DTR identification targets the parameters of a structural nested mean model (the blip function), from which the optimal DTR is derived. Despite much work focusing on deriving such estimation methods, there has been little focus on extending G-estimation to the case of non-additive effects, non-continuous outcomes or on model selection. We demonstrate how G-estimation can be more widely applied through the use of iteratively-reweighted least squares procedures, and illustrate this for log-linear models. We then derive a quasi-likelihood function for G-estimation within the DTR framework, and show how it can be used to form an information criterion for blip model selection. These developments are demonstrated through simulation, as well as in application to data from the Sequenced Treatment Alternatives to Relieve Depression study.","David A. Stephens McGill University, Michael Wallace University of Waterloo, Erica E.M. Moodie McGill University",Monday 15:30-16:00,N
On the Bayesian Semi-Parametric Double Robustness Property,"We have studied Bayesian doubly robust causal inferences under a framework where the causal contrast is specified in terms of a prediction problem under a hypothetical randomized experiment, and inverse probability of treatment (IPT) weights are introduced as importance sampling weights in Monte Carlo integration. A posterior distribution is produced by sampling from a non-parametric model, with parametric working models introduced for smoothing purposes. Marginal structural models can be estimated similarly through maximization of a parametric utility function. However, it is still unclear how certain other causal estimands such as treatment effects among the treated and structural mean models, and estimation methods such as propensity score regression and g-estimation, could be incorporated into the Bayesian semi-parametric framework. We will review the ideas behind Bayesian doubly robust estimation using IPT weights, and consider extending these to the aforementioned directions. ",Olli Saarela University of Toronto,Monday 16:00-16:30,N
Achieving Higher Efficiency and Robustness in Longitudinal Studies with Dropout,"Intrinsic efficiency and multiple robustness are desirable properties in missing data analysis. We establish both for estimating the mean of a response at the end of a longitudinal study with drop-out. The idea is to calibrate the estimated missingness probability at each visit using data from past visits. We consider one working model for the missingness probability and multiple working models for the data distribution. Intrinsic efficiency guarantees that, when the missingness probability is correctly modelled, the multiple data distribution models, combined with data prior to the end of the study, are optimally accommodated to maximize efficiency. Multiple robustness ensures estimation consistency if the missingness probability model is misspecified but one data distribution model is correct. Our proposed estimators are all convex combinations of the observed responses, and thus always fall within the parameter space.",Peisong Han University of Waterloo,Monday 16:30-17:00,N
Self-Normalized Weak and Strong Functional Limit Laws in the Domain of Attraction of Stable Laws,"We will present a glimpse of Donsker-type weak invariance principles that have been established in these years for self-normalized partial sums processes in the domain of attraction of a stable law with index number in the interval (0, 2]. Special attention will be given to the case of index number 2, i.e., to DAN (domain of attraction of the normal law), and to applications to change-point analysis in this domain, with summands possibly having an infinite variance. Strassen-type strong functional laws and invariance principles in DAN will also be explored in the latter case. ",Miklos Csorgo Carleton University,Monday 15:30-16:00,N
Functional Central Limit Theorems for Self-Normalized Processes with Statistical Applications,"We will revisit Donsker-type functional central limit theorems for self-normalized processes that are based on independent random variables. We will also present some statistical applications of these theorems, including the ones for linear errors-in-variables and regression models, and for construction of asymptotic confidence intervals for a population mean. Some modifications and generalizations of these theorems will be outlined.",Yuliya V. Martsynyuk University of Manitoba,Monday 16:00-16:30,N
Self-Normalized Partial Sums of Randomly Weighted Data,In this talk we address applications of self-normalized partial sums of randomly weighted data (i) in increasing the accuracy of the CLT based confidence intervals and (ii) in drawing inference based on sub-samples drawn from large data sets.,Masoud Nasari Carleton University,Monday 16:30-17:00,N
Discordance in Hormone Receptor Status Between Two Primary Breast Cancers: The Impact of Misclassification,"A Bayesian method is proposed to address misclassification errors in both independent and dependent variables. Our work is motivated by a study of women who have experienced new breast cancers on two separate occasions. Hormone receptors (HRs) are important in breast cancer biology, and it is well recognized that the measurement of HR status is subject to errors. This discordance in HR status for two primary breast cancers is of concern and might be an important reason for treatment failure. We consider a logistic regression model for the association between the HR status of the two cancers and introduce the misclassification parameters accounting for the misclassification in HR status. The prior distribution for sensitivity and specificity is based on how HR status is assessed in laboratory procedures. B-spline terms are used to account for the nonlinear effect of an error-free covariate. Our findings indicate that the true concordance rate of HR status between two primary cancers is greater than the observed value.","Juxin Liu University of Saskatchewan, Paul Gustafson University of British Columbia, Dezheng Huo University of Chicago",Monday 15:30-16:00,N
Estimating the Population Size of Greater Victoria’s Injection Drug Users using Capture-Recapture Methods,"Population size estimation is critical for planning public health programs for injection drug users. Estimation is difficult, as these populations are considered “hidden” or “hard to reach”. The accepted population size estimate for Greater Victoria, Canada is between 1500 and 2000 individuals, which is dated prior to the year 2000, and is likely an underestimate. I will discuss the use of both closed and open mark-recapture models to estimate population size using cross-sectional survey data collected in 2003, 2005 and 2009. All methods provided population size estimates that were higher than the currently accepted estimate. The open-population estimate of the number of the injection drug users in Greater Victoria was relatively stable over time with fewer than 3000 individuals over the 6-year study. Improved estimates of population size and dynamics will assist in improving access to harm reduction services, which may reduce higher risk drug use practices.",Laura L.E. Cowen University of Victoria,Monday 16:00-16:30,N
Statistical Opportunities and Challenges in Multiple-Platform Biomedical Studies,"It has become increasingly common in biomedical research to involve multiple fields (e.g. neuroscience, nutrition, psychiatry, pediatrics etc.) with researchers sharing common overarching goals, under which specific project aims and study designs are carried out by different teams. Due to the nature of linked and often parallel research programs, information regarding the data structure and integration steps is essential when considering statistical analyses. Examples taken from ongoing multiple platform studies will be presented to illustrate some statistical applications, and selected statistical methods for quality assurance, dimension reduction, and visualization, will be discussed. The pros and cons of commonly used statistical approaches in multiple platform studies will be illustrated through real examples.",Wendy Lou University of Toronto,Monday 16:30-17:00,N
Dependent Models for the Duration and Size of BC Fires,"The goal of our research is to jointly model time spent (duration) in days and area burned (size) in hectares from ground attack to final control of a fire as a bivariate survival outcome using either a copula model framework that connects the two outcomes functionally, or a joint modeling framework that connects the two outcomes with a shared random effect. The challenges include the specific framework to be employed, how the longitudinal environmental variables (e.g. precipitation, drought indices) are incorporated, and the complexity of computation associated with two outcomes considered jointly. A key aspect of the project is knowledge translation through collaboration with fire scientists at the federal government to implement the optimal framework developed as a component in the fire prediction system that is concurrently under development by Natural Resources Canada. The direct comparison between the statistical properties of the two broad frameworks is also of interest.","Da Zhong (Dexen) Xi Western University, Charmaine B. Dean Western University, Steve Taylor Pacific Forestry Centre",Monday 15:30-15:45,N
Testing for Parallel Carryover Effects in Redundant Systems,"Redundancy is an important method to improve the reliability of a repairable system. In this study, we consider clustering of failures in redundant systems due to parallel type of carryover effects, in which the event intensity of a recurrent event process is temporarily increased after event occurrences in other processes. Our main goal is to develop formal test procedures for the assessment of such effects in redundant systems with repairable components connected in parallel and subject to recurrent failures. We therefore develop partial score tests for testing the presence of parallel carryover effects in recurrent event settings. Asymptotic properties of the test statistics are discussed analytically as well as through simulations. A data set including failure times of diesel power generators operating in remote and isolated communities is analyzed to illustrate the methods developed.","Yongho Lim Memorial University of Newfoundland, Candemir Cigsar Memorial University of Newfoundland",Monday 15:45-16:00,N
Building a Basketball Analytics Startup as a Statistics Student,"We hear it time and time again: data surrounds us, data scientist is the career of the future, etc. With all of this opportunity for us, what does a statistician with entrepreneurial interests need to know before taking the leap? In this talk I'll share my experiences of building a startup that helps university coaches prepare for their next opponent by leveraging modern statistical techniques on play-by-play data. I'll discuss lessons learned that I didn't get in school about product development, business strategy, user acquisition, and customer success. I'll briefly acknowledge successes before focusing on the failures that were specific to the product being developed and marketed as a statistics product, hopefully deriving some useful general advice. I'll conclude with useful resources. My hope is to provide the information that I wish I saw someone speak about before I started this endeavour, and that it encourages anyone with interests in starting their own venture. ",Steven Wu Simon Fraser University,Monday 16:00-16:15,N
Efficient Conversion Probability Estimator for Display Advertising,"The goal of online display advertising is to entice users to “convert” (i.e., take a pre-defined action such as making a purchase) after clicking on the ad. An important measure of the value of an ad is the probability of conversion. The focus of our project is finding an efficient, accurate, and precise estimator of conversion probability. Challenges associated with the data are the delays in observing conversions and the size of the data set (both number of observations and number of predictors). Two models have been previously considered as a basis for estimation: A logistic regression model and a joint model for the conversion status and delay times. Fitting the former is simple, but ignoring the delays in conversion leads to an under-estimate of conversion probability. The latter is more realistic but computationally expensive to fit. Our proposed estimator is a compromise between these two estimators. We apply our results to a data set from Criteo which is a subset of the clicks occurred over a two-month period along with their final conversion status.","Abdollah Safari Simon Fraser University, Rachel MacKay Altman Simon Fraser University",Monday 16:15-16:30,N
Parametric Spectral Density Estimation in Atomic Force Microscopy,"Atomic force microscopy (AFM) experiments provide nanoscopic measurements of the dynamic properties of molecules and atoms. Parametric spectral density estimation features prominently in the analysis and calibration of these experiments, which typically generate large amounts of data. To this end, maximum likelihood estimation can be prohibitively expensive, while a much faster least-squares estimation method incurs a considerable loss in statistical precision. Moreover, both methods are highly sensitive to systematic sources of instrument vibration. We propose a variance-stabilizing procedure to combine the simplicity of least-squares with the efficiency of maximum likelihood, and a de-noising procedure based on significance testing for spectral periodicities. Simulation and experimental results indicate that a two- to ten-fold reduction in mean square error can be expected by applying our methodology.","Bryan Yates University of Waterloo, Martin Lysy University of Waterloo, Aleks Labuda Asylum Research",Monday 16:30-16:45,N
Clustering of Functional Data using Mixture Model-Based Clustering,"Clustering of functional data using mixture model-based clustering is considered. Dimension reduction is carried out via functional PCA and the principal components are clustered using a Gaussian mixture model. The proposed method uses an MM algorithm to calculate the MLEs of the mixture model, which allows our algorithm to scale efficiently as the data dimensionality increases.This work is motivated by an applied problem whose goal is to identify different physical activity patterns in children's movement over time. The movement is measured by wearable accelerometers and measurements are recorded every 3 seconds over the course of a week. ","Peter Tait McMaster University, Paul D. McNicholas McMaster University",Monday 16:45-17:00,N
Simple Measures of Individual Cluster-Membership Certainty for Hard Partitional Clustering,"Hard partitional clustering methods such as the Partitioning Around Medoids (PAM) algorithm give no measure of individual cluster-membership certainties. We propose two posterior-probability-like measures of individual cluster-membership certainty which can be applied to a hard partition of the sample. One measure is an extension of the individual silhouette widths and the other is based on the pairwise dissimilarities in the sample. We apply both measures to a set of dissimilarity matrices for categorical data with the PAM algorithm and evaluate their performance on an individual with ambiguous cluster membership in simulated datasets. As a benchmark, we also present the results of a soft clustering algorithm and two model-based clustering methods. The proposed measures behave similarly to posterior probabilities from the soft clustering and model-based clustering methods and are worth considering as a way to augment hard partitional clustering methods.","Dongmeng Liu Simon Fraser University, Jinko Graham Simon Fraser University",Monday 15:30-15:45,N
Statistical Modelling of Spot Fires in Forest Fire Control,"A spot fire is a term used in wildfire management to indicate that an airborne ember ignited a new fire. Under certain meteorologic and wildfire conditions, a spot fire can cross a fuel break, such as a river or road, and allow the wildfire to progress over an otherwise unreachable location. We model the progression of a wildfire under a variety of conditions, and the associated generation of airborne embers that can result in a spot fire. We then derive the probability distributions of the time to the first spot fire occurring across a fuel break. A wildfire simulator is developed, and we present procedures with various types of practical data to estimate the model parameters.","Trevor Thomson Simon Fraser University, X. Joan Hu Simon Fraser University, John Braun University of British Columbia - Okanagan",Monday 15:45-16:00,N
Spatial Modeling of Repeated Events: An Application to Disease Mapping,"Mixed models are commonly used to analyze spatial data, which frequently occur in practice such as health sciences and life studies. It is customary to incorporate spatial random effects into the model to account for the spatial variation of the data. In particular, Poisson mixed models are used to analyze the spatial count data. We assume that the observations in each area, conditional on spatial random effects, are independent of each other. However, this may not be a valid assumption in practice. E.g., multiple asthma visits by a patient to clinics within a year are not clearly independent observations. To address this issue, we develop spatial models with repeated events. In particular, compound Poisson mixed models are introduced to account for the repeated events as well as the spatial variation of the data. Performance of the proposed approach is evaluated through a simulation study and by application to a dataset of patients' asthma visits to clinics in the province of Manitoba, Canada.",Shabnam Balamchi University of Manitoba,Monday 16:00-16:15,N
A Penalized Approach to Estimating Network Changes in the Human Microbiome,"The microbiome is the collection of microorganisms colonizing the human body, and plays an integral part in human health. A growing trend in microbiome analysis is to construct a network to estimate the level of co-occurrence between different taxa. Current methods do not facilitate investigation into how these networks change with respect to some phenotype. We propose a model to estimate network changes with respect to a binary phenotype. The counts of individual taxa in the samples are modeled through a Poisson distribution whose mean depends on a Gaussian random effect term. The penalized precision matrix of all the random effect terms determines the co-occurrence network among taxa. The model fit is obtained and evaluated using Monte Carlo methods. Finally, we introduce a means of coding established biological information into the model. The performance of the model is evaluated through an extensive simulation study, and tested in samples from recent gut microbiome studies.","Kevin D.J. McGregor McGill University, Aurélie Labbe HEC Montréal, Celia M.T. Greenwood Lady Davis Institute, Jewish General Hospital, Montreal and McGill University",Monday 16:15-16:30,N
Statistical Modelling of Carbon Dioxide Flux Data,"Climate change is now having such a profound impact on life systems that scientists are preoccupied with phenology. Usually, two sources of data are used in phenology studies, the first of which is remote sensing data, while the other is ground based data. It is costly to get ground-based data at a new location. For this reason, we are interested in building a model and using remote sensing data to predict ground-based data at one location, then connecting multiple nearby locations to predict ground-based data across a surface at a wider range. The data are seasonal with variable phase. The data need to be registered (phase standardized) by year. We build a model based on FDA and time warping function. This gives a statistic model linking ground and remote data. Future work will include spatial aspects of this idea. ","Fang He University of Western Ontario, Duncan Murdoch University of Western Ontario, Reg Kulperger University of Western Ontario",Monday 16:30-16:45,N
Identifying Expression Quantitative Trait Loci in Genome-Wide Association Studies using Matrix eQTL,"The objective of this study is to identify an efficient, statistically sound and user friendly method for analysis of Expression quantitative trait loci (eQTL) studies. In this study, we performed expression quantitative trait loci (eQTL) analysis using the Matrix eQTL R package. This technique implements matrix covariance calculation and efficiently runs linear regression analysis. False discovery rate (FDR) is used to identify significant cis and trans eQTL for multiple testing corrections. We applied matrix eQTL to a real data set consisting of 730,256 SNP and 33,298 RNA for 173 samples. After processing data, gene SNPs associations can be identified using the ANOVA model. In this study, 15,408 cis eQTL and 27,562 trans eQTL are identified, at a FDR less than 0.05. We found out that matrix eQTL is a computationally efficient and user friendly method for analysis of eQTL studies. The results provide insight into the genomic architecture of gene regulation in inflammatory bowel disease (IBD). ","Fahimeh Moradi University of Alberta, Elham Khodayari Moez University of Alberta, Irina Dinu University of Alberta",Monday 16:45-17:00,N
Classical and Modern Multivariate Statistical Analysis,"Modern multivariate statistical analysis has flexible distributional assumptions, can be applied to continuous and discrete variables, and does not require the classical Gaussian assumption. The most popular methods are based on copulas and the most flexible copula constructions for high dimensions are based on graphical objects called vines. Vine pair-copula constructions, including those with latent variables, can be considered as Gaussian extensions after the correlation or loading matrix is parametrized in terms of correlations and partial correlations that are algebraically independent. It will be shown that some Gaussian models with parsimonious dependence and their copula extensions can be represented with the same graphical models. The truncated vine also provides a parsimonious dependence model for multivariate Gaussian applications. Concrete examples will be used to illustrate the main ideas. ",Harry Joe University of British Columbia,Tuesday 08:40-09:45,N
Modelling Multi-Population Mortality Dependence with a Regime-Switching Copula,"Multi-population mortality modelling plays a crucial role in the securitization of longevity risk. For example, the payoff of Swiss Re’s Kortis bond involves the mortality improvements of both the U.S. and U.K. male populations. Capturing the mortality dependence between the two populations is critical for accurate longevity risk pricing. Recently, some researchers investigated the use of copula in modelling mortality dependence, and observed that mortality dependence is stronger during mortality deteriorations than during mortality improvements. In order to capture observed asymmetric dependence, we propose to use a multivariate regime-switching copula. We study how the choice of copula affects the risk profile of a longevity security and examine the impact of asymmetric dependence and regime-switching in pricing the security.",Rui Zhou University of Manitoba,Tuesday 10:20-11:05,N
A Hierarchical Credibility Approach to Modeling Mortality Rates for Multiple Populations,"In this talk, we propose a hierarchical credibility approach to modeling multi-population mortality rates using data from the Human Mortality Database. Hierarchical credibility is used in a premium calculation with grouped data from different levels in a tree structure. Traditional mortality models for a single population estimate the model parameters with mortality data grouped into two levels (age and year). We group the mortality data for both genders of several developed countries into four levels (country, gender, age and year) or three levels (population, age and year), apply the hierarchical credibility to each level to better reflect the correlation of mortality rates among populations or genders/countries, and compare the forecasting performances between the model and some multi-population mortality ones.","Cary Chi-Liang Tsai Simon Fraser University, Adelaide Di Wu Simon Fraser University",Tuesday 11:05-11:50,N
The Silicon Valley Wage Cartel: Understanding its Effects on the Regional Labour Market with Dynamic Networks,"In 2013, Google, Apple, Adobe, Intel settled a class action lawsuit for \$415 million. The suit alleged that these companies colluded with other major corporations in Silicon Valley to suppress employee wages by agreeing not to poach workers from other cartel members. We build on niche theory in a dynamic context through the analysis of a network of manager job changes before and during the period the cartel was active. We investigate two related questions. First, how did the cartel affect managerial labour flows? And second, how did the cartel affect other firms competing for talent in the labour market? We use the temporal exponential graph model (TERGM) to examine the changes in manager job transitions before and during the cartel’s activities. We find that firms that were not part of the cartel lost relatively more workers than expected when their niches overlapped with a higher proportion of cartel member firms. This work suggests paths for future labour market policy making.","James David Wilson University of San Francisco, Jon Mackay Waterloo Institute for Complexity Innovation",Tuesday 10:20-10:50,N
Generative Mixture of Networks,"We introduce a generative model based on training deep architectures. The model consists of K networks that are trained together to learn the underlying distribution of a given data set. The process starts with dividing the input data into K clusters and feeding each into a separate network. We use an EM-like algorithm to train the networks together and update the clusters of the data. We call this model Mixture of Networks. The provided model is a platform that can be used for any deep structure and be trained by any conventional objective function for distribution modeling. As the components of the model are neural networks, it has a high capability in characterizing complicated data distributions as well as clustering data. We apply the algorithm on MNIST handwritten digits and Yale faces datasets. The model can learn the distribution of these data sets. One can sample new data points from these distributions that look like a real handwritten digit or a real face. We also demonstrate the clustering ability of the model using some real-world and toy examples.",Ali Ghodsi University of Waterloo,Tuesday 10:50-11:20,N
Practical Testing,"Technology companies, especially consumer facing ones, have seen an explosion in the use of experiments. Many companies now consider statistical hypothesis testing (such as AB testing) to be an appropriate solution to many questions. Outside of a few well-defined examples, however, a lack of statistical knowledge hampers the efforts of real-world practitioners. The purpose of this talk is to document a few experiments and explain why they failed. We will also consider steps that we, as researchers, academics and practitioners, can take to avoid such failures in the future.",Nick Ross University of San Francisco,Tuesday 11:20-11:50,N
Is All That Coffee I’m Drinking Hurting or Harming Me? Understanding Causality,"Students know the adage “Correlation is not Causation” yet research shows that, even after completing an introductory statistics course, they can be fooled into making causal conclusions by relationships that, on the surface, appear plausible, and they perform poorly on assessments of the purpose of randomization in study design. Moreover, most research questions are causal, and yet most data science problems rely on observational data. Unfortunately, research in the statistics education literature has focused largely on sample-to-population inference rather than experiment-to-causation inference. I will talk about some of the problems I have observed that may be inhibiting students’ ability to make appropriate causal inferences, including ambiguity in language and lack of facility in multivariate thinking and I will consider what we might do in the first (and often last) non-mathematical course in statistics, to give our students a deeper, practical understanding of causal inferences.",Alison L. Gibbs University of Toronto,Tuesday 10:20-11:05,N
How to tell when Association might be Causation,"The most important decisions we make, as individuals and collectively, are based on our perceptions of answers to causal questions. The cacophony of conflicting claims about the effects of diets, drugs, social and economic policies, are largely the result of the fact that, for most causal questions, we only have observational evidence. As statisticians, we have a unique appreciation of the issues involved with causal inference and we can help improve the public understanding of causality by helping students in our service courses to develop the judgment to assess causal claims. The talk will discuss some attempts to achieve this.",Georges A. Monette York University,Tuesday 11:05-11:50,N
Ergodicity and Approximation of the Conditional Causation Probability of Engineering Systems,"We report on the ergodicity of and the approach to approximate the conditional causation probabilities and the conditional cause of failure probabilities via finite Markov chains. This is significant because they provide additional important information to the failure distribution of a system. They especially provide information about the reliability for an aged system. Assisted by a fast algorithm, we demonstrate to readers how to obtain the probabilities and verify our theoretical derivations. The paper should be of interest to a broad range of audiences, including academic researchers and reliability system engineers. This work is a continuation of that entitled “Distributions and causation probabilities of multiple-run-rules and their applications in system reliability, quality control and start-up tests” which has already been accepted for publication.","Hsing-Ming Chang National Cheng Kung University, Taiwan",Tuesday 10:20-10:50,N
A Simple Variance Control Chart with Double Sampling Scheme,"Control charts are effective tools for signal detection in both manufacturing processes and service processes. Much of the data in service industries come from processes exhibiting non-normal or unknown distributions. The commonly used Shewhart variable control charts, which depend heavily on the normality assumption, are not appropriately used here. This paper thus proposes a DS EWMA-AV chart for monitoring process variability. We further explore the sampling properties of the new monitoring statistics, and calculate the average run lengths when using the proposed DS EWMA-AV chart. The performance of the DS EWMA-AV chart and those of non-parametric variance charts are compared by considering cases in which the critical quality characteristic presents a normal and non-normal distribution. Comparison results show that the proposed chart always outperforms the latter. ","Su-Fen Yang National Chengchi University, Taiwan, Chung-Ming Yang Ling Tung University, Taiwan, Sin-Hong Wu National Chengchi University, Taiwan",Tuesday 10:50-11:20,N
A Combined Estimating Function Approach to Count Data Regression Modeling,"A flexible semi-parametric regression model for autocorrelated count data is proposed. Unlike earlier models available in the literature, the model does not require construction of a likelihood function and only entails the specification of the first two conditional moments. A simple and flexible combined estimating function approach that makes efficient use of the information contained in the data is adopted for the model in the absence of a likelihood function. Simulation studies are conducted to study the performance of the semi-parametric method when the likelihood is either correctly specified or misspecified. The methodology is illustrated using a monthly polio counts dataset.","Melody Ghahramani University of Winnipeg, Scott S. White University of Winnipeg, Alexander R. de Leon University of Calgary",Tuesday 11:20-11:50,N
Adaptive Functional Regression with Manifold Structures,"Statistical methods that adapt to unknown population structures are attractive due to both numerical and theoretical advantages over their non-adaptive counterparts. We contribute to adaptive modelling of functional regression, where challenges arise from the infinite dimensionality in their underlying spaces. We are interested in the scenario where the functional predictor process lies in a nonlinear manifold that is intrinsically finite-dimensional and embedded in an infinite-dimensional function space. We proposed a novel approach built upon local linear manifold smoothing that achieves a polynomial convergence rate that adapts to contamination level and intrinsic manifold dimension even when functional data are observed intermittently and contaminated by noise, in contrast to the logarithm rate in nonparametric functional regression literature. We demonstrate that the proposal enjoys favourable finite sample performance relative to commonly used methods via simulated and real examples.","Zhenhua Lin University of Toronto, Fang Yao University of Toronto",Tuesday 10:20-10:50,N
Parametric Functional Principal Component Analysis,"Functional principal component analysis (FPCA) is a popular approach in functional data analysis to explore major sources of variation in a sample of random curves. These major sources of variation are represented by functional principal components (FPCs). Most existing FPCA approaches use a set of flexible basis functions such as B-spline basis to represent the FPCs, and control the smoothness of the FPCs by adding roughness penalties. However, the flexible representations pose difficulties in interpretation. We consider a variety of applications of FPCA and find that, in many situations, the shapes of top FPCs are simple enough to be approximated using simple parametric functions. We propose a parametric approach to estimate the top FPCs to enhance their interpretability for users. Our parametric approach can circumvent the smoothing parameter selecting process in conventional nonparametric FPCAs. Our simulation study shows that the proposed parametric FPCA is more robust when outlier curves exist. The parametric FPCA is demonstrated in several datasets from a variety of settings.","Jiguo Cao Simon Fraser University, Peijun Sang Simon Fraser University, Liangliang Wang Simon Fraser University",Tuesday 10:50-11:20,N
Spatial Quantile Regression Models for High-Dimensional Imaging Data,"We aim to develop a spatial quantile regression framework for accurately quantifying high-dimensional image data conditional on some scalar predictors. This new framework allows us to delineate spatial quantile association between neuroimaging data and covariates, while explicitly modeling spatial dependence in neuroimaging data. Theoretically, we establish the minimax rates of convergence for the prediction risk under both fixed and random designs. We further develop efficient algorithms such as the ADMM and the primal-dual algorithm to estimate the varying coefficients. Our method is able to estimate the whole conditional distribution of the image response given the scalar covariates. Simulations and real data analysis are conducted to examine the finite-sample performance.","Linglong Kong University of Alberta, Zhengwu Zhang SAMSI, Xiao Wang Purdue University, Hongtu Zhu MD Anderson Cancer Centre",Tuesday 11:20-11:50,N
Analysis of Recurrent Events Data Based on Trend-Renewal Process,Recurrent events data arises in many biomedical and longitudinal studies when failure events can occur repeatedly for each subject during the follow-up time. We are interested in the gap times between recurrent events. We propose a semiparametric accelerated transform gap time model based on a trend-renewal process which contains a trend and a renewal component. We use the Buckley-James imputation approach to deal with censored transform gap times. The proposed estimators are shown to be consistent and asymptotically normal. Model diagnostic plots of residuals and a prediction method for predicting number of recurrent events given a specified covariate and follow-up time are also presented. Simulation studies are conducted to assess the finite sample performances of the proposed method. The proposed technique is demonstrated through an application to two real data sets. ,"Chien-Lin Mark Su McGill University, Russell Steele McGill University, Ian Shrier Lady Davis Institute, Jewish General Hospital, Montreal and McGill University",Tuesday 10:20-10:35,N
On the Mixtures of Length-biased Weibull Distributions,"In this talk, a new class of length-biased Weibull mixtures will be introduced and a review of its basic distributional properties will be given. As a generalization of the Erlang mixture distribution, the length-biased Weibull mixture distribution has an increased flexibility to fit various shapes of data distributions including heavy-tailed ones. Methods of statistical estimation and model selection will be discussed with applications on real catastrophic loss data sets.",Taehan Bae University of Regina,Tuesday 10:35-10:50,N
An Interval Censored MPR Survival Model,"We develop multi-parameter regression survival models for the interval censored survival data setting by means of the multi-parameter Weibull regression survival model which is wholly parametric and non-proportional hazards. We describe the model, develop the interval-censored likelihood with or without frailty involved, conduct a more detailed simulation study designed to investigate the effect of sample size and the proportion of right censored observations, and analyse data from a population-based study of survival from lung cancer conducted in the UK. We compare the findings obtained with an analysis of these data using right censored multi-parameter regression survival methods. To the best of our knowledge this is the first time the effect of interval censoring has been investigated in the MPR survival model setting. ","Defen Peng University of British Columbia and ICVHealth, Kevin Burke University of Limerick, Gilbert MacKenzie University of Limerick",Tuesday 10:50-11:05,N
Practical Considerations when Analyzing Small Samples of Discrete Survival Times Using the Grouped Relative Risk Model,"The grouped relative risk model (GRRM) is a popular semi-parametric model for analyzing discrete survival time data. The maximum likelihood estimators (MLEs) of the regression coefficients in this model are often asymptotically efficient relative to those based on a more restrictive, parametric model. However, in settings with a small number of sampling units, the usual properties of the MLEs are not assured. In this talk, we discuss computational issues that can arise when fitting a GRRM to small samples, and describe conditions under which the MLEs can be ill-behaved. We find that, overall, estimators based on a penalized score function behave substantially better than the MLEs in this setting and, in particular, can be far more efficient.","Rachel MacKay Altman Simon Fraser University, Andrew Henrey Simon Fraser University",Tuesday 11:05-11:20,N
 A Flexible Parametric Proportional Hazards Model for Time-to-Event Data,"Proportional hazard (PH) models can be formulated with or without assuming a distribution for survival times. The former assumption leads to parametric models, whereas the latter leads to the semi-parametric Cox model which is by far the most popular in survival analysis. However, a parametric model may lead to more efficient estimates than the Cox model under certain conditions. Only a few parametric models are closed under the PH assumption, the most common of which is the Weibull that accommodates only monotone hazard functions. We propose a three-parameter distribution, which is closed under the PH assumption. The model is flexible and parsimonious in the sense that it accommodates all four basic shapes of the hazard function (increasing, decreasing, unimodal and bathtub shape) at the small cost of estimating one additional parameter compared to the Weibull PH model. The model can be used in analyzing time-to-event data, recurrent event data and joint modeling of time-to-event and longitudinal data. Comparative studies based on real and simulated data reveal that the model can be valuable in adequately describing different types of time-to-event data.","Saima K. Khosa University of Saskatchewan, Shahedul A. Khan University of Saskatchewan",Tuesday 11:20-11:35,N
Empirical Likelihood and Semiparametric Inference with Survey Data,"Survey data are often collected under a complex sampling design and the finite population parameters are typically defined as the solution to the so-called population (census) estimating equations. In this paper, we address inferential problems where the population estimating equations also involve an unknown nuisance function which does not depend on the parameters. Survey weighted estimating equations with a semiparametric plug-in component are constructed for empirical likelihood based inferences. Maximum empirical likelihood estimators are introduced together with a set of sufficient conditions that ensure the root-n consistency and asymptotic normality of the estimators. Effects of using a plug-in estimator of the nuisance function on the empirical likelihood inference of the finite population parameters are examined. The proposed method is applicable to a wide range of problems, including the generalized Lorenz curve and the Gini measure of income inequalities. Numerical results based on simulated and real data are provided.","Puying Zhao University of Waterloo, David Haziza Universit$\acutee $ de Montr$\acutee $al, Changbao Wu University of Waterloo",Tuesday 10:20-10:35,N
The Relative Deviation from Predicted Values as a Tool to Prioritize Units for Failed Edit Follow-Up in IBSP,"The production of official statistics can be considerably hindered by errors in collected data. The process of correcting these errors is referred to as statistical data editing and has traditionally been an expensive and time consuming manual effort. However, it is becoming increasingly accepted in practice that correction of a small subset of influential errors is often sufficient to guarantee a good quality estimate and therefore, that additional correction of data past a certain threshold yields only minor improvements in quality. The concept of selective editing involves identifying these significant errors, estimating their impact on the final estimate and providing for a signal to halt editing operations when an error tolerance is reached. In this work, we compare several methods based on predicted values for estimating reporting errors and for prioritizing units for failed-edit follow-up in the context of the Integrated Business Statistics Program at Statistics Canada.","Matei Mireuta Statistics Canada, Jessica Andrews Statistics Canada, Business Surveys Methods Division (BSMD), Pierre Daoust Statistics Canada, Business Surveys Methods Division (BSMD)",Tuesday 10:35-10:50,N
Job Vacancy and Wage Survey: Balancing Sampling and Operational Requirements using the Cube Method,"Balanced sampling is a sampling method where the totals estimated with the Horvitz-Thompson estimator are the same or close to the true population totals for a given set of auxiliary variables on the survey frame. If the auxiliary variables are highly correlated to the variables of interest, the variances of the estimators for totals will be small. In Statistics Canada’s Job Vacancy and Wage Survey (JVWS), the quarterly sample of business locations has to be split into three monthly subsamples for data collection. In parallel, all locations under the same enterprise must be collected the same month. In this presentation, we will show how one can use the balanced sampling method to allocate the sampling units at the enterprise level in a way that keeps the number of employees and the number of locations balanced for each province and industry between months. This allocation strategy was implemented for the JVWS survey using the Cube method which was proposed by Deville and Tillé (2004).",Min Jiang Statistics Canada,Tuesday 10:50-11:05,N
Segmenting the Canadian Population to Remedy Non-Response in Statistics Canada Household Surveys,"Statistical agencies are facing the challenge of declining response rates with an increased demand for national statistics. We must find innovative ways to improve collection strategies while maintaining high-quality data. One innovation is to use characteristics associated with survey non-response in order to segment the population into homogenous clusters. Advertising campaigns and communication tools can then be tailored to specific clusters of the population, emphasizing areas with higher non-response. In this talk, we combine data from various household surveys with sociodemographic population characteristics from the 2011 Canadian Census in order to profile survey non-response. Logistic regressions identify characteristics associated with survey non-response and then a cluster analysis creates clusters of geographic areas having similar prevalence for these characteristics associated with non-response.","Amanda Halladay Statistics Canada, Ming Jie Yang Statistics Canada, François Brisebois Statistics Canada",Tuesday 11:05-11:20,N
The Collection Challenges of the 2016 Reverse Record Check Survey," The 2016 Reverse Record Check (RRC) Survey is used to estimate the number of in-scope persons who were missed by the 2016 Census of Population. The RRC is one of the most important and challenging surveys done by Statistics Canada. Successfully tracing and interviewing the persons selected in the RRC sample is one of the biggest challenges, even though several sources of administrative data are used to provide more information to find these persons. The RRC collection operations faced quite a few challenges for the 2016 survey, including the redesign of the survey contents and of the multi-mode collection methodology, a new Computer Assisted Telephone Interview (CATI) application and a new tracing application. This article describes the 2016 RRC collection challenges and strategies. ",Jane Wang Statistics Canada,Tuesday 11:20-11:35,N
Methods of Selecting a Person within a Household: Mailed Invitations for an Electronic Questionnaire,"Currently, within-household selection for electronic questionnaire surveys consists of sending a paper invitation to a random sample of households asking for an online roster of the occupants. Then, a random selection of a person is made within the electronic questionnaire. A household with more than one occupant might require two people to connect to the application. There is also a risk that the second person does not want to answer, which creates non-response even if the first person completed their part. The challenges of the roster method raised some questions. What if the selected person could instead be determined from instructions on the mailed invitation? Would it be clear enough so people know who is selected? Will they follow these instructions? To answer these questions, the last-birthday method and a new version of the age-order method were tested and compared to the roster method. Response rates and selection inaccuracy rates were analyzed, and results will be presented.","Gabrielle Poirier Statistics Canada, Kevin Bosa Statistics Canada, François Gagnon Statistics Canada",Tuesday 11:35-11:50,N
"Joint Modelling of Count, Continuous and Semi-Continuous Longitudinal Data","In medical studies, different types of outcomes are frequently collected over time on each of many subjects. For example, both CD4 cell counts and viral loads of patients are usually observed longitudinally in human immunodeficiency virus (HIV) research. Therefore, the effectiveness of treatments or interventions can be profiled based on these bivariate responses while accounting for their association. As the nature of relationships between these two outcomes are of particular interest, these two outcomes should be analyzed jointly instead of separately. In this study, we propose to model data of mixed types jointly by incorporating both subject-specific and temporally dependent random effects into Tweedie models of different index parameters. An optimal estimation of our model has been developed using orthodox best linear unbiased predictor of random effects. Our analysis results do not rely on any distributional assumption of random effects.","Jiaxiu Li University of New Brunswick, Guohua Yan University of New Brunswick, Renjun Ma University of New Brunswick",Tuesday 10:20-10:35,N
Efficiently Computing the Projection Median in $R^3$,"This talk presents efficient algorithms for computing the projection median exactly in $R^3$. The projection median, first introduced by Durocher and Kirkpatrick (2009), is a robust, non-parametric, descriptor of multivariate location. Computing the projection median in $R^d$ involves averaging projections over infinite directions on the d-dimensional unit sphere. Previously, only approximation algorithms existed for computing the projection median for d>2. Our algorithm begins by transforming the problem from computing this average over infinite directions to computing the median level in an arrangement of a finite set of hyperplanes. Next, the randomized algorithm of Agarwal et al. (1998) for calculating the median level in an arrangement of planes is applied, before transforming the solution back to the original setting. Exact and efficient computation opens the door to practical use of this statistic in higher dimensions, as well as further study on the properties of the projection median.","Kelly Ramsay University of Manitoba, Stephane Durocher University of Manitoba, Alexandre Leblanc University of Manitoba",Tuesday 10:35-10:50,N
Common-Factor Multivariate Stochastic Volatility Modeling with Observable Proxy,"Multivariate modeling of financial assets often plays a critical role in portfolio analysis and risk management. While Stochastic Volatility (SV) models have enjoyed enormous success in the univariate setting, multivariate SV models are more challenging to design and implement, owing to the large amount of latent variables and the need for correlations to satisfy a positive-definiteness constraint. We propose a common-factor multivariate SV model, naturally extending the univariate case, with an interpretable hierarchical correlation structure for which positive-definiteness is guaranteed. We show how the common factor can be flexibly proxied by observable volatilities, resulting in enormous computational savings for parameter inference. The model is used to conduct a multivariate SV analysis for several major components of the S\&P500. ","Yizhou Fang University of Waterloo, Martin Lysy University of Waterloo, Don Mcleish University of Waterloo",Tuesday 10:50-11:05,N
Superfast Inference for Stationary Gaussian Processes,"Stationary Gaussian processes are popular models in many areas of statistical applications. When the observations are regularly spaced, the structure of the variance matrix admits ""fast"" likelihood evaluations via the famous Durbin-Levinson algorithm, scaling as $O(N^2)$ in the number of observations instead of the usual $O(N^3)$. Here we adapt the lesser-known Generalized Schur algorithm to further reduce this cost to $O(N log^2 N)$. Our ``superfast"" method extends to both gradient and hessian calculations, and thus is applicable to many inference algorithms Bayesian and Frequentist alike. Our implementation is available via the R package ``SuperGauss"", beating Durbin-Levinson around N = 300. We present an application to Gaussian process factor analysis.","Yun Ling University of Waterloo, Martin Lysy University of Waterloo",Tuesday 11:05-11:20,N
Strong Heredity Penalized Regression Models for Non-Linear Gene-Environment Interactions,"Diseases are now thought to be the result of changes in entire biological networks whose states are affected by a complex interaction of genetic and environmental factors. In general, power to estimate interactions is low, the number of possible interactions could be enormous and their effects may be non-linear. Existing approaches such as the lasso might keep an interaction but remove a main effect, which is problematic for interpretation. We develop a model for linear and non-linear interactions in penalized regression models that automatically enforces the strong heredity property. A computationally efficient fitting algorithm combined with a non-parametric screening approach scales to high-dimensional datasets and has been implemented in an R package. We apply our method to identify gene-prenatal maternal depression interactions on negative emotionality in mother–infant dyads from the Maternal Adversity, Vulnerability, and Neurodevelopment (MAVAN) cohort.","Sahir R. Bhatnagar McGill, Yi Yang McGill University, Alexia Jolicoeur-Martineau Jewish General Hospital, Ashley Wazana McGill University, Celia M.T. Greenwood Lady Davis Institute, Jewish General Hospital, Montreal and McGill University",Tuesday 11:20-11:35,N
Computational Neuroscience: A Romance of Many Disciplines,"Computational neuroscience studies the brain from an information processing point of view. A strong body of literature suggests that the nervous system has a fundamental stochasticity, making statistical models very attractive in computational neuroscience. This being said, the adage “all models are wrong, but some are useful” applies. Among the useful is a collection of data-driven models tailored to understand neurophysiological phenomena and the process of information coding in the brain. This talk reviews, briefly, some of the common statistical methods and recent advancements in the analysis of neural spike trains. Data visualization, computational challenges, biological justification, and performance of these models will also be discussed in addition to exciting opportunities for future research.","Reza Ramezan California State University, Fullerton",Tuesday 11:35-11:50,N
Nonparametric Estimation in a Compound Mixture Model with Application to a Malaria Study,"Malaria can be diagnosed by the presence of parasites and symptoms (usually fever). However, an individual may have fever attributable either to malaria or to other causes. As such, the parasite level of an individual with fever follows a two-component mixture distribution. Further, the parasite levels of some nonmalaria individuals are exactly zero so that the distribution of the parasite levels in nonmalaria individuals also follows a mixture distribution. We propose a maximum multinomial likelihood approach for estimating the proportion of clinical malaria using parasite-level data from two groups of individuals. The first is collected from the endemic area, whereas the second is from the community, where all parasite levels are from nonmalaria individuals. We develop an EM-algorithm and show its convergence locally. Simulations show that the proposed estimator is more efficient than existing nonparametric estimators using only the frequencies of zero and nonzero data. The proposed method is used to analyze data from a malaria survey carried out in Tanzania.","Zhaoyang Tian University of Waterloo, Pengfei Li University of Waterloo, Kun Liang University of Waterloo",Tuesday 10:20-10:35,N
Investigating the Impact of Design Characteristics on Statistical Efficiency within Discrete Choice Experiments: A Systematic Survey,"This systematic survey aimed to review simulation studies of discrete choice experiments (DCEs) to determine what design features affect statistical efficiency. Electronic searches were conducted in JSTOR, Science Direct, PubMed and OVID. Screening and data extraction were performed independently and in duplicate. The reporting quality of simulation studies were also evaluated in each study. From 371 potentially relevant studies, 9 proved eligible. Studies showed statistical efficiency improved when: increasing the number of choice tasks or alternatives; decreasing the number of attributes, attribute levels, or overlaps; incorporating response behaviour or heterogeneity; correctly specifying Bayesian priors; minimizing prior variances; or matching the method to the research question. Studies need to improve reporting of: study objectives, failures, random number generators, starting seeds, and software. These results may help to inform investigators during DCE design creation.","Thuva Vanniyasingam McMaster University, Caitlin Daly McMaster University, Xuejing Jin McMaster University, Yuan Zhang McMaster University, Charles Cunningham McMaster University, Gary Foster McMaster University, Lehana Thabane McMaster University",Tuesday 10:35-10:50,N
Geo-Dependent Individual-Level Models for Infectious Diseases Transmission,"In recent years, a class of complex statistical models, known as individual-level models (ILMs), have been effectively used to model infectious disease transmission in discrete time. These models are well developed but assume the probability of disease transmission between two individuals depends only on their spatial (or network-based) separation and not on their spatial locations. However, spatially varying demographic and environmental factors could influence the disease transmission. Thus, it might be useful to incorporate the effect of the spatial location itself into the ILMs. In this study, we extend ILMs to Geo-dependent ILMs (GD-ILMs) that allow the evaluation of the effect of spatially varying social risk factors, environmental factors as well as unobserved spatial structure, upon the transmission of infectious disease. We consider a conditional autoregressive (CAR) model to capture the effects of unobserved spatially structured latent covariates or measurement error. We show how GD-ILMs can be fitted to data within a Bayesian statistical framework using Markov chain Monte Carlo (MCMC) methods. ","Md. Mahsin University of Calgary, Rob Deardon University of Calgary",Tuesday 10:50-11:05,N
Seasonality in Influenza Mortality: Moving Beyond the Annual Cycle,"Influenza is an infectious disease, and its periodic patterns are commonly modeled focusing on a yearly cycle. However, considering that four pandemics have occurred in the past century, this poses the question of whether there could be any hidden patterns. Using times series analysis and multitaper spectral analysis techniques, an investigation of the mortality due to Influenza that occurred in the United States from 1910 to 2016 has shown some interesting features. Periodic signals of a one-year period were found to be significant, as well as signals in the low frequency range, near 30 years. The analysis also revealed significant frequencies of 2, 4 and 5 cycles per year. It is then possible that these frequencies have a biological connection to influenza, or that they are an artifact created by the fact that the yearly cycle is not absolutely identical each year. It then follows that there might be a more complex periodic structure to this influenza mortality data. ","Claire Boteler Queen's University, David Thomson Queen's University, Troy Day Queen's University",Tuesday 11:05-11:20,N
Benefits of Transformations in the Analysis of Health Utility Data,"Health utilities represent a single global measure of Health Related Quality of Life and are necessary for economic evaluation of health interventions. Their data distribution is semi-continuous, skewed and leptokurtic with upper bound at 1 and probability mass at the bound. Linear regression is not appropriate for the analysis of these data, since many assumptions (linearity, homoscedasticity and normality) are violated. Here we investigate the benefits of applying a mathematical transformation to the response variable, before fitting a linear regression model. We use simulated and real health utility data to compare a number of transformations with the untransformed model, using several measures of model accuracy and goodness of fit. Our investigation identifies the benefits of transformations in this context, offering suggestions for future analysis and modeling of health utilities. ",Nicholas Mitsakakis University of Toronto,Tuesday 11:20-11:35,N
Progressively Type-II Censored Competing Risks Data from the Linear Exponential Distribution,"We consider competing risks under a progressively type-II censored sample where the competing risks each follow a linear exponential law. We develop likelihood inference when there is a known number of competing risks. To demonstrate the performance of the maximum likelihood estimates, we consider the case of two competing risks and carry out an extensive simulation study. We also apply our inferential method to a real data set.","Katherine Davies University of Manitoba, William Volterman Syracuse University",Tuesday 11:35-11:50,N
High Dimensional Clustering: tHDDC,"High-dimensional data are collected daily in many fields of work, such as DNA analysis and social media data analysis. The presence of clusters in a data set can be crucial to discovering new trends and patterns, e.g., disease subtypes. Therefore, it is necessary to have reliable and quick techniques which can detect and define clusters. Model-based clustering is a very popular method and uses a mixture of distributions where each component density corresponds to a cluster. The most common model-based clustering techniques are based on using a mixture of multivariate normal distributions. A method called high-dimensional data clustering (HDDC) has given rise to a very computationally efficient family of Gaussian mixture models for high-dimensional data. HDDC is based on the idea that high-dimensional data can be represented in much lower-dimensional subspaces. The HDDC family of models has gained vast attention due to its superior performance compared to other families of mixture models. We propose a t-analogue of this family, which we call the tHDDC family. The tHDDC family extends the high-dimensional data clustering models to include the multivariate-t distribution.","Angelina Pesevski McMaster University, Brian Franczak MacEwan University, Paul D. McNicholas McMaster University",Tuesday 12:00-17:30,N
Stop Signal Reaction Time: A New Weighted Frequentist Estimation Method,"The stop signal reaction time (SSRT), a measure of the latency of the stop signal process, has been theoretically formulated using a horse race model of go and stop signal processes by the American scientist Gordon Logan in 1994. Current analysis from a general population sample of 13696 children age 6-17 in Toronto demonstrated significantly higher probability of successful inhibit on stop trials following a stop trial compared to those following a go trial, suggesting the need for a modified weighted mixture approach to estimate SSRT. Our Results showed: (i) Significantly reduced go response times after stop trials versus after correct go trials where mean go response times were assumed to follow an Ex-Gaussian distribution (Mean Difference=53.4 ms; p-value<0.0001); (ii) Significantly larger estimates of SSRT using a pooled estimate weighted by trial type versus the original Logan 1994 method with an Ex-Gaussian distribution assumption (Mean Difference=7.6 ms; p-value<0.0001). ","Mohsen Soltanifar, Annie Dupuis Clinical Research Services, Sickkids Hospital, Toronto, Canada, Russell Schachar Psychiatry Research, Neuroscience and Mental Health, Sickkids Hospital, Toronto, Canada, Michael Escobar Biostatistics Division, Dalla Lana School of Public Health, University of Toronto, Canada",Tuesday 12:00-17:30,N
Advanced Regression Models for Automated Valuation Model,"The automated valuation model (AVM) is a mathematical program to estimate the market value of real estates based on the real estate data. Currently, boosting is a popular predictive approach in AVM. In 2006, Geoffrey Hinton ignited the popularity of neural networks by showing substantially better performance with a ``deep"" neural network. However, the application of deep learning algorithm has not been fully explored in the AVM market. In this paper, we investigated advanced statistical learning approaches for AVM. With the linear regression model as the reference, this study investigated the accuracy of advanced statistical learning approaches such as boosting, random forest, support vector regression and other models for real estate data. Moreover, a constructing deep belief networks (DBN) for regression is considered in this application. An ensemble approach to integrate deep learning method and other statistical learning methods for AVM is also investigated in this research.","Junchi Bin University of British Columbia - Okanagan, Bryan Gardiner Data Nerds, Zheng Liu University of British Columbia (Okanagan)",Tuesday 12:00-17:30,N
Genetic Risk Prediction for Type X Colorectal Cancer Families,"Type X colorectal cancer (CRC) occurs in individuals at approximately age 50, and they typically have at least two affected relatives. No major gene has been found so far to explain the clustering of CRC in these families. We will investigate several genetic models for the familial Type X CRC phenotype by carrying out a complex segregation analysis on Type X pedigree data, which will provide evidence in favour of a major gene, polygenic or environmental component. Furthermore, we will incorporate a polygenic threshold model into a genetic risk model to estimate the probability that an individual is a carrier of the disease-causing genetic component. We predict that individuals with a family history of Type X CRC will have an increased probability of carrying the genetic component, and thus have a higher chance of being diagnosed with colorectal cancer. Ultimately, this research will have applications in genetic counselling and cancer prevention strategies such as screening guidelines.","Tayler Dawn Scory University of Calgary, Karen Kopciuk Alberta Health Services",Tuesday 12:00-17:30,N
Modeling and Treatment of Surveillance Flu Data ,"In this paper, we propose a novel look at the CDC (Centers for Disease Control and Prevention, USA) surveillance data on yearly influenza outbreaks. First we apply a flexible model, based on a non-homogeneous birth and death process, which allows to estimate a lower bound to the basic reproduction ratio, an indicator of the transmission potential of an epidemic. Further, we explore various ways of comparing such data across years and regions by resorting to a conditional model which allows to circumvent some of the drawbacks of the original data. In particular, we apply some standard procedures in mixture modeling in order to assess whether two outbreaks are similar, for example if they attain their peak at the same time. ","René Ferland Université du Québec à Montréal, Sorana Froda Université du Québec à Montréal, Anthony Coache Université du Québec à Montréal",Tuesday 12:00-17:30,N
Estimating a Disease Prevalence with Group Testing Data and Imperfect Assays,"Infectious disease assays can be imperfect. When estimating disease prevalence, this imperfection is accounted for by incorporating assay sensitivity and specificity into point and variance estimates. Unfortunately, these accuracy measures are often treated as fixed constants, rather than taking into account that they are actually estimates from an assay validation process. The purpose of this presentation is to show the detrimental effect of not taking into account this sampling variability when samples are obtained through group testing (a.k.a., pooled testing). We show that confidence interval coverage can dramatically decline as the sample size increases for the main sample of interest. As a remedy for this problem, we propose a new confidence interval which takes into account the extra sampling variability. This new interval is shown to obtain coverage near the nominal level.","Dola Pathak University of Nebraska at Lincoln, Christopher Bilder University of Nebraska at Lincoln",Tuesday 12:00-17:30,N
Modelling the Effects of Memory on Bumblebee Pollination Services,"Experimental and observational studies have shown that bumblebees return faithfully to particular forage locations, which can have a significant effect on the pollination services they provide. This research project develops stochastic models for bumblebees with and without spatial memory to investigate the resulting pollination services. The topics of memory in movement models and pollination services have previously been researched separately, but have not yet been combined. In this work, movement is modelled with a combination of correlated and uncorrelated random walks, in an agent based model inspired by partial differential equations and informed by recent bumblebee radar tracking studies. Pollination services are quantified by flower visitation rates, tracked as the bumblebee moves throughout the landscape. The results of this work will inform future modelling work on the landscape requirements for optimal pollination of blueberry crops by bumblebees.","Sarah A. MacQueen University of British Columbia Okanagan, Rebecca Tyson University of British Columbia Okanagan",Tuesday 12:00-17:30,N
Spatial Mixed Model for Binary Response with an Application to Single Low-Density Lipoprotein Dyslipidemia,"Low-density lipoprotein (LDL) dyslipidemia, LDL$\geq$3.4 mmol/L, is one of the leading factors contributing to cardiovascular diseases. Previous studies suggest regional variations in LDL-dyslipidemia; however, random effect due to geography could cause overdispersion in binary responses. To investigate the variability of spatial random effect and how factors impact the binary response, we fitted spatial mixed model to LDL dyslipidemia, a binary response from the individuals in local regions (n=80) in Newfoundland and Labrador (NL). Using the data from Laboratory Information System in 2014, among adults who had a LDL test (N=170,506), 32 \% had LDL-dyslipidemia. Using method of moments, we are assessing the variability of the random component. The responses of the individuals of a given geographical region could be correlated through a common random effect shared by the individuals. Spatial mixed model is useful by obtaining consistent and efficient estimates of the parameters of the model. ","Hensley Hubert Mariathas Memorial University of Newfoundland, Shabnam Asghari Memorial University of Newfoundland, Alvin Simms Memorial University of Newfoundland, Masoud Mahdavian Memorial University of Newfoundland, Oliver Hurley Memorial University of Newfoundland",Tuesday 12:00-17:30,N
A Time Series Approach for the Underlying Driven Process in Stocks and its Large Sample Theory,"An additive structure of multivariate GARCH type model is proposed to describe a dynamic common driven process in stocks and indices. The observable sequence is divided into two parts, a common risk term and an individual risk term, both following a GARCH type structure. The conditional volatilities of all stocks can increase dramatically together because of a sudden peak in the common volatility. We provide sufficient conditions for the strict stationarity and ergodicity of the model. All the parameters in the model are identifiable in terms of the conditional second moments under mild assumptions. Under the general assumptions we proposed, without specifying the distribution of the innovation, different initial values would lead to the same estimates asymptotically. Sufficient conditions for the strong consistency and asymptotic normality of the quasi maximum likelihood estimator (QMLE) are proposed.","Jingjia Chu University of Western Ontario, Reg Kulperger University of Western Ontario, Hao Yu University of Western Ontario",Tuesday 12:00-17:30,N
A Second Chance? - A SoTL Project on Implementing Midterm Corrections in a Large Introductory-Level Statistics Course,"A common practice students do towards assessments such as quizzes and/or exams is to look at the grade and file them away. In this case, the learning potential of assessments is not fully reached. Assessment corrections, which allow students a second chance to improve their performance and protect their GPA and thus reducing the D/Fail/Withdraw (DFW) rates, actually encourage students to identify their knowledge gaps and try to fill these gaps up. It is expected that by doing corrections students not only gain deeper understanding of the course materials but also accumulate confidence in their learning skills. The main goal of this poster presentation is to disseminate the research findings of a scholarship of teaching and learning (SoTL) project which aimed to examine the impact of midterm corrections on student learning in a large service course setup. ","Bingrui (Cindy) Sun University of Calgary, MohanaGowri Arumugam University of Calgary",Tuesday 12:00-17:00,N
Discriminant Analysis for Longitudinal Data,"Various approaches for discriminant analysis of longitudinal data are investigated, with some focus on model-based approaches. The latter are typically based on the modified Cholesky decomposition of the covariance matrix in a Gaussian mixture; however, non-Gaussian mixtures are also considered. Where applicable, the Bayesian information criterion is used to select the number of components per class. The various approaches are demonstrated on real and simulated data.","Kevin Matira McMaster University, Paul D. McNicholas McMaster University",Tuesday 12:00-17:30,N
Sensitivity to Model Misspecification of a Von Bertalanffy Growth Model with Measurement Error in Age,"The Von Bertalanffy growth function (VoB) specifies the length of fish as a function of age. However, in practice age is measured with error. We study the structural errors-in-variables (SEV) approach to account for ageing error. Recent research has proposed this approach for fish growth data. They assumed the distribution of true unobserved age was a Gamma distribution. They showed that this particular SEV approach provided improved regression parameter inferences compared to the standard nonlinear estimation approach that does not account for covariate measurement error (ME). In this presentation we investigate whether SEV VoB parameter estimators are robust to misspecification of the gamma true age distribution. By robust we mean no large sample bias. We outline a numerical method for evaluating the large sample bias in SEV VoB parameter estimators. Our simulation results demonstrate that the SEV VoB using a gamma distribution for true age is not robust. When ME in age is small, the misspecification bias is low. However, for large ME the bias of estimators may be large.","Rajib Dey Memorial University of Newfoundland, Noel Cadigan Fisheries and Marine Institute of Memorial University of Newfoundland, Taraneh Abarin Memorial University of Newfoundland",Tuesday 12:00-17:30,N
On Identifiability of Regression Models with Interactions,"Before performing any statistical inference on regression model parameters, we need to know whether the parameters of interest are ``estimable"" or ``identifiable"". A model is said to be identifiable if all the unknown parameters of the model can be estimated uniquely provided data. We consider different interaction models, both with and without measurement error. We will look at different measurement error models such as the Berkson and classic models, and apply some remedies for non-identifiability such as instrumental variables as well as replicated and validated data.","Sahar Arshadi Memorial University of Newfoundland, Taraneh Abarin Memorial University of Newfoundland",Tuesday 12:00-17:30,N
Clustering Discrete Valued Time Series,"There exists a need for the development of models that are able to account for discreteness in data, along with its time series properties and correlation. A review of the application of thinning operators to adapt the ARMA recursion to the integer-valued case is first discussed. A class of integer-valued ARMA (INARMA) models arises from this application. Our focus falls on INteger-valued AutoRegressive (INAR) type models. The INAR type models can be used in conjunction with existing model-based clustering techniques to cluster discrete valued time series data. This approach is then illustrated with the addition of autocorrelations. With the use of a finite mixture model, several existing techniques such as the selection of the number of clusters, estimation using expectation-maximization and model selection are applicable. The proposed model is then demonstrated on real data to illustrate its clustering applications.","Tyler Roick McMaster University, Dimitris Karlis Athens University, Paul D. McNicholas McMaster University",Tuesday 12:00-17:30,N
Local Influence Analysis of a State-Space Fish Stock Assessment Model Compared to Conventional Modelling Approaches,"Surplus production models provide simple analytical methods of assessing fish populations by taking the annual biomass, the growth rate and the carrying capacity into account. However, these simple models may not adequately reflect fish stock dynamics that can be substantially more complex with age and length specific birth, growth, and death processes at play. To account for this, process errors can be included in the production model in a state-space modelling framework. In this paper, we compare the sensitivity of estimators of state-space and conventional nonlinear production models (without process errors) using the local influence analysis method introduced by R.D. Cook (1986). We apply these diagnostics to different fish stocks to assess how estimated parameters respond to small perturbations of the data. Our diagnostics reveal that the conventional model parameter estimators are more sensitive to small changes in the input data compared to the state space model estimators.","Prageeth Manujaya Senadeera Memorial University of Newfoundland, Noel Cadigan Marine Institute of Memorial University of Newfoundland",Tuesday 12:00-17:30,N
A Power Comparison of Robust Tests for Monotone Trend in Recurrent Event Processes,"The analysis of the existence and form of time trends in repairable systems is an important issue in reliability studies. Hence, many trend tests have been proposed and studied in the literature. There has been a recent interest in developing robust trend tests based on the robust estimating function approach. These tests are appealing because they are powerful in a range of settings. In this study, we consider monotone time trends in recurrent event data from repairable systems, and develop a robust trend test based on rate functions of the power law process. Our main goal is to discuss the power of robust trend tests as well as to compare their power with other well-known trend tests under various settings. We conduct extensive Monte Carlo simulations to compute and compare the power of these tests under various cases. Finally, we analyze a data set from industry to illustrate the methodology. ","Jiajia Yue Memorial University of Newfoundland, Candemir Cigsar Memorial University of Newfoundland",Tuesday 12:00-17:30,N
Mixture Model Averaging for Clustering ,"Cluster analysis is commonly described as the classification of unlabeled observations into groups such that they are more similar to one another than to observations in other groups. Model-based clustering assumes that data arise from a statistical (mixture) model. In most clustering applications, it is common to fit several models in a family and only report the results from the ‘best’ model chosen using a selection criterion, often the Bayesian information criterion. Recent interest has been placed on selecting a subset of solutions that are close to the ‘best’ model and averaging these to create a weighted averaging of clustering results. Two averaging approaches are explored, and both use Occam’s window to select models that are in some sense close to the best one. In one of the methods the a posteriori probabilities are averaged and in the other method the models’ parameters are averaged to generate a single interpretable model. The efficacy of these model-based averaging approaches are demonstrated for a family of skew-t mixture models using real and simulated data.","Sarah Ricciuti McMaster University, Paul D. McNicholas McMaster University",Tuesday 12:00-17:30,N
A Relational Model for Predicting Farm-Level Crop Yield Distributions in the Absence of Farm-Level Data,"Central to designing and delivering an individual crop insurance program are historical individual farm-level yields, which serve as the foundation for setting coverage levels and rates. However, data scarcity and credibility, particularly lack of farm-level yield data, make it difficult to calculate individual expected losses. As a result, aggregate county-level data are often used to establish a base premium rate, and this may contribute to adverse selection, and thus, program losses. I develop a new relational model to predict farm-level crop yield distributions in the absence of farm-level yield data, to improve the accuracy of computing crop insurance premium rates. The relational model developed defines a similarity measure based on a Euclidean distance metric to select an optimal county from a reference country, from which farm-level yield data are ""borrowed"". An empirical analysis shows that the relational model achieves lower mean and standard deviation prediction errors compared to the benchmark model, and is able to recover the actual premium rates more closely.","Lysa Porth University of Manitoba, Ken Seng Tan University of Waterloo, Wenjun Zhu Nankai University",Tuesday 13:30-14:00,N
Full-Range Tail Dependence Copulas,"In this talk, I will introduce some flexible new bivariate copulas, the R package CopulaOne, and a few applications for data analytics in insurance and finance. Popular multivariate copulas such as vine copulas and factor copulas are constructed based on bivariate copulas. An ideal bivariate copula should have the following features. First, both upper and lower tails are able to explain full-range tail dependence. That is, the dependence in each tail can range among quadrant tail independence, intermediate tail dependence, and usual tail dependence. Second, it can capture upper and lower tail dependence patterns that are either the same or different. In this talk, I will discuss a general approach for constructing copulas that have the above features. Some promising parametric copula families are to be presented, and both the ideal features and the computational speeds were considered when constructing the copulas. Finally, a few applications using the copulas are to be demonstrated.","Lei Hua, Jianxi Su Purdue University",Tuesday 14:00-14:30,N
Aggregate Claim Analysis in a Two-sided Exit Setting with Dependence,"The two-sided exit problem has been the subject of risk management analysis, used to better understand the dynamic of various insurance risk processes. In the two-sided exit setting, the discounted aggregate claims are investigated under a dependent renewal process (also known as dependent Sparre Andersen risk process). Utilizing Lundberg's generalized equation and Laplace transform, we identify the fundamental solutions to a given integral equation, which will be shown to play a role similar to the scale matrix for spectrally-negative Markov-additive processes. Explicit expressions and recursions are then identified for the two-sided probabilities and the moments of the aggregate claims respectively. A numerical example for the two-sided exit probabilities involving the Farlie-Gumbel-Morgenstern (FGM) copula is provided.","Di (Cindy) Xu University of Nebraska-Lincoln, David Landriault University of Waterloo, Bin Li University of Waterloo",Tuesday 14:30-15:00,N
Improving Risk Assessment using Market Expectations and Information from Model Risks,"Market Risk is usually assessed by means of point estimates for risk measures which are exposed to randomness in data and/or misspecification of underlying model assumptions. In this paper, we extent the view of risk measurement on the distribution of estimators for risk measure forecasts to account for possible model risk. In addition, we analyze to which degree unexpected economic losses may be reduced when using information from the risk measure estimator distribution and combine it with future market expectations. The idea behind this approach is that more conservative risk measure forecasts than the point estimate of the estimator's distribution should be used when market expectations point to times of financial turmoil.",Ralf Kellner University of Regensburg,Tuesday 13:30-14:15,N
Stochastic Correlation and Regulatory Capital,"Current banking regulations require banks to use a so-called risk-weight function to determine the amount of capital that is held as a buffer against extraordinary losses. The function is based on a simple and intuitive model developed by Vasicek (2002), which makes several strong simplifying assumptions but retains its popularity due to its intuitive appeal (workarounds to some of the shortcomings caused by these assumptions have been proposed in the literature and adopted in practice). One of these assumptions is that correlations do not vary with the business cycle, which is in stark contrast to empirical evidence. In this talk we generalize the Vasicek model to allow for correlations that vary systematically with the state of the economy. The model includes a parameter that allows the user to control the degree to which correlations to rise during adverse economic scenarios, and we demonstrate that this parameter has a profound impact on regulatory capital calculations.",Adam Metzler Wilfrid Laurier University,Tuesday 14:15-15:00,N
Robust Small Area Estimation in Generalized Linear Models,"In this talk, I will present novel methods for small area estimation when the outcome variable is discrete. The methods developed in the framework of the generalized linear mixed models for clustered correlated data will be robust against slight deviations from the underlying distributions for both outcome and auxiliary variables. The empirical properties of the proposed estimators will be studied using Monte Carlo simulations. An application will also be provided using actual survey data.",Sanjoy Kumar Sinha Carleton University,Tuesday 13:30-14:00,N
Small Area Model Selection,"Complex statistical models that have multiple sources of dependencies and variability in the observations are of primary importance in studying data from multiple disciplines. These include spatial, temporal, spatio-temporal, various mixed effects and other statistical models. Of special importance among such models are those that are useful for studying problems where there is limited directly observed data, for example, as in small area models. In this talk we present a new resampling-based method that can be used for simultaneous variable selection and inference in several complex models, including small area and other mixed effects models. Theoretical results justifying the proposed resampling schemes will be presented, followed by simulations and real data examples. This talk is based on research involving several students and collaborators from multiple institutions.","Snigdhansu Chatterjee, Kaibo Gong University of Minnesota, Megan Heyman Rose-Hulman Inst of Technology, Taps Maiti Michigan State University",Tuesday 14:00-14:30,N
Robust Hierarchical Bayes Small Area Estimation for Nested Error Regression Model,"National statistical offices are mandated to produce reliable statistics for important characteristics for many sub-populations or small areas, defined by geography and/or demography. Model-based methods that ``borrow strength"" from other areas and variables are extensively used to generate reliable statistics. Standard model-based small area estimates perform poorly in presence of outliers. Sinha and Rao (2009) proposed a robust frequentist approach to handle outliers. We propose a robust hierarchical Bayes (HB) method to handle outliers in unit-level data. We consider a two-component scale mixture of normal distributions for the unit error to model outliers and produce noninformative HB predictors of small area means. An example and extensive simulations convincingly show robustness of HB predictors. Simulation evaluation of these two procedures shows their superiority over the M-quantile small area estimators. Our HB procedure enjoys dual (Bayes and frequentist) dominance.","Gauri Datta U.S. Bureau of the Census, Abhyuday Mandal University of Georgia, Adrijo Chakraborty NORC",Tuesday 14:30-15:00,N
"The Randomized Registry Trial — Opportunities, Challenges and Solutions ","Registry-based randomized controlled trials (RRCTs) are defined as pragmatic trials that use registries as a platform for case records, data collection, randomization and follow-up. Recently the application of RRCTs has attracted increasing attention in health research to address comparative effectiveness research questions in real-world settings, mainly due to their low cost, enhanced generalizability of findings, rapid consecutive enrollment, and the potential completeness of follow-up for the reference population, when compared with conventional randomized effectiveness trials. However, several challenges of RRCTs have to be taken into consideration, including registry data quality, ethical issues and methodological challenges. In this talk, I will summarize the advantages and challenges and areas for future research related to RRCTs. ",Lehana Thabane McMaster University,Tuesday 13:30-14:00,N
Registry-Based Randomized Controlled Trials: The Case for Explanatory-Pragmatic Continuum,"Randomized controlled trials (RCTs) have been broadly categorized as either having a pragmatic or explanatory attitude. Pragmatic trials are designed to evaluate the effectiveness of interventions in real-life routine practice conditions, whereas explanatory trials aim to test whether an intervention works under optimal situations. In this talk, I will discuss the development of a Pragmatic-Explanatory Continuum Indicator Summary (PRECIS) for classifying trials and its application to registry-based trials. ",Kevin E. Thorpe University of Toronto,Tuesday 14:00-14:30,N
Statistical Methods for Synthesizing Evidence from Explanatory and Pragmatic trials,"Randomized controlled trials (RCTs) of treatments and interventions are typically described as either explanatory or pragmatic. Meta-analysis of RCT studies typically pools evidence of treatment effects from included studies, regardless of their classification as ‘pragmatic’ or ‘explanatory’ trials. Given that treatment effects in explanatory trials may be greater than those obtained in pragmatic trials, conventional meta-analytic approaches may not accurately account for the heterogeneity among the studies. Stratified meta-analysis of systematically review studies in which treatment effects from explanatory trials are meta-analyzed and reported separately from pragmatic trials is generally used in systematic reviews. In this study we investigate a variety of meta-analytic approaches for synthesizing evidence from pragmatic and explanatory trials. Discussions about the key statistical and design considerations when pooling evidence from both types of trial designs are provided. ","Tolulope T. Sajobi University of Calgary, Oluwagbohunmi Awosoga University of Lethbridge, Bijoy Menon University of Calgary, Michael Hill University of Calgary, Lehana Thabane McMaster University",Tuesday 14:30-15:00,N
A Bayesian Approach to Diagnosis of the Gaussian Random Field Assumption,"The Latent Gaussian Model (LDM) is used in various fields such as ecology, the study of cancer, and the stock market. The model is usually used on high dimensional data, with the Integrated Laplace Approximation (INLA) an advanced approach to inference in the LDM. Compare to the traditional approach of Monte Carlo Markov Chain, INLA has much better performance in terms of running speed and error rate. However, one main assumption of using INLA is that the data come from a Gaussian Markov random fields (GMRF). In this paper, we present a method to diagnose the normality assumption of LDM in high dimensions. This method uses Ordinary Differential Equations (ODE) with a Bayesian approach. ","Haoxuan Zhou Simon Fraser University, Dave Campbell Simon Fraser University",Tuesday 13:30-13:45,N
A Bayesian Approach for Pattern Detection in Extremely Small Sample Sizes: Application to Proteomic Cell Line Data of Human Leukemia,"Human cancer cell line experiments are a valuable tool for investigating drug sensitivity biomarkers. The number of biomarkers measured in these experiments is typically on the order of several thousand; whereas the number of samples is often limited to one or at most three replicates for each experimental condition. We have developed an innovative Bayesian approach that efficiently identifies clusters of markers with similar patterns. We define prior distributions on cluster parameters that allow us to obtain biologically meaningful clusters and to better discriminate them. Motivated by the availability of ion mobility mass spectrometry data on cell line experiments in myelodysplastic syndromes and acute myeloid leukemia from the ``Moon Shots"" Program at MD Anderson Cancer Center, our methodology can identify protein markers that follow biologically meaningful trends. ","Thierry Chekouo Tekougang University of Minnesota Duluth, Francesco Stingo University of Florence, Kim-Anh Do The University of Texas MD Anderson Cancer Center",Tuesday 13:45-14:00,N
Bayesian Sensitivity Analysis for Non-ignorable Missing Data in Longitudinal Studies,"The use of Bayesian statistical methods to handle missing data in biomedical studies has become popular in recent years. In this paper, we propose a novel Bayesian sensitivity analysis (BSA) model that accounts for the influences of missing outcome data on the estimation of treatment effects in randomized trials with non-ignorable missing data. We implement the method using the probabilistic programming language STAN, and apply it to data from the Vancouver At Home (VAH) Study, which is a randomized control trial that provided housing to homeless people with mental illness. We compare the results of BSA to those from an existing Bayesian longitudinal model that ignores missingness in the outcome. Furthermore, we demonstrate in a simulation study that BSA credible intervals have greater length and higher coverage rate of the target parameters than existing methods.","Tian Li Simon Fraser University, Lawrence C. McCandless Simon Fraser University, X. Joan Hu Simon Fraser University, Julian M. Somers Simon Fraser University",Tuesday 14:00-14:15,N
Individual Level Modelling of Infectious Disease Data: EpiILM,"The statistical modelling of infectious disease spread through a population generally requires the use of non-standard statistical models. This is primarily due to the fact that infection events depend upon the infection status of other members of the population and hence we cannot assume independence of infection events. Further complication is added by the fact that there are often complex heterogeneities in the population which we wish to account for, since, for example, populations do not tend to mix homogeneously. Sometimes, we may wish to account for such heterogeneities using spatial mechanisms that assume that transmission events are more likely to occur between individuals close together in space than individuals further apart. Sometimes, it is more natural to model such heterogeneities using contact networks that represent, for example, the sharing of supplier companies between farms. Typically, statistical inference for these models (e.g., parameter estimation) is done in a Bayesian context using computational techniques such as Markov chain Monte Carlo (MCMC). Here we examine the main characteristics of individual level infectious disease models, and how to fit them to data within a Bayesian statistical framework using the R package ""EpiILM”. ","Vineetha K.V. Warriyar University of Calgary, Rob Deardon University of Calgary",Tuesday 14:15-14:30,N
 Bayesian Optimality for Beran's Class of Tests of Uniformity around the Circle,We show that the locally most powerful tests of uniformity on the circle given by Beran have optimality properties not just against the parametric alternatives described by Beran but also against many non-parametric alternatives. We describe both local and asymptotic versions of this optimality.,"Shaun Zheng Sun University of the Fraser Valley, Richard Lockhart Simon Fraser University",Tuesday 14:30-14:45,N
Development of Quality Assurance Program for Jaundice Meter Management,"Jaundice meters are intended for use as a screening device in newborns. Meters need to be validated before putting them into clinical use. Transcutaneous bilirubin readings are compared to the clinical laboratory standard, serum bilirubin obtained from blood samples. Meter reliability is evaluated by a regression model where errors are in both variables since an error is not only associated with the jaundice meter method but also the clinical standard. A Bayesian approach is discussed with independent and noninformative priors. Since reliability varies from meter to meter, a number of clustering techniques are examined to develop a statistical quality assurance program for the management of multiple meters using reliability index. A simulation study is carried out to see robustness of the developed program under various conditions, including unbalanced samples size over meters, over-ranged and outlying observations, error ratio, and other factors that cannot be controlled in practice.","Chel Hee Lee University of Calgary, Martha E. Lyon University of Saskatchewan, Krista Baerg University of Saskatchewan",Tuesday 14:45-15:00,N
A New Weighted Quantile Regression,"Quantile regression has wide applications in many fields. Studies of heavy tailed distributions have rapidly developed. For multivariate heavy tailed distributions, estimation of conditional quantiles at very high or low tails is of interest in numerous applications. This presentation proposes a new weighted quantile regression method on high quantile regression. The Monte Carlo simulations show good efficiency of the proposed weighted estimator relative to regular quantile regression estimator. The presentation also investigates a real-world example by using the proposed weighted method. Comparisons of the proposed method and existing methods are given.","Jenny Tieu Brock University, Ramona Rat Brock University, Mei Ling Huang Brock University",Tuesday 13:30-13:45,N
Generalized Bent-Cable Regression for Changepoint Data,"The choice of the model framework in a regression setting depends on the nature of the data. The focus of this study is on changepoint data, exhibiting three phases: incoming and outgoing, both of which are linear, joined by a curved transition. Bent-cable regression is an appealing statistical tool to characterize such trajectories, quantifying the nature of the transition between the two linear phases by modeling the transition as a quadratic phase with unknown width. We demonstrate that a quadratic function may not be appropriate to adequately describe many changepoint data. We then propose a generalization of the bent-cable model by relaxing the assumption of the quadratic bend. The properties of the generalized model are discussed and Bayesian approach for inference is proposed. This study suggests that the proposed generalization can produce a fit, which is either comparable or superior to that of the quadratic bent-cable model.",Shahedul A. Khan University of Saskatchewan,Tuesday 13:45-14:00,N
Combatting Overfitting in Finite Mixture Models,"The standard expectation-maximization approach to parameter estimation for Gaussian mixture models is modified by a within-cycle incorporation of the nonparametric bootstrap in order to prevent overfitting in model-based clustering applications. Benefits include more reasonable parameter estimates (including estimates of standard error), more ‘realistic’ clustering results, and increased avoidance of local maxima. In addition, we find better efficiency versus a traditional nonparametric bootstrap approach to the EM for mixture models. Proposals for issues that arise, such as lack of monotonicity and convergence, are discussed and the method is applied to real and simulated data.",Jeffrey L. Andrews University of British Columbia,Tuesday 14:00-14:15,N
Quantile Regression with Nominated Samples,"In this paper, we study quantile regression with nominated samples. Nominated samples have a wide range of applications in medical, ecological and environmental research, and have been shown to perform better than SRS in estimating several population parameters. We propose a new objective function which takes into account the ranking information to estimate the unknown model parameters based on the maxima or minima nomination sampling designs. We compare the mean squared error of the proposed quantile regression estimates using maxima (or minima) nomination sampling design and observe that it provides higher relative efficiency when compared with its counterparts under SRS design for analyzing the upper (or lower) tails of the distribution of the response variable. We also evaluate the performance of our proposed methods when ranking is done with error.","Olawale Fatai Ayilara University of Manitoba, Mohammad Jafari Jozani University of Manitoba",Tuesday 14:15-14:30,N
Estimation Strategies for Weighted Least Absolute Deviations Regression Models,"We consider the estimation problem of the weighted least absolute deviation (WLAD) regression parameter vector when there are some outliers in the response and the leverage points in the predictors. We propose the pretest and shrinkage WLAD estimators when some of the parameters may be subject to certain restrictions. We derive the asymptotic risk of the pretest and shrinkage WLAD estimators and show that the asymptotic risk of the shrinkage estimator is strictly less than the unrestricted WLAD estimator. On the other hand, the risk of the pretest WLAD estimator depends on the validity of restrictions. Furthermore, we study the WLAD absolute shrinkage and selection operator and compare its relative performance with the pretest and shrinkage WLAD estimators. A simulation study is conducted to evaluate the performance of the proposed estimators relative to the unrestricted WLAD estimator. A real life data example is used to illustrate the performance of the proposed estimators. ","Md. Shakhawat Hossain University of Winnipeg, Mina Norouzirad Shahrood University of Technology, Mohammad Arashi Shahrood University of Technology",Tuesday 14:30-14:45,N
Estimation and Identification of Varying-Coefficient Additive Models for Locally Stationary Process,"Nonparametric regression models allowing for locally stationary covariates have received increasing interest. To catch the dynamic nature of regression function, we adopt more flexible structural nonparametric model than classical varying-coefficient model (VC) and additive model (AM), named varying-coefficient additive model (VCAM). For this model, we propose a three-step spline estimation method for varying-coefficient and additive component functions and show their consistency and rate of convergence. Furthermore, we develop a two-stage penalty procedure to identify varying-coefficient and additive terms in the VCAM. We demonstrate that the proposed identification procedure is consistent, i.e., with probability approaching to one, we correctly select varying-coefficient and additive terms, respectively. Simulation studies explore the finite sample performance and validate the asymptotic theories. Two real data applications illustrating the methodologies are presented as well.",Jinhong You,Tuesday 14:45-15:00,N
Predicting the Unpredictable: Severe Wildfire Risk Prediction in Canada,"About 8000 wildfires occur in the protected area of Canada each year. Approximately 2\% of these fires exceed 150+ ha in size, but account for most of the suppression costs and are the greatest threat to our communities. Although statistical approaches to fire occurrence prediction (FOP) have evolved over the past 40 years, FOP has not been implemented at a national scale in Canada. We develop a big data (supervised machine learning) based statistical modeling approach to predict severe large wildfire occurrences in Canada using a set of spatially gridded meteorological, topographic and demographic covariates. Our modeling framework is focused on allowing for better preparedness and more effective decisions to share resources among provincial/territorial and federal agencies, and to enhance Canada’s resilience to large wildfires. The talk will focus on large wildfire prediction in British Columbia, Canada, to showcase our methodology.","Khurram Nadeem Natural Resources Canada, Charmaine B. Dean Western University, Douglas Woolford Western University, Steve Taylor Pacific Forestry Centre, Natural Resources Canada",Tuesday 13:30-13:45,N
Piecewise Linear Quantile Regression for Estimation of Ecological Breakpoints,"The relationships between ecological variables are usually estimated by fitting statistical models which go through the conditional means of the variables. For example, the piecewise linear regression model - which goes through the conditional mean of the response variable given the predictor(s) - is used to obtain relationships and breakpoints in variables to answer relevant ecological questions. In contrast, the piecewise linear quantile regression model - which goes through the quantiles of the conditional distribution of the response variable given the predictor(s) - provides much richer information in terms of estimating relationships, breakpoints and confidence bands. The methods are illustrated with two examples from the ecological literature – relating an index of wetlands’ fish community ‘health’ to the amount of human activity in wetlands’ adjacent watersheds; and relating the biomass of Cyanobacteria to the Total Phosphorus concentration in Canadian lakes.","Jabed H. Tomal University of Toronto Scarborough, Jan J.H. Ciborowski University of Windsor, Karen Fung University of Windsor",Tuesday 13:45-14:00,N
Geostatistical Inference under Preferential Sampling,"Preferential sampling in geostatistics occurs when the process that determines the sampling locations may depend on the spatial process that is being modeled. In this case inference cannot generally be performed conditionally on the locations. Here we revisit the seminal work of Diggle et al. (2010, JRSS-C, 59, 191-232) and show that, under certain conditions satisfied by most commonly used geostatistical models, previously proposed Monte Carlo estimates for the likelihood function may not behave as expected. We also show that alternative numerical methods to approximate the likelihood function yield noticeably better results than Monte Carlo-based methods. Although we illustrate these results with the relatively simple models in Diggle et al. (2010), the advantage of our approach is particularly important for more complex preferential sampling models.","Matias Salibian-Barrera University of British Columbia, Daniel Dinsdale University of British Columbia",Tuesday 14:00-14:15,N
Parameter Estimation for PDEs Defined over Irregular Domains,"Spatial temporal data are abundant in many scientific fields, some examples include; temperature readings from multiple weather stations and the spread of an infectious disease over a particular region taken over time. In many instances the spatial data are accompanied by mathematical models expressed in terms of partial differential equations (PDEs). A PDE determines the theoretical aspects of the behaviour of the physical, chemical or biological phenomena considered. The parameters of the PDE are typically unknown and must be inferred from expert knowledge of the phenomena considered. I will introduce a methodology for attaining data driven estimates of the PDE parameters by extending the profiling with parameter cascading procedure outlined in Ramsay et al. (2007). We also incorporate splines on triangulations Lai and Schumaker (2007) to account for attributes of the geometry of the problem such as irregular shaped domains and internal boundary features.","Michelle Carey McGill University, James O. Ramsay McGill University",Tuesday 14:15-14:30,N
Empirical Eigenvalue Based Testing for Structural Breaks in Linear Factor Models,"Testing for stability in linear factor models has become an important topic in both the statistics and econometrics research communities. We develop a general test for structural stability of linear factor models that is based on monitoring for changes in the largest eigenvalue of the sample covariance matrix. The asymptotic distribution of the proposed test statistic is established under the null hypothesis that the mean and covariance structure of the cross sectional units remain stable during the observation period. We show that the test is consistent assuming common breaks in the mean or factor loadings. These results are investigated by means of a Monte Carlo simulation study, and their usefulness is demonstrated with an application to U.S. treasury yield curve data, in which some interesting features of the 2007-2008 subprime crisis are illuminated.","Gregory Rice University of Waterloo, Lajos Horvath University of Utah",Tuesday 13:30-13:45,N
Modeling the Random Effects Covariance Matrix for Longitudinal Data with Covariate Measurement Error,"Longitudinal data occur frequently in practice such as medical studies and life sciences. Generalized linear mixed models (GLMMs) are commonly used to analyze such data. It is typically assumed that the random effects covariance matrix is constant across the subject (and among subjects) in these models. In many situations, however, this correlation structure may differ among subjects and ignoring this heterogeneity can cause the biased estimate of model parameters. Covariates measured with error also happen frequently in the longitudinal data set-up. Ignoring this issue in the data may also produce bias in model parameters estimate and lead to wrong conclusions. In this work, we propose an approach to properly model the random effects covariance matrix based on covariates in the class of GLMMs where covariates are subject to measurement error. The resulting parameters from the decomposition of random effects covariance matrix have a sensible interpretation and can easily be modeled without the concern of positive definiteness of the resulting estimator. Performance of the proposed approach is evaluated through simulation studies as well as a real longitudinal data application from Manitoba Follow-up study.","Md. Erfanul Hoque University of Manitoba, Mahmoud Torabi University of Manitoba",Tuesday 13:45-14:00,N
Empirical Likelihood Multiscale Inference of the Scaling Property of Time Series Models,"Over the years, financial time series have become available at increasingly higher frequencies. It is well-known that time series sampled at different frequencies carry information relevant to different components in the original time series, e.g. fast- and slow-moving components. We are interested in a research question of how to take advantage of samples at multiple frequencies in order to improve statistical inference in the presence of multiscale phenomenon. Multiscale phenomenon in financial time series have been reported by researches in a wide range of fields, ranging from physicists, applied mathematicians, to econometricians. In this talk, we introduce an empirical likelihood (EL) based test for model scaling property using multiple frequency samples. The method is used to test whether or not a given model can be consistently used across different time scales. For application, we apply it to models for capturing multiscale phenomenon in financial asset’s volatility.","Lichen Chen University of Waterloo, Adam Kolkiewicz University of Waterloo, Tony Wirjanto University of Waterloo",Tuesday 14:00-14:15,N
A Nonparametric Test for Detecting Periodicity in Time Series,"Traditional methods of detecting periodicity in time series depend on the assumption of identical independent Gaussian noises and are sensitive to outliers. We then propose an innovative nonparametric robust periodicity test which is highly resistant to outliers and is distribution free. It also works when the second moment of the error is not finite while traditional methods fail. We derive an asymptotic distribution for the test statistic and show that this method is efficient under the assumption of Gaussian noises. A response surface regression approach is also implemented to approximate the finite distribution of the test statistic. In addition, the implementation of this approach is dramatically sped up by utilizing the parallel computation through the Shared Hierarchical Academic Research Computer Network.","Yuanhao Lai Western University, A. Ian McLeod Western University",Tuesday 14:15-14:30,N
Joint Analysis of Longitudinal Zero-heavy Panel Count Outcomes with Application to Understanding Desistance from Criminal Activity,"Regression models for zero-inflated count data often need to accommodate within-subject correlation and between-individual heterogeneity; frequently random effects models are utilized for incorporating complex correlation structures. In cases where several longitudinal zero-heavy count outcomes are jointly considered, excess zeros may arise from several distinct sources. This is the case for a study of the patterns and mechanisms of the process of desistance from criminal activity. Methodological challenges in the analysis of longitudinal criminal behaviour data include the need to develop methods for multivariate longitudinal discrete data, incorporating modulating exposure variables and several possible sources of zero-inflation. There are additional complications such as some outcomes being prohibited during time in a secure facility; as well as intervention carry-over effects for some outcomes, and that the underlying process generating events may resolve for some individuals.",Charmaine B. Dean University of Western Ontario,Tuesday 15:30-16:00,N
Joint Modeling of Longitudinal Proportional Measurements and Survival Times with a Cure Fraction,"In cancer clinical trials and other medical studies, both longitudinal measurements and data on a time to an event (survival time) are often collected from the same patients. Joint analyses of these data would improve the efficiency of the statistical inferences. We propose a new joint model for the longitudinal proportional measurements which are restricted in a finite interval and survival times with a potential cure fraction. A penalized joint likelihood is derived based on the Laplace approximation and a semiparametric procedure based on this likelihood is developed to estimate the parameters in the joint model. A simulation study is performed to evaluate the statistical properties of the proposed procedures. The proposed model is applied to data from a clinical trial on early breast cancer. ","Dongsheng Tu, Hui Song Dalian University of Technology, Yingwei Peng Queen's University",Tuesday 16:00-16:30,N
Joint Analysis of a Longitudinal Binary Outcome and Recurrent Events with Application to Cystic Fibrosis Outcomes,"In cystic fibrosis, chronic Pseudomonas aeruginosa (Pa) infection is associated with worse clinical outcomes including more frequent pulmonary exacerbations (PE). PE are themselves a leading cause of morbidity in CF and important endpoints in CF clinical trials. This talk discusses joint models that address the challenge of understanding the co-dependence of progression of Pa infection and recurrent PE over time. Using data from the ongoing Early Pseudomonas Infection Control study, we propose a joint model built within the frameworks of hidden Markov chain models to model progression of Pa, and dynamic recurrent event models to model the recurrence of PE. We address additional challenges in the motivating study including missing and misalignment of covariates, the dynamic aspect of the recurrence of PE, and the latent aspect of the different Pa states of infection.","Elizabeth Juarez-Colunga University of Colorado Denver, Brandie Wagner University of Colorado Denver, Edith Zemanick University of Colorado Denver",Tuesday 16:30-17:00,N
"A Simple Tool for Bounding the Deviation of Random Matrices on Geometric Sets, with Applications","Let A be an isotropic, sub-gaussian m by n matrix. We give a simple sufficient condition for a random sub-Gaussian matrix to be well conditioned when restricted to a subset of $R^n$. We also prove a local version of this theorem, which allows for unbounded sets. These theorems have various applications, such as a general theory of compressed sensing. ","Yaniv Plan University of British Columbia, Chris Liaw University of British Columbia, Abbas Mehrabian University of British Columbia, Roman Vershynin University of Michigan",Tuesday 15:30-16:00,N
Approximation of Squared Integrable Random Variables with Nonlinear Stochastic Integrals,"Global quadratic hedging is the problem of finding a trading strategy which minimizes the expected squared error between the value of the portfolio and the value of a contingent claim. Under some conditions, the value of the portfolio is given by a stochastic integral and the value of the contingent claim by a random variable. In this context, the problem of global quadratic hedging can be seen as the projection of an $L^2$ random variable in the linear space of stochastic integrals respect to a semimartingale. More recently, to incorporate the effect of illiquidity, stock prices are given by semimartingales with space parameters. In this case, the value of the portfolio is given by a stochastic integral which is no more linear with respect to the integrand. During this talk, we will look at some conditions which allow to find an $L^2$ approximation of random variables with nonlinear stochastic integrals. ",Clarence Simard Université du Québec à Montréal,Tuesday 16:00-16:30,N
Probabilistic Symmetries and Random Locations,"In this talk we briefly discuss different types of symmetries in probability, including stationarity, stationarity of the increments, isotropy, self-similarity, exchangeability, and their combinations. Each of these symmetries is naturally related to an operator in the path space, in the sense that the symmetry can be expressed as the invariance of the distribution of the stochastic processes with respect to the corresponding operator. In particular, we consider the random locations of the stochastic processes, such as the hitting times or the location of the path supremum over a fixed interval. On one hand, we see how the probabilistic symmetries can imply the properties of the distributions of these random locations; on the other hand, we also discuss how the distributions of the random locations can be used to characterize the probabilistic symmetries.",Yi Shen University of Waterloo,Tuesday 16:30-17:00,N
Statistical Methodology on Human Microbiome Data Analysis,"The technological development in genomic sequencing has enabled researchers to unveil the wide variability of bacteria presented within different locations of the body. It is necessary to better understand both environmental and host genetic factors impact the composition of the microbiome to improve disease management. Several analytic approaches are introduced to summarize and assess the single or multiple OTUs using different computational algorithms. Motivated by the multivariate nature of microbiome data with hierarchical taxonomic clusters, counts that are often skewed and zero inflated, we propose a Bayesian latent variable methodology to jointly model multiple operational taxonomic units within a single taxonomic cluster. This novel method can incorporate both negative binomial and zero-inflated negative binomial responses, and can account for serial and familial correlations. This method can help discover both genetic and environmental factors that influence the microbiome. ",Wei Xu,Tuesday 15:30-16:00,N
Aspects of Genetic Meta-Analysis under Data Uncertainty,"Genotype imputation is a technique extensively used in genome-wide association studies (GWAS), as well as in their meta-analysis. In this talk, we outline strategies to account for imputation accuracy when including imputation-based GWAS results in a meta-analysis, considering both fixed-effect and random-effects models. While adding studies to the meta-analysis typically boosts the power to detect genetic associations, inclusion of imputation-based studies may adversely affect power due to increased genotype uncertainty. We address this trade-off via a reweighing scheme that controls the contribution of imputation-based studies in meta-analyses. The proposed method achieves a better detection power relative to traditional approaches, and improves the validity and reliability of imputation-based meta-analysis results.",Elif Fidan Acar University of Manitoba,Tuesday 16:00-16:30,N
Correcting False Discovery Rates for their Bias Toward False Positives,"Conventional methods of adjusting p values for multiple comparisons control a family-wise error rate (FWER) such as a genome-wise error rate. The recognition that they lead to excessive false negative rates in genomics applications has led to widespread use of false discovery rates (FDRs) in place of the conventional adjustments. While this is an improvement, the way FDRs are used in the analysis of genomics data leads to the opposite problem, excessive false positive rates. In this sense, the FDR overcorrects for the excessive conservatism (bias toward false negatives) of the FWER-controlling methods of adjusting p values. Estimators of the local FDR (LFDR) are much less biased but have not been widely adopted due to their high variance and lack of availability in software. To address both issues, we propose estimating the LFDR by correcting an estimated FDR or the level at which an FDR is controlled. Preprint: http://hdl.handle.net/10393/34277 ",David R. Bickel University of Ottawa,Tuesday 16:30-17:00,N
Geographic Topic Modelling to Predict Regional Beer Success,"This study explores online beer reviews to identify which types and flavours of beer are popular among consumers in different geographic regions. The exploration of beer reviews was carried out with Latent Dirichlet Allocation, a Natural Language Processing approach to cluster text from beer reviews into flavour and sentiment topics. Combining text and topic data from reviews with numerical ratings of beers in categories of flavour, scent, and colour we can determine flavour preferences. When matching breweries with their geographical locations, we explore beer flavour and type preference trends across the country. This analysis could be used as a recommendation system for predicting regional beer success from flavour and beer type combinations.",Pulindu Ratnasekera Simon Fraser University,Tuesday 15:30-16:00,N
How to Brew your Beer Tour with Data Science,"Motivated by the emerging craft brewery scene in Vancouver B.C., we set our goal to design a beer tour to visit some of the amazing hotspots in town. Using Hadley Wickham’s tidyverse package, we automate data extraction to pinpoint exact locations of breweries, and proceed to extract distances and travelling times between them from GoogleMaps API. A travelling salesman algorithm is then applied to optimize for the best tour path, which can be tailored for any individual’s preferences. To improve the usability of our tour planning, we extend our workflow into an interactive and customizable web application built with Shiny. Our app shows the location of all the craft breweries in Vancouver and their respective menus, with average rating scores of all menu options. Users can simply point and click to express their preferences and obtain a personalized path. A utility chart derived from user inputs that displays the marginal utility gained from each stop is also provided. ","Joanna Zhao Simon Fraser University, Derek Qiu Simon Fraser University",Tuesday 16:00-16:30,N
Gosset at Guinness: How ‘Student’ Derived (and Checked) the Distribution of his z-statistic,"William Gosset (“Student”) published “The Probable Error of a Mean” in 1908. Despite its seminal nature, modern-day statistics textbooks give him, and this article, short shrift. Few of today’s students – or their teachers – are aware of the “z” statistic whose sampling distribution he actually derived, the mathematical derivation, his simulations to check his work, the material used in the simulations, the table he produced, the “one-line” missing proof supplied by the 22-year old Fisher (still a student himself) or the subsequent switch, in collaboration with Fisher, from the z to the t-statistic. I remind readers of these. I hope the next generation of statisticians come to know more about the man than simply that “he worked for the Guinness brewery,” and appreciate that not all statistical distributions are derived in a single pass. Students would do well to use his paper as a model when writing their first statistical article. Extra material: www.epi.mcgill.ca/hanley/Student/",James A. Hanley McGill University,Tuesday 16:30-17:00,N
Innovations in Sampling for Insecure and Data-Poor Environments,"The standard challenges faced in survey design – completeness of sample frame, sample selection, interviewer quality, etc. – can manifest themselves in unique ways in the developing world. Sample frames are incomplete or non-existent. Interviewer education and capacity are limited. Field conditions are often challenging, and the possibility (or probability) of unforeseen events must be built into survey designs. The presentation covers a set of illustrative recent case studies of the challenges faced in sub-Saharan Africa. Topics include: using satellite imagery as a sampling frame in Kinshasa; a proposed alternative to random walk for second stage selection in conflict areas of Mogadishu; the application of a random geographic cluster sample methodology to a survey of pastoralist and nomadic populations in eastern Ethiopia; and the implementation of a high frequency cell phone survey to measure the socioeconomic impacts of Ebola in Sierra Leone and Liberia. Focus will be on technical and implementation challenges faced, how they were addressed while maintaining a generalizable probability design, and lessons learned for future applications.",Kristen Himelein World Bank,Tuesday 15:30-16:00,N
Measuring Cash and Payment Card Usage at the Point-of-Sale for Consumers and Merchants,"Measuring cash holdings and usage is particularly difficult due to the anonymous nature of cash. The Bank of Canada has undertaken some surveys of consumer and merchants to address this issue. However, there are several challenges involved such as recruitment of respondents and low response rates. We discuss some methodology employed to address these concerns.",Kim P. Huynh Bank of Canada,Tuesday 16:00-16:30,N
How the Design of the New Zealand Health Survey Contributes to Informed Decision Making in Public Health Policy,"The New Zealand Health Survey monitors the health of the nation’s adult and child populations. It produces a consistent core set of indicators, and must also reflect emerging policy priorities, and provide regular information on national, demographic, ethnic and regional subgroups. To achieve these goals, the survey is continuously in the field, but reports annually. Its continuous nature allows a set of core questions to be combined with a flexible programme of modules. Data are pooled over different periods to balance timeliness and precision. The sample design oversamples key subpopulations using a novel method which reflects the strengths and weaknesses of the data available for design. Using these mechanisms, the survey supports policy directions including the Smokefree 2025 goal, the Childhood Obesity Plan, the Rheumatic Fever Prevention Programme, and the continuous improvement of the responsiveness and equity of health services.","Robert Clark University of Wollogong, New Zealand, Steven Johnston New Zealand Ministry of Health",Tuesday 16:30-17:00,N
Foldover Designs with Column Permutations,Foldover is a follow-up technique used in the design of experiments. Traditional foldover designs are obtained by changing the signs of some columns of an initial design. We further consider performing a column permutation. We investigate when a column permutation results in a combined foldover design that has better G-aberration than the corresponding traditional combined foldover design. Properties of such foldover designs are studied.,"Po Yang Unviersity of Manitoba, William Li University of Minnesota",Tuesday 15:30-15:45,N
A Doubly Robust Estimator for Indirectly Standardized Mortality Ratios,"Routinely collected administrative and clinical data are increasingly being utilized for comparing quality of care outcomes between hospitals. This problem can be considered in a causal inference framework, as such comparisons have to be adjusted for hospital-specific patient case-mix, which can be done using either an outcome or an assignment model, but is subject to misspecification. It is often of interest to compare the performance of the hospitals against the average level of care in the health care system, using indirectly standardized mortality ratios. A doubly robust estimator makes use of both outcome and assignment model in the case-mix adjustment, requiring only one of these to be correctly specified for valid inferences. We present the causal estimand in indirect standardization in terms of potential outcome variables, propose a doubly robust estimator, and study its properties. ","Katherine Daignault University of Toronto, Olli Saarela University of Toronto, Dalla Lana School of Public Health",Tuesday 15:45-16:00,N
Should a Propensity Score be Super? A Comparison of Estimation Methods,"The propensity score (PS) is a tool to eliminate imbalance in the distribution of confounding variables between treatment groups. Previous work (Pirracchio et al. 2015) suggested that Super Learner (SL), an ensemble method, outperforms logistic regression (LR) in non-linear settings. We investigated wider range of settings of varying complexities to compare the performances of logistic regression, generalized boosted models (GBM) and SL. We estimated the average treatment effect using PS regression, PS matching and inverse probability of treatment weighting (IPTW). We found that SL and LR are comparable in terms of bias, covariate balance and mean squared error, and both outperform GBM, however SL is computationally very expensive. We also found that PS regression is superior to either IPTW or matching. We used two real data examples to demonstrate the performances of the estimators to support our findings, and find LR provides the best balance in two empirical analyses.","Shomoita Alam McGill University, Erica E.M. Moodie McGill University, David A. Stephens McGill University",Tuesday 16:00-16:15,N
Estimation in Model Selection after Imputation,"Missing data is often handled using multiple imputation and the parameters are estimated using Rubin's Rules. If model selection is performed after multiple imputation, we propose bootstrap imputation followed by model selection, then model averaging over the bootstrap distribution. The estimated variance is efficiently computed from the same bootstrap sample as in Efron (2014) and confidence intervals are computed using the normal distribution. We call this bootstrap imputation with Efron’s Rules. We compare Efron’s Rules estimator to the Rubin’s Rules estimator. We also compare a single imputation followed by model selection (the Impute-Select estimator) with bootstrap percentile confidence intervals. Simulation studies show that the model averaged estimators perform better than the Impute-Select. The confidence intervals based on Rubin’s Rules can have severe undercoverage. The Efron’s Rules estimator improves the performance of normal theory confidence intervals. ","Lin Liu University of California, San Diego, Loki Natarajan University of California, San Diego, Karen Messer University of California, San Diego",Tuesday 16:15-16:30,N
 Robust Discrimination Designs over Hellinger Neighbourhoods,"To aid in the discrimination between two, possibly nonlinear, regression models, we study the construction of experimental designs. Considering that each of these two models might be only approximately specified, robust ``maximin'' designs are proposed. The rough idea is as follows. We impose neighbourhood structures on each regression response, to describe the uncertainty in the specifications of the true underlying models. We determine the least favourable -- in terms of Kullback-Leibler divergence -- members of these neighbourhoods. Optimal designs are those maximizing this minimum divergence. Sequential, adaptive approaches to this maximization are studied. Asymptotic optimality is established.","Rui Hu MacEwan University, Doug Wiens University of Alberta",Tuesday 16:30-16:45,N
Optimal Designs for Estimating Parameters of the Bradley-Terry Model for Paired Comparisons,"The Bradley-Terry model for paired comparisons has been broadly applied in many areas such as statistics, sports and machine learning. In this work we determine the maximum likelihood estimates of the parameters of the latent variable models such as Bradley-Terry model by using the theory of optimal design. We consider the parameters of the Bradley-Terry model in terms of a set of another parameters which we consider as weights in optimal design. These weights are positive and sum to one. In order to solve this problem, we first consider the likelihood function as our criterion function, and then we determine the optimality conditions in terms of point to point directional derivatives of the likelihood function. We then determine the maximum likelihood estimates using a class of algorithms, indexed by a function which depends on the derivatives of the likelihood function. Finally, we apply this problem to a data set from American League Baseball Teams.","Saumen Mandal University of Manitoba, Monsur Chowdhury University of Winnipeg",Tuesday 16:45-17:00,N
An Overview of Existing and a Novel Approaches to Multi-label Classification,Multi-label classification is a supervised learning problem where an observation may be associated with multiple binary (outcome) labels simultaneously. We give an overview over common approaches to multi-label classification and also introduce a new approach as an extension of the nearest neighbor principle. Experiments on benchmark multi-label data sets show that the proposed method on average outperforms other commonly used approaches in terms of classification performance.,"Hyukjun (Jay) Gweon Waterloo University, Matthias Schonlau University of Waterloo, Stefan Steiner University of Waterloo",Tuesday 15:30-15:45,N
Estimation and Classification for Finite Mixture Models with Order Statistics,"We study the problems of maximum likelihood estimation and classification based on different collections of order statistics from finite mixture models. We consider these problems under both labeled and unlabeled collections of order statistics. New missing mechanisms and expectation-maximization algorithms are developed to exploit the structure of observed data in estimation procedures. Various model-based classification criteria are then developed for observed and unobserved data from the underlying FMM. Through simulation studies, we evaluate the performance of estimation and classification methodologies. Finally, the proposed methods are used for a real data analysis.","Armin Hatefi University of Toronto, Nancy Reid University of Toronto, Mohammad Jafari Jozani University of Manitoba, Omer Ozturk Ohio State University",Tuesday 15:45-16:00,N
Two Innovative Applications of Classification and Regression Trees,"Classification and regression trees (CART) are machine learning methods for constructing prediction models by recursively partitioning the data space and fitting a simple model within each partition. Two innovative applications of CART were proposed: discretization and interaction detection in fitting regression models. Discretization is to convert a continuous variable to a categorical which is necessary for statistical methods such as analysis of variance (ANOVA), chi-square tests, fitting a multinomial model, and etc. Simulation studies and medical applications are used to illustrate the effectiveness of classification and regression trees in transferring continuous data to categorical and extracting the interactions structure between explanatory variables. ",Wanhua Su MacEwan University,Tuesday 16:00-16:15,N
A deep learning-based integrative analysis framework for molecular classification of breast cancer,"Classification of molecular subtypes of breast cancer using deep learning poses an exceptional challenge as we have to train a deep neural network (DNN) with hundreds of thousands of parameters using a small dataset with thousands of samples and tens of thousands of features. It is expected that integration of knowledge from multiple data sources measured on the same individuals can overcome the challenge. In this paper, we proposed a number of DNN models to integrate different omics data sets collected from the same set of breast cancer patients for predicting their molecular subtypes. We compared the results of our DNNs with traditional machine learning models such as Support Vector Machine (SVM) and Random Forest (RF). We demonstrated that our DNN models are superior to SVM and RF. ","Pingzhao Hu University of Manitoba, Md. Mohaiminul Islam University of Manitoba, Yang Wang University of Manitoba",Tuesday 16:15-16:30,N
Minimum Profile Hellinger Distance Estimation for Two-Class Location Models,"Minimum Hellinger Distance estimator is obtained by minimizing the Hellinger distance between an assumed parametric model and a nonparametric estimator of a model density. The method receives increasing attention over the past decades due to their asymptotic efficiency and excellent robustness against small deviation from the model. In this work, we propose to use a Minimum Profile Hellinger Distance estimation (MPHD) for the parameters in two-class symmetric location models. Asymptotic normality and the property of robustness of the estimator are discussed and a comparison with MLE is carried out through Monte Carlo simulation studies. This estimation is applied to a breast cancer dataset. ","Jian Yang University of Calgary, Jingjing Wu University of Calgary, Haocheng Li Tom Baker Cancer Centre and University of Calgary, Xiaolan Feng Tom Baker Cancer Centre and University of Calgary",Tuesday 16:30-16:45,N
Comparison of Multivariate Classifiers in the Presence of Covariates in Repeated Measures Designs,"Classification models based on multivariate outcomes are commonly used for discriminating between groups in repeated measures (RM) designs. However the majority of these models rely on the assumption of multivariate normality and rarely incorporate covariate information, which are routinely collected in RM designs. Using Monte Carlo simulation methods, we investigate the impact of time-varying and time-invariant covariates on the misclassification error rate of classical linear discriminant analysis (LDA), LDA based on linear mixed effect regression, a Bayes classifier, logistic regression, and quadratic inference function classifiers. We provide recommendations about using these classifiers in RM designs with multiple covariates.","Anita Brobbey University of Calgary, Tyler Williamson University of Calgary, Samuel Wiebe University of Calgary, Lisa M. Lix University of Manitoba, Tolulope T. Sajobi University of Calgary",Tuesday 16:45-17:00,N
Deflation Risk and Implications for Life Insurers,"Life insurers are exposed to deflation risk: falling prices could lead to insufficient investment returns, and inflation-indexed protections could make insurers vulnerable to deflation. In this spirit, this study proposes a market-based methodology for measuring deflation risk based on a discrete framework: the latter accounts for the real interest rate, the inflation index level, its conditional variance, and the expected inflation rate. US inflation data are then used to estimate the model and show the importance of deflation risk. Specifically, the distribution of a fictitious life insurer’s future payments is investigated. We find that the proposed inflation model yields higher risk measures than the ones obtained using competing models, stressing the need for dynamic and market-consistent inflation modelling in the life insurance industry.",Jean-François Bégin Simon Fraser University,Tuesday 15:30-15:45,N
Outlier Robust Estimation of the Total Private Cost of Payment Methods for Large Businesses,"The Bank of Canada conducted the 2015 Retailer Survey on the Cost of Payment Methods for two reasons: to estimate the total private and social costs to Canadian retailers of cash and cards and to analyse the efficiency of payment methods. We focus specifically on the private costs to large businesses. The presence of outliers in the sample suggests the need for a robust estimation method. Three robust versions of Royall's (1970) best linear unbiased predictor (BLUP) are computed; these estimators are the naive, the conditional bias (Beaumont, Haziza and Ruiz-Gazen 2013 [BHR]) and the Chambers (1986). BHR propose an adaptive method of choosing the tuning constant. We derive the tuning constant for the Chambers (1986) estimator by minimising a parametric bootstrap estimator of the mean squared error (MSE). The results of the total private costs to retailers for cash and cards are compared; the robust Chambers estimator and the conditional bias estimator are also compared. ",Valéry Dongmo Jiongo Bank of Canada,Tuesday 15:45-16:00,N
Optimal Investment Strategies and Inter-Generational Risk-Sharing for Target Benefit Pension Plans,"We consider a stochastic model for a target benefit pension fund in continuous time, where the plan member's contributions are set in advance while the pension payments depend on the financial situation of the plan, implying risk-sharing between different generations. The pension fund is invested in both a risk-free asset and a risky asset. In particular, stochastic salary and the correlation between the salary movement and the financial market fluctuation are considered. Using the stochastic optimal control approach, we derive closed-form solutions for optimal investment strategies as well as optimal benefit payment adjustments, which minimize the combination of benefit risk (deviating from the target benefit) and inter-generational transfers. Numerical analysis is presented to illustrate the sensitivity of the optimal strategies to parameters of the financial market and salary rates. We also consider how the optimal benefit changes with respect to different target levels.","Suxin Wang Simon Fraser University, Yi Lu Simon Fraser University, Barbara Sanders Simon Fraser University",Tuesday 16:00-16:15,N
Constrained Mean-Variance Portfolio in a Market with Stochastic Volatility,"In this talk, we will study a specific stochastic control problem known as the mean–variance investment problem where the portfolio strategy is subject to a no short-selling constraint. More specifically, under a Black-Scholes type financial market modelization where the stock's volatility follows a Heston diffusion process, we will construct an optimal portfolio through carefully chosen adjoint processes or backward stochastic differential equations. ","Francois Watier Université du Québec à Montréal, Yassin El Kasmi Université du Québec à Montréal",Tuesday 16:15-16:30,N
Portfolio of Exchangeable Risks: Aggregation with Partial Information on Dependence,"In this paper, we consider the computation of risk measures, such as the VaR and the TVaR, for a portfolio of exchangeable risks assuming that the marginal distributions are known but that the dependence structure is partially known. In our approach, we compute lower and upper bounds on risk measures, such as the VaR and the TVaR, for the aggregate risk of the portfolio. Our approach is based on some known moments and on stochastic orders. Numerical examples are provided to illustrate our proposed approach.","Etienne Marceau Université de Laval, Hélène Cossette Université Laval",Tuesday 16:30-16:45,N
On the Moments and the Distribution of Aggregate Discounted Claims in a Markovian Environment,"This paper studies the moments and the distribution of the aggregate discounted claims in a Markovian environment, where the claim arrivals, claim amounts and forces of interest (for discounting) are influenced by an underlying Markov process representing external environment changes. Specifically, we assume that claims occur according to a Markovian arrival process (MAP). A recursive formula is derived for the moments of the aggregate discounted claims occurred in certain states. We also study two types of covariances, the covariance of the aggregate discounted claims occurred in any two subsets of the state space and the covariance of aggregate discounted claims occurred at two different times. The distribution of the aggregate discounted claims occurred in certain states by any specific time is also investigated. For a two-state Markov-modulated model, numerical results are presented. ","Yi Lu Simon Fraser University, Shuanming Li University of Melbourne",Tuesday 16:45-17:00,N
The Power of Simple Statistical Techniques in the Era of Big and Complex Data: Some Recent Examples from Genetic Association Studies,"In many scientific studies, different statistical tests are proposed with competing claims about method performance. Discussions of optimality rely on assumptions, and an optimal test is often impossible in multi-dimensional parameter space. For example, in studying association between multiple genetic variants and a complex human trait, we show that most existing methods belong to linear or quadratic class of tests, each being powerful only in sub-spaces. In another, location- or scale-test is preferred depending on the presence of main or interaction effect. In both cases, we show that the two classes of tests are asymptotically independent of each other under the global null hypothesis. Thus, we can use Fisher’s method to derive a new class of robust tests and obtain asymptotic distribution; this is desirable when analyzing big data. The talk is based on work from Derkach, Lawless, Sun (2014, Statistical Science), Soave et al. (2015, AJHG), and Soave and Sun (2017, Biometrics).",Lei Sun University of Toronto,Wednesday 08:40-09:45,N
‘Bio-logic’ Inputs to Statistical Analysis,"I will share some career highlights, and a few embarrassments. I will tell how the ROC papers arose, and ask why they disseminated so widely. I will describe issues arising from extra-mural consultations, including: a ‘rule of thumb’ for logistic regression that requires 10 outcome events/parameter; our work to revise the decision limits used by the World Anti-Doping Agency; a Court of Arbitration for Sport case involving a cyclist who had tested +ve; and communicating with non-statisticians. I will end with my 12-year mission to help cancer screeners replace the proportional hazards (PH) model (the prevailing way to measure mortality reductions produced by cancer screening). Before reading a paradigm-changing 2002 paper, I had never asked what characteristics of the disease process being targeted, and of the countermeasures, are prerequisites for the PH model. For answers, we should look not to data, or formal tests of (non)proportionality, but to logic, i.e. to bio-logic reasoning. ",James A. Hanley McGill University,Wednesday 08:40-09:45,Y
Optimal Investment Strategies for Participating Contracts,"Participating contracts are popular insurance policies, in which the payoff to a policyholder is linked to the performance of a portfolio managed by the insurer. We consider the portfolio selection problem of an insurer that offers participating contracts and has an S-shaped utility function. Applying the martingale approach, closed-form solutions are obtained. The resulting optimal strategies are compared with portfolio insurance hedging strategies (CPPI and OBPI). We also study numerical solutions of the portfolio selection problem with constraints on the portfolio weights.","Hongcan Lin University of Waterloo, David Saunders University of Waterloo, Chengguo Weng University of Waterloo",Wednesday 10:20-10:42,N
Archimedean Copulas through Multivariate Gamma Distributions,"Inspired by Marshall-Olkin’s approach, multivariate distributions can be constructed through the use of exponential mixtures. In this paper, we propose an alternative hierarchical Archimedean copula, obtained from multivariate survival functions of multivariate mixed exponential distributions. The key element of our construction is that the latter are defined with a vector of mixing random variables, which follows a multivariate gamma distribution such as Kibble’s bivariate gamma distribution. After presenting the construction technique, properties of this new family of copulas are investigated, simulation algorithms are provides and illustrative examples are given. Risk aggregation and capital allocation under this newly proposed dependence structure are also examined.","Ihsan Chaoubi Université Laval, Hélène Cossette Université Laval, Etienne Marceau Université Laval",Wednesday 10:42-11:04,N
A Multi-State Model for a Life Insurance Product with Integrated Health Rewards Program,"With the prevalence of chronic diseases that account for a significant portion of deaths, a new approach to life insurance has emerged to address this issue. The new approach integrates health rewards programs with life insurance products; the insured are classified by fitness statuses according to their level of participation and would get premium reductions at the superior statuses. We introduce a Markov chain process to model the dynamic transition of the fitness statuses, which are linked to corresponding levels of mortality risks reduction. We then embed this transition process into a stochastic multi-state model to describe the new life insurance product. Formulas are given for calculating its benefit, premium, reserve and surplus. These results are compared with those of the traditional life insurance. Numerical examples are given for illustration.","Lidan (Lucy) Zhang Simon Fraser University, Yi Lu Simon Fraser University",Wednesday 11:04-11:26,N
On Total Dividends under a Threshold Strategy,"We consider the total dividends paid prior to ruin under a threshold strategy. We investigate a class of Sparre-Andersen risk processes and its associated class of delayed risk processes with rational-distributed inter-claim times. With the presence of a constant dividend barrier, we present a structure of integro-differential equation, which is applicable to numerous quantities such as the Gerber–Shiu function, the maximum surplus, etc. Finally, we study the fraction of time of dividends payment prior to ruin. This is of interest to an investor who wants to know how often to expect dividends. ","Ruixi Zhang University of Western Ontario, Kristina Sendova University of Western Ontario",Wednesday 11:26-11:50,N
A More Dynamic Lab Experience: Implementation of Weekly Automated and Teaching Assistant Supported Lab Exercises to Increase Student Engagement through Small-Step Learning,"The lab component provides an intimate and stimulating setting for a student to interact with lectured content. The lab is designed to supplement the lecture material and mature student comprehension. However, too often labs have mutated into rarely attended communal homework periods that actually diminish every pedagogical underpinning associated with higher education. The focus is to present the findings of a dynamic statistics lab experience, designed to refresh and re-establish the noble venture of supplemental learning. A multi-staged weekly seminar that compels preparation, requires participation, enforces comprehension, and evaluates retention. This approach utilizes small-step-learning to replace the anxiety of cumulative lab examination. We will also display the open-source software integration of WebWork and The R Project for Statistical Programming to alleviate additional resource overhead. ","Jim Stallard University of Calgary, Scott Robison University of Calgary",Wednesday 10:20-10:50,N
Educreations: Video Tools for Statistical Teaching,"Educreations is a free downloadable app for the iPad that allows instructors to make narrated videos as they write out problems with a stylus pen on the iPad screen. The created videos can then be uploaded for student access on a website. The ability for students to access videos on their own time and review questions other than their own notes can be a valuable instructional tool for both distance education and regular in-class students. In this session, I will be talking about ways the program can be used effectively inside and outside of the classroom, basics of creating videos and how to make them accessible to your students. Though not necessary, feel free to bring along an iPad if you would like to follow along with the instructions.",Jenna G. Tichon University of Manitoba,Wednesday 10:50-11:20,N
Automated Homework Generation and Grading with R,In this talk I will show how one can use R to simulate personalized assignments and to automatically grade them. This is particularly useful if one wants to make sure that students in large introductory statistics courses do their homework individually. We have successfully implemented this strategy in our introductory statistics course for engineers that is taken by about 400 students. In this course students are assigned two homework for which they have to analyze their own individual randomly generated datasets. A few R scripts generate the datasets and take care of the grading.,Thierry Duchesne Université Laval,Wednesday 11:20-11:50,N
Predictive Mean Matching Imputation in Survey Sampling,"Predictive mean matching imputation is popular for handling item nonresponse in survey sampling. We study the asymptotic properties of predictive mean matching for mean estimation. Moreover, the conventional bootstrap inference for matching estimators with fixed matches has been shown to be invalid due to the nonsmoothess nature of the matching estimator. We propose asymptotically valid replication variance estimation for predictive mean matching estimators. The key strategy is to construct replicates of the estimator directly based on linear terms of the estimator, instead of individual records of variables. Extension to nearest neighbor imputation is also discussed. A simulation study confirms that the new procedure provides confidence intervals with correct coverage properties. ","Jae Kwang Kim Iowa State University, Shu Yang North Carolina State University",Wednesday 10:20-10:50,N
Vine Copulas for Imputation of Monotone Non-response,"Monotone patterns of non-response may occur in longitudinal studies. When the measured variables are dependent it is beneficial to use their joint statistical model to impute the missing values. We propose to use vine copulas to factorize the density of the observed variables into a cascade of bivariate copulas that yield a flexible model of their joint distribution. The structure of the vine depends on the non-response pattern. We build on the work of Aas et al. (2009) and propose a method to select the model, to estimate the parameters of the bivariate copulas of the selected model, and to impute using the constructed model. The imputed values are drawn from the conditional distribution of the missing values, given the observed data. An example using the United Kingdom Labour Force Survey quaterly data illustrates the proposed methodology.","Louis-Paul Rivest Université Laval, Caren Hasler University of Toronto, Radu Craiu University of Toronto",Wednesday 10:50-11:20,N
Balanced Imputation for Swiss Cheese Nonresponse,"Swiss cheese nonresponse refers to the case where all the variables can contain missing values in a general pattern even if this wording is abuse since most of the Swiss cheeses do not have holes. However, in the case of Swiss cheese nonresponse, it is not possible to consider that some variables are known for every units and can thus be used as auxiliary variables. We propose a new technique of random donor imputation. The method is based on the establishment of consistency principles. A nonrespondent and its donor should be close to each other. The same donor must be used for all missing variables of a unit. Moreover we impose that, if we were imputing the known variables of a nonrespondent by the values of the donor, the totals of these variables must remain the same. The procedure is based on the calculation of imputation probabilities. Next, the donors are selected randomly in such a way that the constraints are satisfied.","Yves Tillé Université de Neuchâtel, Audrey-Anne Vallée Université de Neuchâtel",Wednesday 11:20-11:50,N
Dynamic Treatment Regimes via Reward Ignorant Modelling,"Personalized medicine optimizes patient outcome by tailoring treatments to patient-level characteristics. This approach is formalized by dynamic treatment regimes (DTRs): decision rules that take patient information as input and output recommended treatment decisions. The DTR literature has seen the development of increasingly sophisticated causal inference techniques, which attempt to address the limitations of our typically observational datasets. We note, however, that in practice most patients should receive optimal or near-optimal treatment, and so the outcome used as part of a typical DTR analysis does not provide much extra information. In light of this, we propose reward ignorant modelling: ignoring the outcome and eliciting an optimal DTR by regressing the observed treatment on relevant covariates, as in a more standard analysis. We present some early results investigating this concept, and analysis of data from the International Warfarin Pharmacogenetics Consortium.","Michael Wallace University of Waterloo, Erica E.M. Moodie McGill University, David A. Stephens McGill University",Wednesday 10:20-10:50,N
Issues with Inference in Adaptive Optimal Design,"Informative designs are increasingly common in experiments with sequential accrual of subjects, such as when observed responses are used to estimate an optimal design for the next cohort, making early stopping decisions for safety or efficacy, or re-estimating sample size. Typically, estimates following informative designs are not normally distributed asymptotically, even assuming a normal model with unknown mean. Instead of ignoring ancillary processes, inference can be based on fully unconditional probabilities or conditional on an ancillary statistic. Indeed, norming parameter estimates by measures of information other than Fisher information will often produce asymptotic normal random variables. We present these alternatives explicitly for a two-stage adaptive optimal design in which data accumulated in the first stage are used to select the design for a second stage. Lane \& Flournoy JPS (2012). Lane, Yao \& Flournoy JSPI (2014). Lane Wang \& Flournoy mODa 11 (2016). ",Nancy Flournoy University of Missouri,Wednesday 10:50-11:20,N
Model-Robust Designs for Generalized Linear Mixed Models with Possible Misspecification,"Generalized linear mixed models (GLMMs) are commonly used for analyzing clustered or correlated data with non-continuous responses including longitudinal data or repeated measurements. In this paper, we discuss the impact of different types of model misspecification on the estimation of regression coefficients in a GLMM and explore the construction methods of model-robust designs for GLMMs against such possible misspecification. We first derive the asymptotic distribution of the maximum likelihood estimators of the fixed effect parameters when imprecision appears in the assumed linear predictor. Then, we investigate the techniques for sequentially designing experiments where the values of the explanatory variables in a GLMM can be chosen in an optimal and robust way. Design problems in a general setting of GLMMs are addressed yet with an emphasis of their application to longitudinal data. Finally, the performance of our proposed designs is assessed by some simulation studies.","Xiaojian Xu Brock University, Sanjoy Kumar Sinha Carleton University",Wednesday 11:20-11:50,N
Causal Inference from Big Data: Theoretical Foundations and the Data-Fusion Problem,"In this paper, we summarize some of the latest results in the field of causal inference that are related to big data. In particular, we address the problem of data-fusion -- piecing together multiple datasets collected under heterogeneous conditions (i.e., different populations, regimes, and sampling methods) so as to obtain valid answers to queries of interest. The availability of multiple heterogeneous datasets presents new opportunities to big data analysts since the knowledge that can be acquired from combined data would not be possible from any individual source alone. However, the biases that emerge in heterogeneous environments require new analytical tools. Some of these biases, including confounding, sampling selection, and cross-population biases, have been addressed in isolation, largely in restricted parametric models. We here present a general, non-parametric framework for handling these biases and, ultimately, a theoretical solution to the problem of data-fusion in causal inference tasks.",Elias Bareinboim Purdue University,Wednesday 10:20-10:42,N
Breaking the Myth of Breaking Randomization: A Causal Examination of Arm-Based Meta-Analysis,"In the analysis of multi-arm randomized trials, methods for pooling data across trials belong to one of two broad classes. The first class of methods consists of contrast-based estimators that estimate the contrast in treatment effect for each pair of treatment levels and then pool across the estimated contrasts. The second class encompasses arm-based methods that contrasts marginal estimates for each treatment arm. Leading researchers have assailed arm-based methods under the broad criticism of “breaking randomization”, implying biased estimation for causal effects of treatment. However, no one has established a formal causal definition of “breaking randomization”, nor a critical examination of the amount of bias that would result. In this talk, I characterize the conditions under which the arm-based methods be biased for population causal effects and discuss the advantages that arm-based methods have over contrast-based methods with regards to precision.","Russell Steele McGill University, Mireille E.  Schnitzer Université de Montréal, Ian Shrier Lady Davis Institute, Jewish General Hospital, Montreal and McGill University",Wednesday 10:42-11:04,N
A Bayesian HSROC Model for Network Meta-analysis of Diagnostic Tests,"When evaluating the accuracy of diagnostic tests, three designs are commonly used: (1) the crossover design; (2) the randomized design; and (3) the non-comparative design. Existing methods on meta-analysis of diagnostic tests mainly consider the simple cases when the reference test in all or none of the studies can be considered as a gold standard test, and when all studies use either a randomized or non-comparative design. Yet the proliferation of diagnostic instruments and diversity of study designs being used have boosted the demand to develop more general methods. We extend the Bayesian hierarchical summary receiver operating characteristic model to network meta-analysis of diagnostic tests to simultaneously compare multiple tests under a missing data framework. Our model accounts for the potential correlations between multiple tests within a study and the heterogeneity across studies. It also allows different studies to perform different subsets of diagnostic tests. Our model is evaluated through simulations and illustrated using real data from deep vein thrombosis tests.","Qinshu Lian University of Minnesota-Twin Cities, Haitao Chu University of Minnesota-Twin cities",Wednesday 11:04-11:26,N
Hierarchical Models for Combining N-of-1 Trials,N-of-1 trials are single-patient multiple-crossover studies for determining the relative effectiveness of treatments for an individual participant. A series of N-of-1 trials assessing the same scientific question may be combined to make inferences about the average efficacy of the treatment as well as to borrow strength across the series to make improved inferences about individuals. Series that include more than two treatments may enable a network model that can simultaneously estimate and compare the different treatments. Such models are complex because each trial contributes data in the form of a time series with changing treatments. The data are therefore both highly correlated and potentially contaminated by carryover. We will use data from a series of 100 N-of-1 trials in an ongoing study assessing different treatments for chronic pain to illustrate different models that may be used to represent such data.,"Christopher Schmid Brown University, Youdan Wang Brown University",Wednesday 11:26-11:50,N
A Novel Hidden Markov Model Approach for Differentially Methylated Region Identification in DNA Methylation Data,"A new hidden Markov model (HMM)-based method is proposed to identify differentially-methylated CpG sites and regions in data arising from genome sequencing in conjunction with bisulphite treatment (BS-seq). Features of BS-Seq data include variable read depth, unevenly distributed sites, and non-smooth patterns of correlations between sites. Shortcomings of available methods include the inability to compare more than two groups or to examine associations with continuous covariates, and difficulties in handling missing and low read depth data. The proposed method is flexible in choosing HMM order, uses weights to account for read depth, and introduces a “noise state” to allow some sites to not follow the general HMM pattern. A comprehensive comparison of this new method with several existing methods will be presented, based on real and simulated data.","Farhad Shokoohi McGill University, Celia M.T. Greenwood Lady Davis Institute, Jewish General Hospital, Montreal and McGill University, David A. Stephens McGill University, Aurélie Labbe HEC Montréal",Wednesday 10:20-10:35,N
A Multivariate Rare-Variant Association Test for Non-Normally Distributed Phenotypes,"Recently, several region-based multivariate tests have been proposed to identify rare variant (RV) associations when multiple correlated continuous phenotypes are observed. However, most of such methods assume multivariate normality (MN) distribution for the phenotypes. If the MN assumption is violated, modelling phenotypic dependence using the Pearson correlation may not be suitable and can lead to inflated type I error. We address this issue using copulas to model the phenotypic dependence in a bivariate case. This allows us to assume marginal phenotypic distribution from any exponential family. We derive a copula-based variance-component score test for association between RVs and bivariate phenotypes and we provide an analytic test p-value calculation. We illustrate the performance of the proposed methodology using simulation studies and analyzing real data of the PETALE project from Sainte-Justine Hospital research center on patients who survived childhood leukemia. ","Julien St-Pierre Université du Québec à Montréal, Karim Oualkacha Université du Québec à Montréal",Wednesday 10:35-10:50,N
Measurement and Association Testing of 5-hydroxymethylcytosine (5hmC) Methylation,"Testing association between DNA methylation and health phenotypes is an important area of research. Illumina arrays exploit reisistance of methylated DNA to the effects of bisulfite treatment to allow genome-wide inference of the methylation-phenotype relationship. This method does not distinguish between two methylations types: 5mC and 5hmC. The latter form has been shown to be abundant in the human brain and so may be important for neurological phenotypes. A new method called oxidative bisulfite treatment allows for isolation of 5mC methylation and so, in theory, we can find the 5hmC methylation signal by taking the difference of signals from two parallel runs; one using the regular bisulfite treatment and one using the oxidative bisulfite treated DNA. Both of these measures are very noisy and so the difference is often negative. We examine statistical methods to evaluate the amount of 5hmC methylation signal from two parallel runs and the association of 5hmC with a phenotype.","Angelo J. Canty McMaster University, Keelin Greenlaw Lady Davis Institute, Jewish General Hospital, Montreal and McGill University, Celia M.T. Greenwood Lady Davis Institute, Jewish General Hospital, Montreal and McGill University",Wednesday 10:50-11:05,N
A Probabilistic-Based Approach for Clustering Sparse Single-Cell DNA Methylation Data,"Recent advances in technology and laboratory protocols have allowed the generation of high-throughput sequencing data from a single cell, including DNA methylation data. One of the main goals in this field of research is to cluster the data from different cells together according to their DNA methylation profiles. Because the amount of DNA material per cell is limited, the generated single-cell data are usually sparse, i.e., they contain a large amount of missing data. To address this problem we propose a probabilistic-based algorithm to infer the missing data and cluster the cells simultaneously by borrowing statistical strength across cells and neighbouring sites. We investigate the properties of our proposed method under different scenarios via simulation studies. We apply our algorithm to publicly available single-cell DNA methylation data sets. ","Camila P.E. de Souza University of British Columbia, Mirela Andronescu University of British Columbia, Sohrab Shah University of British Columbia",Wednesday 11:05-11:20,N
Modeling T-cell Trajectory Data using Hidden Markov Models,"Recent technological advances have allowed for the real-time tracking of immune cells, such as cytotoxic T-cells, as they search their environment for pathogens. Understanding the way these cells move is of interest to cell biologists who aim to improve their search efficiency via treatment. In the literature, several different types of stochastic models have been considered for T-cell movements, including Levy flight, Brownian motion, and Persistent random walks. The data that we consider is based on observing the motion of cytotoxic T-cells in a synthetic collagen matrix. Based on our observations, cells often switch between several modes of motion, and these are apparently affected by the attendance of cells nearby in the collagen. We therefore propose a hidden Markov model with separate, but correlated random effects between different Cartesian axes for the increments of the cells' trajectory. We assess the performance of the proposed model using simulation studies. ","Elaheh Torkashvand University of Waterloo, Joel Dubin University of Waterloo, Gregory Rice University of Waterloo",Wednesday 11:20-11:35,N
Block-Wise Descent Algorithms for Group Variable-selection in Quantile Regression,"Quantile regression (QR) models are attractive in several fields due to their capability to provide a rich description of a set of predictor effects on an outcome without making assumptions on the shape of the outcome distribution. In this work we consider the problem of selecting grouped variables in linear quantile regression models. We introduce a Group variable-Selection framework for Quantile Regression (GSQR) with most appealing group-penalties: the group lasso penalty, the group non-convex penalties (MCP and SCAD), the local approximation of the non-convex penalties and the sparse group lasso penalty. We propose a smooth block-wise descent algorithm that approximates the QR check function by a smooth pseudo-quantile check function, which is differentiable in zero. Then, using a maximization-minimization trick to update each group predictors, we derive a simple and efficient group-wise descent algorithm. We illustrate the GSQR approach performance using simulations in high dimensional settings and we analyze a breast cancer gene expression dataset.","Karim Oualkacha Université du Québec à Montréal, Mohamed Ouhourane Université du Québec à Montréal, Yi Yang McGill University",Wednesday 11:35-11:50,N
Mode Detection in a Non-Stationary Environment,"Multitaper spectrum estimation techniques can be used to test for the presence of modes in a space physics dataset. The model assumes the presence of random-amplitude periodic components. Random modulators only account for a single form of non-stationary process, and other forms will be sought which reside in the signal. Accounting for these processes reduces the effects of test bias due to departures from the model assumption of stationarity. To detect and identify non-stationary processes, it is standard to use a map of two-frequency coherence, where the process is manifest as a discrete set of high-coherence curves. However, when the dynamic range of the two-frequency spectrum is high, the coherence statistic is the subject of severe bias contamination. The proposed alternative is a test statistic which makes use of circularity coefficients, a set of coefficients which contains the effects of the data tapers on coherence over frequency when bias is high.",François Alastair Marshall,Wednesday 10:20-10:35,N
Box-Cox Time Series," A new algorithm is developed for Box-Cox estimation, forecasting and simulation of time series models. The algorithm is implemented in R. Some illustrative applications are presented. ","Samira Soleymani Western University, A. Ian McLeod Western University",Wednesday 10:35-10:50,N
An Extended Compound Symmetry Model for High-dimensional Covariance Matrix Estimation,"We study high-dimensional covariance matrix estimation under the assumption of the low-rank and diagonal matrix decomposition. The covariance matrix estimator is decomposed into a low-rank matrix L and a diagonal matrix D, and the rank of L is either fixed to be small or suppressed by a penalty function. Under moderate conditions on the population covariance matrix and the penalty function, our estimator enjoys some consistency results. An algorithm which iteratively updates L and D is applied to solve for the estimator. Some simulations and real data analysis are presented to show the performance with finite sample size. ","Yilei Wu University of Waterloo, Yingli Qin University of Waterloo, Mu Zhu University of Waterloo",Wednesday 10:50-11:05,N
Incorporating Contact Network Uncertainty in Individual-Level Models of Infectious Disease using Approximate Bayesian Computation,"Infection disease transmission between individuals in a heterogeneous population is often best modelled through a contact network. However, such contact network data are often unobserved. This form of missing data can be accounted for in a Bayesian data augmented framework using Markov chain Monte Carlo (MCMC). However, fitting models in an MCMC a framework can be highly computationally intensive. We investigate the fitting of network-based infectious disease models with completely unknown contact networks using both a full Bayesian MCMC framework and approximate Bayesian computation population Monte-Carlo (ABC-PMC) methods. This is done in the context of both simulated data, and data from the UK 2001 foot-and-mouth disease epidemic. We show that ABC-PMC is able to obtain reasonable approximations of the underlying infectious disease model, with huge savings in computation time. ","Waleed Almutiry Calgary University, Rob Deardon Calgary University",Wednesday 11:05-11:20,N
DM-PhyClus: A Bayesian Phylogenetic Clustering Algorithm Incorporating Dirichlet-Multinomial Priors,"Conventional phylogenetic clustering approaches rely on arbitrary cutpoints applied a posteriori to phylogenetic estimates. Although in practice, Bayesian and bootstrap-based clustering tend to lead to similar estimates, they often produce conflicting measures of confidence in clusters. Further, bootstrap support for phylogenetic clusters does not benefit from a straightforward interpretation. The current study proposes a new Bayesian phylogenetic clustering algorithm, which we refer to as DM-PhyClus, that identifies sets of sequences resulting from quick transmission chains, thus yielding easily-interpretable clusters, without using any ad hoc distance or confidence requirement. Simulations reveal that DM-PhyClus can outperform conventional clustering methods in terms of mean cluster recovery. We apply DM-PhyClus to a sample of real HIV-1 sequences, revealing a set of clusters whose inference is in line with the conclusions of a previous thorough analysis.","Luc Villandré McGill University, Aurélie Labbe HEC Montréal, Bluma Brenner McGill AIDS Centre, Lady Davis Institute, Jewish General Hospital, Michel Roger Université de Montréal, David A. Stephens McGill University",Wednesday 11:20-11:35,N
Conditional Dependence Models under Covariate Measurement Error,"In many applications, covariates are subject to measurement error. While there is a vast literature on measurement error problems in regression settings, very little is known about the impact of covariate measurement error on the dependence parameter estimation in multivariate models. We address the latter problem using a conditional copula model, and show that the dependence parameter estimates can be significantly biased if the covariate measurement error is ignored in the analysis. We identify the underlying bias pattern from the direction and magnitude of marginal effect sizes and introduce an exact bias correction method for the special case of the Gaussian copula. For more general conditional copula models, a likelihood-based correction method is proposed, in which the likelihood function is computed via Monte-Carlo integration. The consistency and asymptotic normality of the bias-corrected estimators are established. Numerical studies confirm that the proposed bias-correction methods yield an on target estimation. We demonstrate the proposed correction methods in a subset of the SWAN (Study of Women’s Health Across the Nation) data. ","Kaiqiong Zhao McGill University, Elif Fidan Acar University of Manitoba",Wednesday 10:20-10:35,N
Localized Quantile Regression of Realized Volatility,"It is challenging to accurately assess the volatility of financial assets with high-frequency data. Heteroscedasticity of the realized volatility is one of the factors that causes the difficulty. In this study, we propose a localized quantile regression approach. The proposed approach sequentially identifies homogeneous intervals and then applies a quantile regression model to each homogeneous interval. Hence the quantile regression model has time-dependent coefficients. The quantile regression model does not require distributional assumptions. Direct interpretation of the results at selected quantiles might be of more interests to practitioners in the area of finance. The simulation study shows that localized quantile regression model fits the realized volatility more closely and is also more predictive. ","Janaki Thakshila Koralage Memorial University of Newfoundland, Zhaozhi Fan Memorial University of Newfoundland",Wednesday 10:35-10:50,N
Modelling Bivariate Count Time Series with Extra Zeros," Bivariate time series counts with excessive zeros frequently occur in biological, social and environmental sciences. To deal with such data we propose a model that includes a serial correlated random effect series shared by both responses and the use of the compound Poisson distribution to capture excessive zeros. We will discuss parameter estimation for this model and implement the model in the analysis of musculoskeletal workplace injuries.","Gary Sneddon Mount Saint Vincent University, Tariqul Hasan University of New Brunswick, Renjun Ma University of New Brunswick",Wednesday 10:50-11:05,N
A Convex Regression Approach to Pickands Dependence Function Estimation,"Due to its close connection to extreme value copulas, the Pickands dependence function is a most useful tool to study dependence between extremes. This function must at the same time be convex and satisfy boundary constraints, so proposing a bona fide estimator remains a difficult task. In this work, we interpret the Pickands dependence function as some kind of regression function, and adapt a least-squares type convex regression algorithm to this problem. Despite the nonparametric nature of the problem, the algorithm does not require to select some smoothing parameter. Moreover, it can be implemented by using convex quadratic programming. Besides, consistency results for the function as well as its derivative may be derived from results for weighted empirical processes. ","Nathalie Akakpo Université Pierre et Marie Curie, Paris",Wednesday 11:05-11:20,N
Realizing the Extremes: Estimation of Tail-Risk Measures from a High-Frequency Perspective,"In this talk, we apply realized volatility forecasting to Extreme Value Theory (EVT). We propose a two-step approach where returns are first pre-whitened with a high-frequency based volatility model, and then an EVT based model is fitted to the tails of the standardized residuals. This realized EVT approach is compared to the conditional EVT of McNeil \& Frey (2000). We assess both approaches' ability to filter the dependence in the extremes and to produce stable out-of-sample VaR and ES estimates for one-day and ten-day time horizons. The main finding is that GARCH-type models perform well in filtering the dependence, while the realized EVT approach seems preferable in forecasting, especially at longer time horizons.","Debbie J. Dupuis HEC Montreal, Marco Bee University of Trento, Luca Trapin Scuola Normale Superiore",Wednesday 11:20-11:35,N
Copula Dependence Modeling for Multi-type Recurrent Events using Composite Likelihood,"It is appealing to use marginal models for the analysis of recurrent events in randomized trials. When several types of events arise, interest may lie in the nature of the dependence structure. We adopt multivariate random effect models in which the dependence between the type-specific random effects is accommodated through a Gaussian copula. Such models retain the simple interpretation of marginal treatment effects, separately reflect the heterogeneity in risk for each type of event, and provide insight into the dependence between the different types of events. Inference is proposed based on composite likelihood to avoid high dimensional integration. The relative efficiency of estimators obtained from simultaneous and two-stage estimation is examined. An application to a study of nutritional supplements in malnourished children is given in which the goal is to evaluate the reduction in the rate of several types of infection. Extensions to accommodate interval-censoring are described. ","Jooyoung Lee University of Waterloo, Richard J. Cook University of Waterloo",Wednesday 11:35-11:50,N
Multiple Imputation of Incomplete Accelerometer Data,"Accelerometer data possess unique challenges for Multiple Imputation (MI) that require careful consideration when selecting imputation models and methods, and these challenges become more daunting when imputation is performed at the epoch-level. Yet MI is appealing in this setting where accelerometers are not consistently worn by study participants, and missing epochs of data create issues for commonly employed methods of accelerometer data analysis and may introduce bias into physical activity measures. Zero-inflated Poisson and log-normal imputation models are contrasted with simpler forms of imputation and these models are evaluated based on their epoch-level imputation accuracy as well as their ability to recover common physical activity summary measures of interest. The accelerometer data used in this investigation is from The Active Play Study, our ongoing physical activity study involving children and youth in Kingston, Ontario.","Michael A. McIsaac Queen's University, Lauren Paul Queen's University",Wednesday 10:20-10:42,N
Analysis of Heterogeneous Lifetime Data with Measurement Error in Covariate,"In the analysis of lifetime data, interest often lie in modeling the time to a particular event or the occurrence of certain event, identifying the associated risk factors and evaluating their effects on survival time or event occurrence. We may encounter a heterogeneous group where event of interest may not occur or cease to occur after a period of time for some individuals. Mover-stayer models are developed using a binary variable to indicate whether the underlying process has resolved. In addition, data with measurement error often occurs in medical research and public health, and the problems associated are well known yet often ignored due to technical difficulties. We propose an expectation-maximization (EM) algorithm based method to estimate the parameters and adjust for mismeasurement present in the covariates. Its advantages over the naïve analysis are illustrated in simulation studies. The motivating study in breast cancer is examined to illustrate the proposed method.","Hua Shen University of Calgary, Ying Yan University of Calgary, Haocheng Li Unviersity of Calgary",Wednesday 10:42-11:04,N
Missing Data Mechanisms for Incomplete Longitudinal Observations in both Responses and Covariates,"Missing observations in both responses and covariates are commonly observed in longitudinal studies. When missing data are missing not at random, inferences under the likelihood framework often require joint modelling of response and covariate processes, as well as missing data processes associated with incompleteness of responses and covariates. Specification of these four joint distributions is difficult in both modelling and computation. We discuss three specific missing data mechanisms which have simplified pairwise likelihood formulations. These likelihood functions lead to consistent estimators, and enjoy better robustness and computational convenience. The proposed method is evaluated empirically by simulation studies and applied in a real world dataset.","Haocheng Li University of Calgary, Grace Yi University of Waterloo",Wednesday 11:04-11:26,N
A Self-Selecting Procedure for the Optimal Discretization of the Timeline for Longitudinal Causal Inference Methods with Electronic Health Data,"In longitudinal observational studies, marginal structural models (MSM) can be used to account for time-dependent confounding. Many estimation approaches, such as Longitudinal Targeted Maximum Likelihood Estimation (LTMLE), require a finite number of time points where variables are measured. In contrast, electronic health data (EHD) are produced by mechanisms that collect health system user information in real-time. In common practice, the longitudinal analysis is preceded by an arbitrary discretization. In this talk, we describe the causal inference problem when the operating data are defined as a coarsening (discretization) of the observed data. We propose a novel selection procedure that uses cross-validation on an LTMLE loss function to select an ""optimal"" discretization for estimation of a data-adaptive MSM parameter of interest. We present a simulation study and apply this approach to a study using EHD to evaluate the relative impact of asthma medications during pregnancy. ","Mireille E. Schnitzer Université de Montréal, Steve Ferreira Guerra Université de Montréal, Amélie Forget Université de Montréal, Lucie Blais Université de Montréal",Wednesday 11:26-11:50,N
Predictive Analytics in Business,"This talk will present an overview of the history and current advances of predictive analytics in the world of business, with applications drawn from such diverse fields as retail, utilities, manufacturing, public service, financial services, insurance, and sports. Methods will include discussion of such topics as ensemble models, gradient boosting, partial least squares, support vector machines, text mining, link analysis, and association rule mining.",Shirley E. Mills Carleton University,Wednesday 13:30-14:30,N
"Probability-Scale Residuals for Continuous, Discrete, and Censored Data","We describe a new residual for general regression models defined as pr(Y*<y)-pr(Y*>y), where y is the observed outcome and Y* is a random variable from the fitted distribution. This probability-scale residual (PSR) can be written as E(sign(y,Y*)), whereas the popular observed-minus-expected residual can be thought of as E(y−Y*). Therefore the PSR is useful in settings where differences are not meaningful or where the expectation of the fitted distribution cannot be calculated. We present several desirable properties of the PSR that make it useful for diagnostics and measuring residual rank correlation. We demonstrate its utility for continuous, ordered discrete, and censored outcomes, and with various models including Cox regression, quantile regression, and ordinal cumulative probability models, for which fully specified distributions are not desirable or needed, and in some cases suitable residuals are not available. The residual is illustrated with simulated and real data.","Bryan E. Shepherd Vanderbilt University School of Medicine, Chun Li Case Western Reserve University, Qi Liu Merck",Wednesday 13:30-14:15,N
Robust Estimation under Cellwise and Casewise Contamination,"In traditional robust statistics, it is generally assumed that the majority of the observations in the data are free of contamination, while only a minority of the observations are contaminated. The contaminated observations are flagged as outliers and down-weighted even if only a single component is contaminated. Some observations may fully depart from the bulk of the data. This situation usually refers to casewise outliers. However, observations can be only partially contaminated. This type of contamination often appears as single outlying cells in a data matrix and therefore, usually refers to cellwise contamination. Under cellwise contamination, a lot of information could be lost through down-weighting the whole observation, especially for high-dimensional data. Recent work has shown that procedures that proceed in such way are not robust. In this talk, we will sketch out our proposal to estimate multivariate location and scatter under this cell-and-casewise contamination.","Andy Leung University of British Columbia, Victor J. Yohai Universidad de Buenos Aires, Ruben H. Zamar University of British Columbia",Wednesday 14:15-15:00,N
Maximum Likelihood Estimation of Marine Predators' Diet Proportions,"Diet compositions of marine predators are often of interest for marine ecologists in trophic structure studies where non-lethal sampling has created a need for non-invasive diet estimation techniques. Methods using fatty acids have been developed to obtain dietary estimates that have previously been difficult to acquire. Building on the existing quantitative fatty acid signature analysis, we have constructed a maximum likelihood approach to estimating dietary proportions that includes standard errors and allows for the potential for inclusion of covariates in the model. The model is assessed using simulated as well as real-life data, and results are compared to those of the current approach.",Holly N. Steeves Dalhousie University,Wednesday 15:00-15:30,N
Estimation and Inference in Directional Mixed Models for Compositional Data,"We propose a new class of mixed effects model for compositional data based on the Kent distribution for directional data, where the random effects also have Kent distributions. One useful property of the new directional mixed model is that the marginal mean direction has a closed form and is interpretable. The random effects enter the model in a multiplicative way via the product of a set of rotation matrices and the conditional mean direction is a random rotation of the marginal mean direction. For estimation we apply a quasi-likelihood method which results in solving a new set of generalised estimating equations and these are shown to have low bias in typical situations. For inference we use a nonparametric bootstrap method for clustered data which does not rely on estimates of the shape parameters (shape parameters are difficult to estimate in Kent models). The new approach is shown to be more tractable than the traditional approach based on the logratio transformation.","Janice Lea Scealy Australian National University, Alan Welsh Australian National University",Wednesday 15:30-16:00,N
A Dirichlet Regression Model for Compositional Data with Zeros,"Compositional data are met in many different fields, such as economics, archaeometry, ecology, geology and political sciences. Regression where the dependent variable is a composition is usually carried out via a log-ratio transformation of the composition or via the Dirichlet distribution. However, when there are zero values in the data these two ways are not readily applicable. Suggestions for this problem exist, but most of them rely on substituting the zero values. In this paper we adjust the Dirichlet distribution when covariates are present, in order to allow for zero values to be present in the data, without modifying any values. To do so, we modify the log-likelihood of the Dirichlet distribution to account for zero values. Examples and simulation studies exhibit the performance of the zero adjusted Dirichlet regression.","Michail Tsagris University of Crete, Connie Stewart University of New Brunswick",Wednesday 16:00-16:30,N
"Statistics, Science and Technology","There have been huge advances in measurement, computing and information technology over the 50 years since the University of Manitoba Department of Statistics was founded, and these have transformed the ways that statistics is applied. In this talk I will review some history of statistical applications in science and technology before and then during this period. I will then discuss some current activities and some important issues, including the use of big data, the role of statistics within data science, and the distinction between scientific learning and technological problems involving decision-making.",Jerald F. Lawless University of Waterloo,Wednesday 15:00-15:30,N
Reflections on Interactions with Industry ,"As one of the speakers in this session celebrating the 50th Anniversary of the Department of Statistics at the University of Manitoba, sponsored by the Business and Industrial Statistics Section, I was asked to reflect on some earlier interactions between the Department of Statistics and industrial partners. I will provide examples of these interactions and describe how these interactions arose. I will also discuss the benefits of academic statisticians being involved in such interactions, as well as some of the challenges and lessons learned.",John F. Brewster University of Manitoba,Wednesday 15:30-16:00,N
The Family of Chi-square Copulas and Applications in Spatial Statistics ,"First, some dependence properties of the family of chi-square copulas are presented. This family is very attractive because it generalizes the Gaussian copula and allows for flexible modeling of high dimensional random vectors. This class of dependence structures is then applied in spatial statistics to predict the value of a non-Gaussian stationary random field at a position where it has not been observed. This interpolation method is then compared to the kriging method in a real example and via a simulation study.","Marie-Hélène Toupin Université Laval, Jean-François Quessy Université du Québec à Trois-Rivières, Louis-Paul Rivest Université Laval",Wednesday 15:00-15:22,N
Vine Copula Regression,"Regression analysis is one of the oldest topics in statistics. If explanatory variables and a response variable of interest are simultaneously observed, then fitting a joint multivariate density to all variables would enable prediction via conditional distributions. The vine copulas have proven to be a flexible tool in high-dimensional dependence modeling. We introduce a new regular vine copula based regression model, which avoids issues of variable selection, variable transformation, and heteroscedasticity. It uses general regular vines and we have developed an efficient algorithm to compute the conditional distribution. Our model is able to handle a mix of continuous and discrete variables. That is, the predictors and response variable can be either continuous or discrete; thus we have a unified model for regression and classification problems. We demonstrate the predictive performance of the proposed method using a real data set. ","Bo Chang University of British Columbia, Harry Joe University of British Columbia",Wednesday 15:22-15:45,N
Improved Estimation and Identification of Block-Exchangeable Dependence Structures,"We use concepts from copula modeling and statistical learning to improve the estimation of the Kendall's tau correlation matrix of a set of variables. We are interested in recovering structural properties of the variables' joint distribution. Relaxing the full exchangeability assumption, we assume the existence of exchangeable subsets of variables without explicitly specifying them. These block-exchangeable dependence structures are such that their corresponding tau matrix contains many identical entries. We iteratively modify the usual estimator of tau by introducing constraints corresponding to the structural properties discovered, thus producing a sparser representation of the dependence. A first algorithm is used to produce a sequence of decreasingly complex models among which the final model is to be selected. Final selection is provided by a second algorithm. The procedure yields improved estimation of the tau matrix and better representation of the dependence among the variables. The approach is applied to data from the US stock market.","Samuel Perreault Université Laval, Thierry Duchesne Université Laval, Johanna Neslehova McGill University",Wednesday 15:45-16:07,N
Generalized Simulated Method-of-Moments for Copula Parameters of Arbitrary Dimension,"In this talk, we develop a general method for estimating a vector of parameters of an arbitrary dimension in multivariate copula models. The proposed estimator is based on the first p-moments of the multivariate probability integral transformation that one can associate to a given parametric copula model. An unbiased estimator of these p-moments are first described, from which a method-of-moments estimator of the unknown vector of parameters is defined. In order that the method be applicable even when explicit expressions for the theoretical moments are not available, a simulated version of the estimator is developed as well. Interestingly, the latter can be performed as long as one is able to simulate from a given copula model. The consistency and asymptotic normality of these estimators are formally established under standard and mild conditions. The performance of these estimators in terms of bias and mean-squared errors is investigated through an extensive simulation study.","Mohamed Belalia Université du Québec à Trois-Rivières, Jean-François Quessy University of Québec at Trois-Rivières",Wednesday 16:07-16:30,N
Matrix Linear Discriminant Analysis,"We propose a novel linear discriminant analysis approach for the classification of high-dimensional matrix-valued data that commonly arise from imaging studies. Motivated by the equivalence of the conventional linear discriminant analysis and the ordinary least squares, we consider an efficient nuclear norm penalized regression that encourages a low-rank structure. Theoretical properties including a non-asymptotic risk bound and a rank consistency result are established. Simulation studies and an application to electroencephalography data show the superior performance of the proposed method over the existing approaches. ","Dehan Kong University of Toronto, Wei Hu University of California Irvine, Weining Shen University of California Irvine, Hua Zhou University of California Los Angeles",Wednesday 15:00-15:30,N
Scalar on Image Regression with Application to Multiple Sclerosis MRI Lesion Data.,"Multiple sclerosis (MS) is an autoimmune disease that attacks the central nervous system. Magnetic resonance imaging (MRI) plays a central role in the diagnosis and management of MS patients because damage to the myelin is visible on MRI. A research question of interest is whether these MRI images can predict MS subtype. To answer this question we propose a Bayesian scalar-on-image regression model with scalar outcome (MS subtype) and binary image (MRI) covariates. Parameters of these covariates are spatially varying and are fitted using Gaussian random fields. Our proposed model is fitted to a real data set consisting of 228 MS patients with 3 MS subtypes. A Hamiltonian Monte Carlo (HMC) algorithm is proposed to implement full Bayesian statistical inference. To reduce the computational burden, we code the problem to run in parallel on a graphical processing unit (GPU).","Cui Guo University of Michigan, Timothy D. Johnson University of Michigan",Wednesday 15:30-16:00,N
Towards More Reliable Neuroimaging-Based Biomarkers in Mental Illness: The Case of Schizophrenia Discrimination using fMRI Data ,"Much work in the recent years has shown associations between schizophrenia and abnormal brain functional connectivity, which represents the pattern of interaction between different brain regions. The functional connectivity network can be derived from functional MRI (fMRI) data, which provides an indirect measure of the brain neural activity – during rest or task – at a reasonable spatial and temporal resolution. Despite much work, no reliable “statistical biomarker” has yet been identified that can be used to diagnose schizophrenia or guide treatment. Our work aims at finding reliable neuroimaging-based sets of features or “biomarkers” that are not only statistically significant, but also predictive of the disease in a novel subject and stable across different data subsets. Results of applying this approach to functional connectivity network features extracted from fMRI data of patients diagnosed with schizophrenia and healthy controls from a multi-site fMRI dataset will be presented.","Mina Gheiratmand University of Alberta, Russell Greiner University of Alberta, Matthew Brown University of Alberta, Andrew Greenshaw University of Alberta, Serdar Dursun University of Alberta",Wednesday 16:00-16:30,N
Calibration of Measurements: An Extension of Classic Measurement Error Theory,"The classical notion of measurement error typically relies on a mean-zero assumption on the expectation of the errors, conditional on the data. However, in many applied problems in the medical, social and ecological sciences, such an assumption is often unreasonable. In this talk, we will define the notion of a weakly calibrated measurement for an unobservable true quantity based upon a weaker mean-zero assumption that more accurately reflects the structure of applied problems in these disciplines. We will then explore certain attractive features of this measurement error formulation, some not present in the traditional model. Finally, we will indicate how these theoretical considerations can lead to practical inferential gains in the context of some real problems from the medical, social and ecological sciences. ","Edward Kroc University of British Columbia, Bruno D. Zumbo University of British Columbia",Wednesday 15:00-15:15,N
Non-identifiable Interaction Models with Misclassification ,"In statistical association studies, outcomes are determined by combinations of error-prone and accurately measured variables that interact with each other. As techniques dealing with interaction terms when error-prone variables are involved are quite challenging, in practice, we often see the use of additive models that ignore the interaction effects. However, the consequence of erroneously omitting interactions in those models is recognized as one of the main threats to efficiency of these studies. In fact, in the presence of error-prone variables, detecting the interaction term is more challenging than either the individual effects. In order to improve the accuracy and precision in the assessment these factors, one needs to take into account these errors. On the other hand, including interaction terms often raises the issue of non-identifiability. In this presentation, I will discuss some association studies with a primary focus on interaction terms, subject to misclassification, and some remedies to deal with the issue of non-identifiable parameters. ",Taraneh Abarin Memorial University of Newfoundland,Wednesday 15:15-15:30,N
Revisiting Variance Decomposition when Independent Samples Intersect,"When a sample is obtained by intersecting two independent samples, the variance and the estimator of variance of the expanded estimator can be decomposed in two different ways according to the sample used in the conditional expectations. Even if both methods give the same result, we show that with one decomposition, it is generally more practical to compute the variance and with the other one, it is more convenient to estimate the variance. These differences in the decompositions are due to simplifications in joint inclusion probabilities and this sheds light on two particular cases: the reverse approach used in the nonresponse case and the estimation of variance in two-stage sampling designs.","Audrey-Anne Vallée Université de Neuchâtel, Yves Tillé Université de Neuchâtel",Wednesday 15:30-15:45,N
Combining Multiple Samples through Density Ratio Models for More Efficient Estimation,"Multiple samples are often available under different settings and more efficient inferences on a particular population may be obtained by suitable use of information from all samples. In this talk, we describe the use of empirical likelihood with Density Ratio Model (DRM) to effectively combine information from multiple samples. We show that more samples outcompetes fewer samples for its dominance in estimation efficiency, as long as DRM assumption is satisfied. Our presentation focuses on the estimation of DRM parameters and demonstrates the efficiency gain of parameter estimation by including more samples under DRM.","Yilin Chen Carleton University, Song Cai Carleton University",Wednesday 15:45-16:00,N
Multivariate Response-Dependent Two-Phase Sampling Designs ,"In some studies, it may be relatively affordable to measure the response variable, while a covariate might be expensive to obtain. In this situation, cost-efficient response-dependent two-phase sampling designs could be considered: in phase 1, we have easily measured variables including the response variable for all individuals in the cohort or in a large random sample from the population, and in phase 2, we obtain expensive variables for a subset of individuals selected according to their response variable obtained in phase 1. We consider the likelihood- and pseudo-likelihood based methods for incomplete data analysis to estimate the regression parameters. We extend the estimation methods for the setting where the second phase sampling depends on multiple response variables. The objective is to compare the efficiency of estimators, to develop an efficient sampling model specification, and to address the change in efficiency compared to the univariate response-dependent sampling.","Ananthika Nirmalkanna Memorial University of Newfoundland, Yildiz Yilmaz Memorial University of Newfoundland",Wednesday 16:00-16:15,N
Response Best Subset Selection Model and Efficient Estimation in Multivariate Linear Regression,"In practice, the number of responses is not known prior to data analysis so that responses also need variable selection, for which few research has been found. In this paper, we address a response variable selection approach, propose a novelty response best subset selection (RBS) model and provide an estimation procedure to perform response best subset selection and regression coefficient estimation via penalizing unselected responses variables. Our estimations enjoy the oracle property: model consistency and asymptotic normality of regression coefficient estimators corresponding to the selected response variables. The proposed model and procedure can be extended to the situation where response variables have group effects. Our finite sample-sized sample simulation studies demonstrate that the proposed model and procedure are efficient and completive. We apply our promising approach to study a real data set and the results shed new light on selection of response variables. ","Jianhua Hu Shanghai University of Finance and Economics, China, Jian Huang University of Iowa, Feng Qiu Zhejiang Agriculture and Forestry University, China",Wednesday 16:15-16:30,N
Motivating Introductory Statistics,"Students will be greatly assisted in learning statistics if they feel that it is useful and relevant to their needs. Ideas from science studies can be used to both provide possible ways of motivating student interest in statistics, and also can be used to suggest why students with good mathematical and computational skills may be completely unprepared for most of the content in an introductory statistics course. ",Jeffrey D. Picka University of New Brunswick,Wednesday 15:00-15:15,N
Flipping the Online Learning Environment,"The flipped classroom model is gaining popularity. With it's focus on hands-on learning and practical exercise, it makes sense that this would be a useful model for teaching statistics. Course notes read prior to the live class and practice problems completed in a lab setting (small groups, smaller class size than the lectures) are just a couple of ways that we are using the flipped classroom model to teach Introductory level statistics online. This model appeals to more learning styles, and also to students who want to be more self-directed. The challenge becomes making sure they are prepared for tests and exams. The purpose of my presentation is to share some tools that are working for our course and see if others have different things that have worked in their classrooms.",Jennifer Thornton Mount Saint Vincent University,Wednesday 15:15-15:30,N
Tall Tales vs Tall Tails: Rules of Thumb for Statistical Inference,"There has been discussion recently within the statistical community as to whether the rule of thumb “n > 30” really ensures the central limit theorem holds. Needless to say, the conclusion to date is that this is a gross oversimplification which holds for relatively “well behaved” data. How does this impact our teaching of introductory statistics courses? We want our students to be critical as to how and when basic techniques are applied, but we want them to feel confidence in the results when these techniques are used appropriately. I will discuss a simulation study for typical procedures taught in introductory statistics, examining how and when they can be judged to be reliable. ","Anne Michele Millar Mount Saint Vincent University, Andrea Perreault Mount Saint Vincent University",Wednesday 15:30-15:45,N
Effectiveness of Gamification Feature in Introductory Statistics Course,"Two key components to successful learning are engagement with the material and practice. The introductory Statistics course attracts a large number of students with a wide range of abilities and interests. Twelve WeBWorK homework sets are assigned to students, and they receive instant feedback while they can make infinite attempts to achieve the correct answer until the deadline. We further added gamification feature in the exist WeBWorK, which is designed to increase the amount of time that students want to spend on homework. The gamification techniques we made use of are “leveling” and “achievement”. We created five levels from “Novice” to “Junior Statistician”, five challenge questions, and twelve various achievement badges. In general the achievement badges were constructed to reward students for practicing good homework habits and to encourage them to solve every problem. We will share the result and discuss the effectiveness of these gamification features. ",Sohee Kang University of Toronto Scarborough,Wednesday 15:45-16:00,N
The Status of Statistics Curricula in Canada,"This talk will review the current state of undergraduate Statistics curricula at universities across Canada. We will present both quantitative and qualitative information on the structure and composition of Major programs in Statistics. More specifically, we will look at the number and type of course requirements for each program, the learning outcomes they serve, the topics and skills they develop, as well as other relevant information. The talk intends to give an overview of how we collectively educate Statisticians, with the ultimate goal of helping identify directions for future curricular development. ","Sotirios Damouras University of Toronto Scarborough, Sohee Kang University of Toronto Scarborough",Wednesday 16:00-16:15,N
