% !TEX root = meeting.tex
% !TEX encoding = UTF-8 Unicode

\SectionPlain{Abstracts}{Résumés}
\addtolength{\textheight}{-1in}
\pagestyle{fancy}
\renewcommand{\headrulewidth}{0.4pt}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 1A-A2: SSC Presidential Invited Address\\L'allocution de l'invit\'e du pr\'esident de la SSC}
\begin{center}\large\bfseries Room/Salle: Welch - David Howes\end{center}
\label{abs-sid:spi}
\begin{center}{\large\bfseries Abstract/R\'esum\'e}
\end{center}
%% Talk spi-jdk
\def\whenwhere{[Monday May 30 / lundi 30 mai, 8:45]\\}
{\Author{John D.}{Kalbfleisch}{kalbfleisch}{1A-A2}%
\abshead{\absauthor{JOHN D. KALBFLEISCH}\absaffil{University of Michigan}}
        {Some Statistical Topics of Personal Interest\newline
        Quelques sujets statistiques d'intérêt personnel}

\absSideBySide{I will discuss some statistical ideas and issues that have been recurring themes in my recent methodological research and work in applications. The topics covered will be somewhat idiosyncratic and will include issues of randomization and experimental design, fixed and random effects, empirical Bayes, and profiling of medical providers.
}{Je discuterai d'idées et de sujets statistiques qui constituent des thèmes récurrents dans mes récentes recherches en méthodologie et dans leurs applications. Les sujets couverts seront quelque peu idiosyncratiques et incluront la randomisation et la planification d'expériences, les effets fixes et aléatoires, les méthodes bayésiennes empiriques et le profilage des fournisseurs médicaux.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 1B-I1: CANSSI at the SSC: A Statistical Science Showcase\\L'INCASS \`a la SSC : vitrine de la science statistique}
\begin{center}{\large Organizer and Chair / Responsable et président:  John Braun (University of British Columbia)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Welch - David Howes\end{center}
\label{abs-sid:csa}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk csa-aa
\def\whenwhere{[Monday May 30 / lundi 30 mai, 10:20]\\}
{\Author{Alisha}{Albert-Green}{albertgreen}{1B-I1}\Author{John}{Braun}{braun}{1B-I1}\Author{Charmaine}{Dean}{dean}{1B-I1}%
\abshead{\absauthor{ALISHA ALBERT-GREEN}\absaffil{University of Western Ontario}, \absauthor{JOHN BRAUN}\absaffil{University of British Columbia Okanagan}, \absauthor{CHARMAINE DEAN}\absaffil{University of Western Ontario}}
        {A Spatio-Temporal Cluster Process for Modelling Storm Cells\newline
        Un processus de regroupement spatio-temporel pour la modélisation des cellules orageuses}

\absSideBySide{Storm cells are the smallest component of a storm-producing system.  A cluster of such cells is referred to as a storm and a storm system consists of a cluster of 
storms.  This research develops a model for these storm cells over space and time.  Specifically, we extend the modified Thomas process, which is commonly employed 
for the analysis of clustered point processes, to account for the hierarchical clustering present in our data.  We do this by allowing the parents to follow a doubly
stochastic process, namely a log-Gaussian Cox process.  This model is applied to storm cell data from the Bismarck radar station in North Dakota, USA and parameter 
estimation is done using minimum contrast estimation.
}{Les cellules orageuses sont les plus petites composantes d'un système produisant des tempêtes. Un regroupement de ces cellules est appelé tempête et un système de 
tempêtes consiste en un regroupement de tempêtes. Cette recherche développe un modèle pour ces cellules orageuses à travers le temps et l'espace. Spécifiquement, 
nous élargissons le processus modifié de Thomas, qui est couramment employé dans l'analyse des processus de regroupement de points, pour prendre en compte le 
regroupement hiérarchique présent dans nos données. Pour ce faire, nous permettons aux parents de suivre un processus doublement stochastique, c'est-à-dire un 
processus Cox log-gaussien. Ce modèle est appliqué aux données de cellules orageuses provenant de la station radar Bismarck dans le Dakota du Nord, USA, et l'estimation
de paramètres est effectuée en utilisant une estimation contraste minimum.
}}
%% Talk csa-db
\def\whenwhere{[Monday May 30 / lundi 30 mai, 10:50]\\}
{\Author{Derek}{Bingham}{bingham}{1B-I1}%
\abshead{\absauthor{DEREK BINGHAM}\absaffil{Simon Fraser University}}
        {Design of Experiments in the CANSSI CRT ``Statistical Modeling of the World: Computer and Physical Models in Earth, Ocean, and Atmospheric Sciences''\newline
        Conception d'expériences dans le PRC «~Modélisation statistique du monde~: modèles informatiques et physiques en sciences de la Terre, des océans et de l'atmosphère~» de l'INCASS}

\absSideBySide{Design and analysis of experiments continue to make important and far-reaching contributions to scientific investigation. Indeed, it is hard to imagine any field where it does not play a role. In the CANSSI Collaborative Research Team project ``Statistical Modeling of the World: Computer and Physical Models in Earth, Ocean, and Atmospheric Sciences'', experimental design is used for both physical and computer experiments. This talk will outline the role of experimental design for both settings, and a new design approach for exploring non-convex regions that are common in geophysical application will be presented.
}{La conception et l'analyse d'expériences continuent d'apporter des contributions importantes et de grande ampleur à la recherche scientifique. Il est en effet difficile d'imaginer un domaine où ces concepts ne jouent aucun rôle. Dans le Projet de recherche en collaboration «~Modélisation statistique du monde~: modèles informatiques et physiques en sciences de la Terre, des océans et de l'atmosphère~» de l'INCASS, le plan expérimental sert à la fois pour les expériences physiques et informatiques. Cette présentation précisera le rôle du plan d'expériences dans les deux contextes et proposera une nouvelle approche de conception permettant d'explorer les régions non-convexes si communes dans les applications géophysiques.
}}
%% Talk csa-lr
\def\whenwhere{[Monday May 30 / lundi 30 mai, 11:20]\\}
{\Author{Radu V.}{Craiu}{craiu}{1B-I1}%
\abshead{\absauthor{RADU V. CRAIU}\absaffil{University of Toronto}}
        {Modern Research Topics in Copula Methods: A Report from the CANSSI Collaborative Team Project\newline
        Sujets de recherche modernes sur les méthodes de copules: un rapport de l'équipe de projet collaborative INCASS}

\absSideBySide{We will summarize some of the research activities undertaken by the members of the Copula CANSSI Collaborative Research Team: Christian Genest, Louis-Paul Rivest, 
Elif Acar, Harry Joe, Johanna Neslehova, Jean-Francois Quessy, Bruno Remi\-llard and Radu Craiu.\\
\\
Focus will be placed on our own research efforts on inference for conditional copula models in which the calibration function depends on multiple covariates. 
We will discuss a novel approach that relies on a Gaussian process prior used in conjunction with a single index model formulation. The methodology will be 
illustrated using simulations and real data.
}{Nous résumerons quelques activités de recherche entreprises par les membres de l'équipe de projet collaborative sur les copules INCASS: Christian Genest, Louis-Paul Rivest, 
Elif Acar, Harry Joe, Johanna Neslehova, Jean-Francois Quessy, Bruno Remillard et Radu Craiu.\\
\\
Nous nous intéresserons principalement à notre effort de recherche sur l'inférence pour les modèles de copules conditionnels pour lesquels la fonction de redressement 
dépend de plusieurs covariables. Nous discuterons d'une approche novatrice qui s'appuie sur un processus a priori gaussien utilisé conjointement avec une formulation 
de modèle à indice unique. Cette méthodologie sera illustrée à l'aide de simulations et de données réelles.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 1B-I2: Quantile regression and Extreme Value Analysis\\R\'egression quantile et analyse des valeurs extr\^emes}
\begin{center}{\large Organizer and Chair / Responsable et présidente:  Mei Ling Huang (Brock University)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Welch 209\end{center}
\label{abs-sid:qev}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk qev-vc
\def\whenwhere{[Monday May 30 / lundi 30 mai, 10:20]\\}
{\Author{Vincenzo}{Coia}{coia}{1B-I2}\Author{Harry}{Joe}{joe}{1B-I2}\Author{Natalia}{Nolde}{nolde}{1B-I2}%
\abshead{\absauthor{VINCENZO COIA}\absaffil{University of British Columbia}, \absauthor{HARRY JOE} \& \absauthor{NATALIA NOLDE}\absaffil{The University of British Columbia}}
        {Forecasting Extremes for Flooding\newline
        Prévision des extrêmes pour les inondations}

\absSideBySide{When forecasting using past data, methods such as regression and maximum likelihood are available for reporting a predictive distribution that reflects what a typical 
outcome might be. However, these techniques are inadequate at structuring and fitting the predictive distribution's tail -- a shortfall when the end-user is only 
interested in what an extreme outcome might be. To get around these shortcomings, a copula-based nonlinear modelling technique and a composite quantile regression 
model-fitting technique are proposed. The result is an approach for building the tail of a predictive distribution that addresses questions like ``how bad could it 
get?'', and is applied to flood forecasting of the Bow River in Alberta.
}{Pour les prévisions utilisant des données historiques, les méthodes telles que la régression et le maximum de vraisemblance sont disponibles pour fournir une loi 
prédictive qui reflète ce qu'un résultat typique pourrait être. Cependant, ces techniques sont inadéquates pour structurer et faire correspondre les ailes de la loi 
prédictive -- un manque lorsque l'utilisateur final s'intéresse uniquement aux résultats extrêmes possibles. Pour contourner ces lacunes, une technique de modélisation 
non linéaire fondée sur les copules et une technique de régression quantile composite ajustée sur le modèle sont proposées. Le résultat est une approche pour la 
construction d'une aile de la loi prédictive qui aborde des questions telles que «jusqu'où cela va dégénérer?» et s'applique à la prévision d'inondations de 
la rivière Bow en Alberta.
}}
%% Talk qev-xh
\def\whenwhere{[Monday May 30 / lundi 30 mai, 10:50]\\}
{\Author{Keith}{Knight}{knight}{1B-I2}%
\abshead{\absauthor{KEITH KNIGHT}\absaffil{University of Toronto}}
        {Non-parametric Estimation of Extreme Conditional Quantiles and Lawson's Algorithm\newline
        L'estimation non paramétrique des quantiles conditionnels extrêmes et l'algorithme de Lawson}

\absSideBySide{Lawson's algorithm, developed in the early 1960s by C.L. Lawson, is an algorithm for computing $L_\infty$ estimates in a linear regression model; it is perhaps the original iteratively reweighted least squares (IRLS) algorithm. We will explore the use of a similar IRLS algorithm to compute non-parametric estimates of extreme (and near-extreme) conditional quantiles.
}{L'algorithme de Lawson, développé au début des années 1960 par C.L. Lawson, est un algorithme pour calculer les estimateurs $L_\infty$ dans un modèle de régression linéaire; il est peut-être le premier algorithme utilisant les moindres carrés itérativement repondérés (IRLS). Nous examinons l'utilisation d'un algorithme IRLS similaire afin de calculer des estimateurs 
non  paramétriques des quantiles conditionnels extrêmes (et quasi-extrêmes).
}}
%% Talk qev-cn
\def\whenwhere{[Monday May 30 / lundi 30 mai, 11:20]\\}
{\Author{Christine}{Nguyen}{nguyen}{1B-I2}\Author{Mei Ling}{Huang}{huang}{1B-I2}\Author{Wai Kong}{Yuen}{yuen}{1B-I2}%
\abshead{\absauthor{CHRISTINE NGUYEN}, \absauthor{MEI LING HUANG} \& \absauthor{WAI KONG YUEN}\absaffil{Brock University}}
        {On Weighted Quantile Regression\newline
        Régression quantile pondérée}

\absSideBySide{In recent years, studies of heavy tailed distributions have rapidly developed. For multivariate heavy tailed distributions, estimation of conditional quantiles at 
very high or low tails is of interest in numerous applications. Quantile regression uses an L1- loss function, and the optimal solution of linear programming for 
estimating coefficients of regression. This paper proposes a weighted quantile regression method on high quantile regression for certain extreme value sets. The 
Monte Carlo simulations show good results of the proposed weighted method. Comparisons of the proposed method and existing methods are given. The paper also 
investigates real-world examples by using the proposed weighted method.
}{Ces dernières années, les études sur les lois à ailes lourdes se sont rapidement développées. Pour les lois à ailes lourdes multivariées, l'estimation de quantiles 
conditionnels à ailes très hautes ou très basses est d'intérêt dans de nombreuses applications. La régression quantile utilise une fonction de perte L1, ainsi que la 
solution optimale de la programmation linéaire pour l'estimation des coefficients de régression. Cet article propose une méthode de régression quantile pondérée sur 
régression quantile élevée pour certains ensembles de valeurs extrêmes. Les simulations Monte-Carlo affichent de bons résultats pour la méthode pondérée proposée. 
Des comparaisons entre la méthode proposée et des méthodes existantes sont présentées. Cet article examine aussi des exemples concrets en utilisant la méthode 
pondérée proposée.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 1B-I3: Statistical Methods in Public Health\\M\'ethodes statistiques en sant\'e des populations}
\begin{center}{\large Chair/Président: Patrick Brown (University of Toronto)\protect\\[5pt]
Organizer/Responsable: Lennon Li (Public Health Ontario)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 325\end{center}
\label{abs-sid:smp}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk smp-rd
\def\whenwhere{[Monday May 30 / lundi 30 mai, 10:20]\\}
{\Author{Rob}{Deardon}{deardon}{1B-I3}\Author{Waleed}{Almutiry}{almutiry}{1B-I3}%
\abshead{\absauthor{ROB DEARDON}\absaffil{University of Calgary}, \absauthor{WALEED ALMUTIRY}\absaffil{University of Guelph}}
        {Infectious Disease Modelling in the Presence of Underlying Contact Network Uncertainty\newline
        Modélisation de maladies infectieuses en présence d'incertitude du réseau de contact sous-jacent}

\absSideBySide{Infection disease transmission is often best modelled through contact networks, especially in human populations. These contact networks can represent sexual or 
social relationships, or shared employment, education or domestic status.\\
\\
However, contact network data are often difficult and/or costly to collect; there may be privacy issues, or issues of incomplete or incorrect recall in surveys. As 
a result, contact network data desirable for the modelling may be missing entirely, only partially recorded, or recorded with uncertainty.

Such missing/uncertain data can be incorporated within a data-augmented Bayesian framework using methods such as Markov chain Monte Carlo (MCMC). Here, we consider 
disease transmission modelling in the presence of uncertain contact networks, touching upon issues of computational difficulty.
}{La transmission de maladies infectieuses est mieux modélisée par des réseaux de contact, particulièrement dans les populations humaines. Ces réseaux de contact peuvent 
représenter des relations sociales ou sexuelles, ou un emploi, une éducation ou un statut domestique partagés.\\
\\
Cependant, les données des réseaux de contact sont souvent difficiles et/ou coûteuses à recueillir; il peut y avoir des problèmes de protection de la vie privée, ou 
des problèmes dus à des rappels incomplets ou erronés dans les sondages. Par conséquent, des données de réseaux de contact désirées pour la modélisation peuvent être 
entièrement manquantes, partiellement enregistrées, ou enregistrées avec incertitude.

Des telles données manquantes/incertaines peuvent être incorporées à l'intérieur d'un cadre bayésien augmenté en données en utilisant des méthodes Monte-Carlo par chaînes de 
Markov. Nous considérons la modélisation de la transmission de maladies en présence de réseaux de contact incertains, touchant aux questions de difficulté 
computationnelles.
}}
%% Talk smp-al
\def\whenwhere{[Monday May 30 / lundi 30 mai, 10:50]\\}
{\Author{Andrew}{Lawson}{lawson}{1B-I3}\Author{Rachel}{Carroll}{carroll}{1B-I3}%
\abshead{\absauthor{ANDREW LAWSON} \& \absauthor{RACHEL CARROLL}\absaffil{Medical University of South Carolina}}
        {Spatio-temporal Latent Model Choice: With Application in Multiple Disease Small Area Analysis\newline
        Choix de modèles latents spatio-temporels : application dans plusieurs analyses de maladies de petits domaines}

\absSideBySide{Space-time variation in disease is the natural focus of many health studies. Conventionally the variation of the space-time risk fields is modeled via random effects. However, it is possible to consider spatial and temporal risk components selected from predefined sets of terms and linked via mixing parameters. In addition there can be important information gained from the analysis of multiple diseases within this context, where shared components could inform about shared environmental risks. In this presentation I will demonstrate the development of multivariate spatio-temporal (MVST) linked models for analysis of shared risk. Some simulation results and a data example based on lung and bronchus cancer and melanoma in South Carolina (1996-2009) will be provided
}{Les variations spatiotemporelles des maladies sont le centre d'intérêt naturel de nombreuses études. En général, la variation des zones de risques spatiotemporelles est modélisée au moyen d'effets aléatoires. Cependant, il est possible de considérer que les composantes spatiales et temporelles du risque  sont sélectionnées à partir d'un ensemble de termes prédéfinis et reliés au moyen de paramètres de mélange. De plus, il est possible de gagner beaucoup d'information importante à partir de l'analyse de plusieurs maladies, dans un contexte où les composantes partagées pourraient informer sur les risques environnementaux partagés. Dans cet exposé, je présenterai le développement de modèles multivariés spatiotemporels pour l'analyse du risque partagé. Des résultats de simulations ainsi qu'un exemple de données basé sur le cancer du poumon et des bronches, ainsi que sur les mélanomes en Caroline du Sud (1996-2009) seront aussi présentés.
}}
%% Talk smp-rr
\def\whenwhere{[Monday May 30 / lundi 30 mai, 11:20]\\}
{\Author{Rhonda}{Rosychuk}{rosychuk}{1B-I3}%
\abshead{\absauthor{RHONDA ROSYCHUK}\absaffil{University of Alberta}}
        {Cluster Detection Tests for the Surveillance of Emergency Department Presentation Data\newline
        Tests de détection de grappes pour la surveillance des données de présentation des services des urgences}

\absSideBySide{Alberta has been one of the few provinces in Canada to have an extensive ambulatory care database that includes both hospital-based and community-based services. 
Presentations to Emergency Departments are part of the database and when linked with other provincial databases, excellent population-based analyses can result. 
The provincial health authority sometimes uses this data to provide analyses based on geography. While cluster detection tests have been popular in the statistical 
literature, they have been underutilized in this jurisdiction. This talk will focus on data issues, cluster detection tests, and knowledge translation activities in 
the context of Alberta Emergency Department presentation data.
}{L'Alberta est l'une des seules provinces canadiennes à avoir une base de données détaillée de ses soins ambulatoires qui comprend les services en milieu 
hospitalier et communautaire. Les présentations aux services des urgences font parties de la base de données et lorsque connectée avec d'autres base données 
provinciales, d'excellentes analyses basées sur la population peuvent en résulter. L'autorité sanitaire provinciale utilise parfois ces données pour produire des 
analyses basées sur la géographie. Alors que les tests de détection ont été populaires dans la littérature statistique, ils ont été sous-utilisés dans cette juridiction. 
Cet exposé sera axé sur les questions des données, les tests de détection de grappes, ainsi que les activités de translation des connaissances dans le contexte de 
données de présentations des services des urgences en Alberta.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 1B-I4: Statistical Models in Actuarial Science\\Mod\`eles statistiques en actuariat}
\begin{center}{\large Organizer and Chair / Responsable et président:  Jean-Philippe Boucher (UQAM)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 242\end{center}
\label{abs-sid:sma}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk sma-mp
\def\whenwhere{[Monday May 30 / lundi 30 mai, 10:20]\\}
{\Author{Mathieu}{Pigeon}{pigeon}{1B-I4}\Author{Arthur}{Charpentier}{charpentier}{1B-I4}%
\abshead{\absauthor{MATHIEU PIGEON}\absaffil{UQAM}, \absauthor{ARTHUR CHARPENTIER}\absaffil{Université du Québec à Montréal et Université de Rennes 1}}
        {Macro and Micro Methods in Non-Life Claims Reserving\newline
        Modèles individuels et collectifs pour les réserves en assurance IARD}

\absSideBySide{Traditional methods for loss reserving in non-life insurance (Chain-Ladder, Bornhuetter Ferguson, London Chain, etc.) are constructed for aggregated data. Progress over the past decades in computing resources and greater availability of individual statistical data have resulted in the development of new models for individual dataset. In this talk, we will analyze the impacts of both approaches (macro and micro, respectively) on the evaluation of the reserve risk based on some parametric models (Poisson regression, quasi-Poisson regression, etc.) and generalized method of moments estimators. Results will be illustrated with simulations and a dataset from the industry.
}{La plupart des techniques traditionnelles permettant l'évaluation des réserves en assurances IARD (Chain-Ladder, Bornhuetter-Ferguson, London Chain, etc.) sont construites pour des données dites agrégées. Le progrès des outils informatiques et la disponibilité accrue de données détaillées ont permis récemment l'éclosion de techniques adaptées à des données individuelles (ou non agrégées). Dans cette présentation, nous analyserons les impacts de l'utilisation de données agrégées ou non agrégées sur les risques liés à l'évaluation des réserves. Nous ferons cette analyse à partir de modèles construits autour de la distribution de Poisson (régression Poisson, régression quasi-Poisson, etc.) et d'estimateurs basés sur la méthode des moments généralisés. Les résultats seront illustrés à l'aide de simulations et de données réelles provenant de l'industrie.
}}
%% Talk sma-ps
\def\whenwhere{[Monday May 30 / lundi 30 mai, 10:50]\\}
{\Author{Peng}{Shi}{shi}{1B-I4}\Author{Lu}{Yang}{yang}{1B-I4}%
\abshead{\absauthor{PENG SHI}\absaffil{University of Wisconsin}, \absauthor{LU YANG}\absaffil{University of Wisconsin-Madison}}
        {A Pair Copula Construction Model for Insurance Experience Rating\newline
        Modèle de construction de copule par paires pour la tarification d'antécédents en assurance}

\absSideBySide{In non-life insurance, insurers use experience rating to adjust premium to reflect the policyholder's previous claim experience. Performing prospective experience rating can be challenging when the claim distribution is complex. In this article, we introduce a mixed vine pair copula construction framework for modeling semicontinuous longitudinal claims. Specifically, a two-component mixture regression is employed to accommodate the zero inflation and thick tails in claim distribution. The temporal dependence among repeated observations is modeled using a sequence of bivariate conditional copulas based on a mixed D-vine. In the application of government property insurance from the state of Wisconsin, we show that the proposed approach offers substantial opportunities for separating risks and identifying profitable business when compared with alternative experience rating methods.
}{Dans le cadre d'une assurance non-vie, les assureurs utilisent la tarification personnalisée pour ajuster les primes afin de refléter l'expérience précédente en matière de sinistres du titulaire de police. Les évaluations de l'expérience prospective peuvent être difficiles lorsque la distribution des demandes d'indemnisation est complexe. Dans cet exposé, nous présentons une construction de copules en vignes mixtes par paires pour la modélisation de demandes d'indemnisations semi-continues longitudinales. Notamment, une régression par mélange à deux composantes est employée pour prendre en compte des ailes épaisses à surreprésentation de zéros dans la distribution des demandes d'indemnisation. La dépendance temporelle parmi les observations répétées est modélisée au moyen d'une séquence de copules conditionnelles bivariées en fonction d'une vigne D mixte. Dans le cadre de l'application de l'assurance des biens du gouvernement à partir de l'État du Wisconsin, nous démontrons que l'approche proposée offre des occasions substantielles de séparer les risques et identifier des affaires rentables lorsqu'on les compare à d'autres méthodes d'évaluation d'expérience.
}}
%% Talk sma-cct
\def\whenwhere{[Monday May 30 / lundi 30 mai, 11:20]\\}
{\Author{Cary Chi-Liang}{Tsai}{tsai}{1B-I4}\Author{Tzuling}{Lin}{lin}{1B-I4}%
\abshead{\absauthor{CARY CHI-LIANG TSAI}\absaffil{Simon Fraser University}, \absauthor{TZULING LIN}\absaffil{National Chung Cheng University}}
        {Applications of the Bühlmann Credibility Model to Mortality Forecasting\newline
        Applications du modèle de crédibilité de Bühlmann à la prévision de la mortalité}

\absSideBySide{In this presentation, we first propose the Bühlmann credibility model to forecast mortality rates, and then compare forecasting performances between the proposed Bühlmann model and some leading mortality models. Empirical results based on mortality data for both genders of some country, a wide range of fitting year spans, and three forecasting periods show that the Bühlmann credibility model contributes to much better forecasting performances measured by the MAPE (mean absolute percentage error). Moreover, we give meaningful credibility interpretations regarding the decrement rates of an individual time trend for age $x$ and a group time trend for all ages, and discuss the effects of the slope and intercept of the linear functions for the forecasted mortality rates under the underlying models.
}{Dans cette présentation, nous commençons par proposer que le modèle de crédibilité de Bühlmann peut servir à la prévision des taux de mortalité, puis nous en comparons la performance en matière de prévision à d'autres modèles de mortalité bien connus. Des résultats empiriques basés sur les données de mortalité pour les deux sexes dans un pays donné, diverses fourchettes d'années d'ajustement et trois périodes de prévision montrent que le modèle de crédibilité de Bühlmann contribue à des performances de prévision bien meilleures telles que mesurées par le MAPE (pourcentage d'erreur absolue moyen). Par ailleurs, nous donnons des interprétations valables de la crédibilité en ce qui concerne les taux de décrémentation d'une tendance temporelle individuelle pour un âge $x$ et d'une tendance temporelle de groupe pour tous âges, avant de discuter des effets de la pente et de l'ordonnée des fonctions linéaires pour les taux de mortalité prévus par les modèles sous-jacents.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 1B-I5: Stochastic Modelling for Energy Markets\\Mod\'elisation stochastique des march\'es de l'\'energie}
\begin{center}{\large Chair/Président: Reg Kulperger (University of Western Ontario)\protect\\[5pt]
Organizer/Responsable: Matt Davison (University of Western Ontario)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 246\end{center}
\label{abs-sid:sme2}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk sme2-vk
\def\whenwhere{[Monday May 30 / lundi 30 mai, 10:20]\\}
{\Author{Matt}{Davison}{davison}{1B-I5}\Author{Nicolas}{Merener}{merener}{1B-I5}%
\abshead{\absauthor{MATT DAVISON}\absaffil{University of Western Ontario}, \absauthor{NICOLAS MERENER}\absaffil{Universidad Torcuato di Tella}}
        {Corn/Ethanol Price Spreads Emerging from an Inhomogeneous Set of Price-Influencing Ethanol Producers\newline
        Écarts de prix entre le maïs et l'éthanol dans un ensemble non homogène de producteurs d'éthanol ayant une influence sur les prix}

\absSideBySide{There is some evidence that the recent trend towards producing ethanol from corn has induced a correlation between corn and ethanol prices that was not previously seen. Corn ethanol producers may be considered to hold a real exchange option between the two commodities. We examine a simple model of a set of corn ethanol producers, with different efficiencies, whose activities in the corn and ethanol markets influence prices in those markets. We assume the facilities start (or are idled) in ``merit order'' of efficiency and examine the prediction of such a model on corn/ethanol spreads. We close by presenting our progress toward discerning related patterns in actual market data.
}{Il semble bien que la tendance récente à produire de l'éthanol à partir du maïs a conduit à une corrélation entre le prix du maïs et celui de l'éthanol qui n'existait pas auparavant. On peut considérer que les producteurs d'éthanol de maïs détiennent une option d'échange entre ces deux produits. Nous examinons un modèle simple d'un ensemble de producteurs d'éthanol de maïs, plus ou moins efficaces, dont les activités sur les marchés du maïs et de l'éthanol en influencent les prix. Nous supposons que les installations démarrent (ou sont mises hors service) par «~ordre de mérite~» de leur efficacité et examinons la prévision par ces modèles des écarts de prix entre le maïs et l'éthanol. Pour conclure, nous présentons nos progrès vers l'identification de tendances liées dans les données de marché réelles.
}}
%% Talk sme2-as
\def\whenwhere{[Monday May 30 / lundi 30 mai, 10:50]\\}
{\Author{Anatoliy}{Swishchuk}{swishchuk}{1B-I5}%
\abshead{\absauthor{ANATOLIY SWISHCHUK}\absaffil{University of Calgary}}
        {Stochastic Modelling and Pricing of Energy Markets' Contracts with Local Stochastic Delayed and Jumped Volatilities\newline
        Modélisation stochastique et tarification des contrats sur les marchés de l'énergie avec volatilités stochastiques locales à délais et à sauts}

\absSideBySide{In this talk we study stochastic modelling and pricing of electricity, gas and temperature markets' contracts with delay and jumps, modelled by independent increments processes. The spot price models are based on a sum of non-Gaussian Ornstein-Uhlenbeck processes (including geometric and arithmetic models), describing the long- and short-term fluctuations of the spot dynamics. The models include jumps not only in the spot dynamics but also in the stochastic volatility to describe typical features like spikes of energy spot prices and jumps in the volatility. The basic products in these markets are spot, futures and forward contracts, swaps and options written on these, which will be investigated in our talk.
}{Dans cette présentation, nous étudions la modélisation stochastique et la tarification des contrats sur les marchés de l'électricité, du gaz et de la température avec délais et sauts, telles que modélisées par des processus d'incréments indépendants. Les modèles de prix au comptant (spot) sont basés sur une somme de processus d'Ornstein-Uhlenbeck non gaussiens (notamment des modèles géométriques et arithmétiques), qui décrivent les fluctuations à court et long terme de la dynamique du spot. Les modèles incluent des sauts non seulement dans la dynamique du spot, mais aussi dans la volatilité stochastique pour décrire des caractéristiques typiques comme les pics de prix de l'énergie au comptant et les sauts de volatilité. Les produits de base sur ces marchés sont les contrats spot, à terme et à livrer, les swaps et les options sur ces contrats, que nous étudierons ici.
}}
%% Talk sme2-ht
\def\whenwhere{[Monday May 30 / lundi 30 mai, 11:20]\\}
{\Author{Hans}{Tuenter}{tuenter}{1B-I5}%
\abshead{\absauthor{HANS TUENTER}\absaffil{University of Toronto}}
        {Firming Wind Farm Revenues\newline
        Stabilisation des revenus des parcs éoliens}

\absSideBySide{Wind is an intermittent energy source and introduces randomness in a wind farm's power output and revenues. We show how tailored financial products, such as production puts or swaps, can increase the economic viability of a wind farm. To price these products, we use historical simulation of wind speed in combination with a power curve. A case study will be presented to illustrate the approach.
}{Le vent est une source d'énergie intermittente, ce qui introduit de la variabilité dans la production d'énergie et de revenus des parcs éoliens. Nous démontrons comment des produits financiers adaptés, comme des «puts»  ou des «swaps» de production, peuvent améliorer la viabilité économique d'un parc éolien. Pour calculer le prix de ces produits, nous utilisons des simulations historiques de vitesse du vent ainsi qu'une courbe de puissance. Une étude de cas permettra d'illustrer cette approche.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 1B-I6: What Do School Teachers and Students Need from the SSC?\\Que peut faire la SSC pour les enseignants et les \'el\`eves ?}
\begin{center}{\large Chair/Président: Lorna Deeth (University of Guelph)\protect\\[5pt]
Organizer/Responsable: Alison Gibbs (University of Toronto)}
\end{center}
\par \vspace{3pt}

\begin{center}\large\bfseries Room/Salle: Thistle 258\end{center}
\label{abs-sid:wds}
\begin{center}{\large\bfseries Description}\end{center}
\bigskip
\absSideBySide{The purpose of this panel session is to explore the role of the SSC, both current and potential, in statistics education in Canadian schools. First, we will discuss the current pedagogy of training future teachers in the areas of mathematics and statistics, at both the elementary/junior and secondary levels, with emphasis on the required critical thinking, verbal, and evaluation skills. This will be followed by a discussion on the current state of statistics education at the secondary level, and the challenges faced by mathematics educators. Finally, a discussion on the statistics education curriculum in Ontario, and the potential role of the SSC in future developments, will be presented. The panelists include educators with strong connections to schools and teachers, who will share their experiences and open a dialogue about what the Statistical Society of Canada and its members can and should do to support statistics education, and educators, in Canada.}{Cette réunion de spécialistes a pour but d'explorer le rôle de la Société statistique du Canada (SSC), à la fois actuel et éventuel, dans l'enseignement de la statistique dans les écoles canadiennes. Nous aborderons tout d'abord la pédagogie en vigueur pour la formation des futurs enseignants dans les domaines des mathématiques et des statistiques, à la fois au primaire et au secondaire, en insistant sur les  aptitudes requises sur le plan de la pensée critique, de l'expression verbale et de l'évaluation. Nous traiterons ensuite de l'état actuel de l'enseignement de la statistique au secondaire et des défis auxquels font face les professeurs de mathématiques. Enfin, nous ferons le point sur le curriculum d'éducation statistique en Ontario et sur le rôle potentiel de la SSC dans leur élaboration future. Parmi les panélistes se trouvent des éducateurs en relation étroite avec les établissements scolaires et les enseignants qui viendront faire part de leur expérience, tout en amorçant un échange sur ce que peuvent et doivent faire la SSC et ses membres pour appuyer l'enseignement de la statistique, et ses éducateurs, au Canada.}\bigskip
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 1B-C1: Biostatistics: Methodological Innovation 1\\Biostatistique : innovation m\'ethodologique 1}
\begin{center}{\large Chair/Président: Caitlin Daly (McMaster University)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 259\end{center}
\label{abs-sid:bmi}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk bmi-yw
\def\whenwhere{[Monday May 30 / lundi 30 mai, 10:20]\\}
{\Author{Ying}{Wu}{wu}{1B-C1}\Author{Richard J.}{Cook}{cook}{1B-C1}%
\abshead{\absauthor{YING WU} \& \absauthor{RICHARD J. COOK}\absaffil{University of Waterloo}}
        {A Two-Phase Model For Chronic Disease Processes Under Intermittent Inspection\newline
        Modèle à deux phases pour les processus de la maladie chronique sous inspection intermittente}

\absSideBySide{A two-phase model is developed for chronic diseases with an indolent phase which is followed by a phase with more active disease resulting in progression and damage. Weakly parametric models with piecewise constant baseline hazard and rate functions are specified and an expectation-maximization algorithm is described for model fitting. Simulation studies examining the performance of the proposed model show good performance under maximum likelihood and two-stage estimation. An application to data from the motivating study of disease progression in psoriatic arthritis illustrates the procedure and identifies new human leukocyte antigens associated with the duration of the indolent phase.
}{Un modèle à deux phases est mis au point pour les maladies chroniques avec une phase indolente suivie d'une phase plus active de la maladie qui entraîne son évolution et des dommages. Des modèles paramétriques faibles avec des fonctions de risque de base et de taux constantes par morceaux sont précisés et un algorithme espérance-maximisation est décrit pour l'ajustement du modèle. Des études de simulation portant sur le modèle proposé en montrent la bonne performance sous une estimation par vraisemblance maximale et en deux étapes. Une application aux données tirées de l'étude de l'évolution de la polyarthrite psoriasique illustre la procédure et identifie de nouveaux antigènes des leucocytes humains associés à la durée de la phase indolente.
}}
%% Talk bmi-wz
\def\whenwhere{[Monday May 30 / lundi 30 mai, 10:35]\\}
{\Author{Wenyan}{Zhong}{zhong}{1B-C1}\Author{Jingjing}{Wu}{wu}{1B-C1}\Author{Xuewen}{Lu}{lu}{1B-C1}%
\abshead{\absauthor{WENYAN ZHONG}, \absauthor{JINGJING WU} \& \absauthor{XUEWEN LU}\absaffil{University of Calgary}}
        {Group Selection in Proportional Odds Model for Right-Censored Data\newline
        Sélection de groupe dans le modèle des cotes proportionnelles pour les données censurées à droite}

\absSideBySide{We consider the problem of variable selection for proportional odds model with right-censored data. Proportional odds model is an important alternative to Cox model when proportional hazards assumption is not satisfied. To fit the proportional odds model, we proposed an estimation by minimizing a negatively weighted partial log-likelihood subject to a group bridge penalty. This penalty encourages sparse solutions and selects significant groups and, simultaneously, individual variables within significant groups. To implement this newly proposed method, we developed a computational algorithm that has been shown efficient through simulation studies. Some theoretical properties of the proposed method were also presented.
}{Nous soupesons le problème de la sélection des variables pour le modèle proportionnel de cotes pour les données censurées à droite. Ce dernier modèle est une importante solution de rechange au modèle de Cox lorsque l'hypothèse des risques proportionnels n'est pas satisfaite. Pour ajuster le modèle des cotes proportionnelles, nous proposons une estimation en minimisant un logarithme de vraisemblance partielle pondéré négativement et assujetti à une pénalité liée au groupe. Cette pénalité encourage les solutions à faible densité et sélectionne des groupes significatifs et, simultanément, des variables individuelles dans les groupes significatifs. Pour mettre en œuvre cette nouvelle méthode proposée, nous avons formulé un algorithme computationnel dont des études en simulation ont montré l'efficacité. Nous avons aussi présenté certaines propriétés théoriques de cette méthode.
}}
%% Talk bmi-sfg
\def\whenwhere{[Monday May 30 / lundi 30 mai, 10:50]\\}
{\Author{Steve Ferreira}{Guerra}{guerra}{1B-C1}\Author{Mireille}{Schnitzer}{schnitzer}{1B-C1}\Author{Lucie}{Blais}{blais}{1B-C1}%
\abshead{\absauthor{STEVE FERREIRA GUERRA}, \absauthor{MIREILLE SCHNITZER} \& \absauthor{LUCIE BLAIS}\absaffil{Université de Montréal}}
        {A Self-Selecting Procedure for the Optimal Discretization of a Continuous Timeline for Longitudinal Causal Inference Methods\newline
        Une procédure de sélection automatique pour la discrétisation optimale d'une ligne de temps continue pour l'application de méthodes longitudinales en inférence causale}

\absSideBySide{In health care research, administrative databases have become abundantly used to conduct causal inference for drug effects. In longitudinal settings, we generally rely on a discretization of the patient timeline and bias may result when coarsening is arbitrarily chosen by the researcher. This is partially due to discarded information about time-dependent confounders. Using a Longitudinal Targeted Maximum Likelihood Estimation loss based function, we developed a cross-validation self-selecting procedure of the optimal discretization of a continuous timeline. We use a simulation study to evaluate the bias-variance tradeoff of such a method.
}{En santé, les bases de données administratives sont amplement utilisées afin d'appliquer l'inférence causale aux effets de médicaments. Dans un contexte longitudinal, nous discrétisons généralement la ligne du temps et un biais peut survenir lorsque le raffinement du pas est arbitrairement choisi par le chercheur, biais partiellement attribuable à la perte d'information sur les variables de confusion dépendantes du temps. Utilisant une fonction de perte basée sur l'estimation par maximum de vraisemblance ciblée longitudinale, nous avons développé une méthode de validation croisée auto-sélective pour la discrétisation optimale d'une ligne de temps continue. Nous évaluons le compromis biais-variance de cette méthode par simulation.
}}
%% Talk bmi-zn
\def\whenwhere{[Monday May 30 / lundi 30 mai, 11:05]\\}
{\Author{Zelalem}{Negeri}{negeri}{1B-C1}\Author{Mateen}{Shaikh}{shaikh}{1B-C1}\Author{Joseph}{Beyene}{beyene}{1B-C1}%
\abshead{\absauthor{ZELALEM NEGERI}, \absauthor{MATEEN SHAIKH} \& \absauthor{JOSEPH BEYENE}\absaffil{McMaster University}}
        {Bivariate Random Effects Meta-Analysis Models for Diagnostic Test Accuracy Studies Using Arcsine-Based Transformations\newline
        Modèles de meta-analyse bivariés à effets aléatoires pour les études sur l'exactitude des tests diagnostiques utilisant les transformations arc-sinus}

\absSideBySide{The bivariate random-effects model using the logit transformation is commonly employed to synthesize the sensitivity and specificity of diagnostic test accuracy studies. We propose the arcsine square root and the Freeman-Tukey double arcsine transformation to overcome some shortcomings of the logit transformation. We evaluated the performance of the three transformations using extensive simulations in terms of bias, root mean square error and coverage probability. We varied several parameters including number of studies and sample size. The proposed transformations outperformed the logit transformation in terms of all of the performance criteria. The methods have also been illustrated using real data sets.
}{Le modèle bivarié à effets aléatoires utilisant la transformation logit est souvent employé afin de synthétiser la sensibilité et la spécificité des études sur l'exactitude des tests diagnostiques. Nous proposons la racine carrée de l'arc-sinus et la transformation de Freeman-Tukey à double arc-sinus afin de surmonter certaines lacunes de la transformation logit. Nous avons évalué l'efficacité des trois transformations au moyen de simulations permettant d'évaluer le biais, l'erreur quadratique moyenne et la probabilité de couverture. Nous avons varié plusieurs paramètres, dont le nombre d'études et la taille de l'échantillon. Les transformations proposées ont surpassé la transformation logit concernant tous les critères d'efficacité. Nous avons également illustré les méthodes proposées à l'aide de jeux de données réelles.
}}
%% Talk bmi-mt
\def\whenwhere{[Monday May 30 / lundi 30 mai, 11:20]\\}
{\Author{Maxime}{Turgeon}{turgeon}{1B-C1}\Author{Karim}{Oualkacha}{oualkacha}{1B-C1}\Author{Antonio}{Ciampi}{ciampi}{1B-C1}\Author{Golsa}{Dehghan}{dehghan}{1B-C1}\Author{Celia}{Greenwood}{greenwood}{1B-C1}\Author{Aurélie}{Labbe}{labbe}{1B-C1}%
\abshead{\absauthor{MAXIME TURGEON}\absaffil{McGill University}, \absauthor{KARIM OUALKACHA}\absaffil{Université du Québec à Montréal}, \absauthor{ANTONIO CIAMPI}, \absauthor{GOLSA DEHGHAN}, \absauthor{CELIA GREENWOOD} \& \absauthor{AURÉLIE LABBE}\absaffil{McGill University}}
        {Principal Component of Explained Variance: An Efficient and Optimal Data Dimension Reduction Framework for Association Studies\newline
        Composante principale de variance expliquée: une méthode de réduction dimensionnelle efficace et optimale pour les études d'association}

\absSideBySide{The genomics era has led to an increase in the dimensionality of the data collected. In this context, dimension-reduction techniques can be used to summarize high-dimensional signals, to further test for association with the covariates of interest. We revisit one such approach, renamed here as Principal Component of Explained Variance (PCEV). This method seeks a linear combination of outcomes by maximising the proportion of variance explained by the covariates of interest. We propose a general analytical framework that is conceptually simple and free of tuning parameters, and we provide a simple computational strategy for high-dimensional outcomes, along with testing procedures.
}{L'ère génomique a entrainé une augmentation de la dimension des données. Dans ce contexte, les techniques de réduction dimensionnelle peuvent être utilisées pour réduire un signal multidimensionnel, et ensuite tester l'association avec des covariables d'intérêt. Nous revisitons une de ces méthodes, rebaptisée Composante Principale de Variance Expliquée. Cette méthode cherche la combinaison linéaire des variables réponses qui maximise la proportion de variance expliquée par les covariables. Nous proposons un cadre analytique général qui est conceptuellement simple et dépourvu de paramètres de réglage. Nous proposons aussi une stratégie computationnelle simple pour des variables réponses de grande dimension, accompagnée de procédures pour tests d'hypothèse.
}}
%% Talk bmi-mt2
\def\whenwhere{[Monday May 30 / lundi 30 mai, 11:35]\\}
{\Author{Marie-Hélène}{Toupin}{toupin}{1B-C1}\Author{Jean-François}{Quessy}{quessy}{1B-C1}\Author{Louis-Paul}{Rivest}{rivest}{1B-C1}%
\abshead{\absauthor{MARIE-HÉLÈNE TOUPIN}\absaffil{Université Laval}, \absauthor{JEAN-FRANÇOIS QUESSY}\absaffil{Université du Québec à Trois-Rivières}, \absauthor{LOUIS-PAUL RIVEST}\absaffil{Université Laval}}
        {On the Family of Chi-Square Copulas\newline
        À propos de la famille de copules Khi-carré}

\absSideBySide{The family of Chi-square copulas that has recently appeared in the literature is very attractive because it generalizes the Gaussian copula and allows for flexible modeling for high dimensional random vectors. This presentation will explore the theoretical properties and the practical usefulness of this class of dependence structures. An application of the Chi-square copulas will also be developed, namely parameter estimation.
}{La famille de copules Khi-carré qui a récemment fait son apparition dans la littérature est très attirante, car elle généralise la copule normale et permet la modélisation flexible de vecteurs aléatoires de grande dimension. Dans cette présentation, on explorera les propriétés théoriques et l'utilité pratique de cette classe de structures de dépendance. Une application de la copule Khi-carré sera également développée, à savoir l'estimation de paramètre.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 1B-C2: Biostatistics: Methods and Applications 1\\Biostatistique : m\'ethodes et applications 1}
\begin{center}{\large Chair/Président: Ehsan Karim (McGill University)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 255\end{center}
\label{abs-sid:bma2}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk bma2-ja
\def\whenwhere{[Monday May 30 / lundi 30 mai, 10:20]\\}
{\Author{Jill}{Ainsworth}{ainsworth}{1B-C2}\Author{Erica}{Moodie}{moodie}{1B-C2}\Author{Abbas}{Khalili}{khalili}{1B-C2}%
\abshead{\absauthor{JILL AINSWORTH}, \absauthor{ERICA MOODIE} \& \absauthor{ABBAS KHALILI}\absaffil{McGill University}}
        {Estimating Parameters from a Finite Mixture of Generalised Linear Mixed-Effect Models Using the Maximum Likelihood Method\newline
        Estimer des paramètres d'un mélange fini de modèles linéaires généralisés à effets mixtes en utilisant la méthode de maximum de vraisemblance}

\absSideBySide{Consider a dataset with the following features that the analysis should take into account: (i) discrete outcomes, (ii) the longitudinal measurements, and (iii) 
possibility of subpopulations across which covariate effects differ. In this paper, I propose the use of a finite mixture of generalised linear mixed-effect models 
(FinMix GLMM) as an appropriate analytic approach. The performance of maximum likelihood estimator of the model parameters are explored by a simulation study as well 
as an analysis of data from the Scottish Early Rheumatoid Arthritis (SERA) cohort.
}{Considérez un ensemble de données avec les caractéristiques suivantes, qu'une analyse doit prendre en considération: (i) résultats discrets, (ii) les mesures longitudinales, 
et (iii) la possibilité de sous-populations à travers lesquelles les effets des covariables diffèrent. Dans cet article, je propose l'utilisation d'un mélange fini de 
modèles généralisés à effets mixtes (FinMix GLMM) comme approche analytique appropriée. La performance de l'estimateur de maximum de vraisemblance des paramètres du 
modèle est étudiée à l'aide d'une étude de simulation et d'une analyse des données de la cohorte écossaise de l'arthrite rhumatoïde précoce (SERA).
}}
%% Talk bma2-jee
\def\whenwhere{[Monday May 30 / lundi 30 mai, 10:35]\\}
{\Author{Joycelyne Efua}{Ewusie}{ewusie}{1B-C2}\Author{Monika}{Kastner}{kastner}{1B-C2}\Author{George}{Thomplinson}{thomplinson}{1B-C2}\Author{Sharon}{Straus}{straus}{1B-C2}\Author{Jemila S.}{Hamid}{hamid}{1B-C2}%
\abshead{\absauthor{JOYCELYNE EFUA EWUSIE}\absaffil{McMaster University}, \absauthor{MONIKA KASTNER}\absaffil{Li Ka Shing Knowledge Institute, St Michaels Hospital, Toronto}, \absauthor{GEORGE THOMPLINSON}\absaffil{University of Toronto, Toronto}, \absauthor{SHARON STRAUS} \& \absauthor{JEMILA S. HAMID}\absaffil{Li Ka Shing Knowledge Institute, St Michaels Hospital, Toronto}}
        {Methods for Interrupted Time Series Analysis of Count Data\newline
        Méthodes pour l'analyse des séries chronologiques interrompues de données de dénombrement}

\absSideBySide{This study evaluates different methods of analyzing count data in interrupted time series (ITS) designs. Fixed, random and zero-inflated Poisson models were fitted to an empirical data obtained from a previous ITS study evaluating the effect of an implementation tool to improve osteoporosis disease management. Extensive simulations with different scenarios were also performed to investigate performance. Preliminary results show that the fixed effects and zero-inflated Poisson models were best-fitting for estimating the effect of intervention on number of patients given BMD testing and osteoporosis medication, respectively.
}{Cette étude évalue les différentes méthodes d'analyse de données de dénombrement dans les modèles de séries chronologiques interrompues (SCI). Les modèles de Poisson fixes, aléatoires et à surreprésentation de zéros étaient ajustés à des données empiriques d'une étude SCI précédente évaluant l'effet d'un outil de mise en œuvre pour améliorer la gestion de la maladie de l'ostéoporose. Des études de simulations approfondies ont également été réalisées sous différents scénarios pour étudier la performance. Les résultats préliminaires montrent que les modèles de Poisson à effets fixes et à surreprésentation de zéros conviennent le mieux pour estimer l'effet de l'intervention sur le nombre de patients ayant reçu respectivement un test de DMO et des médicaments contre l'ostéoporose.
}}
%% Talk bma2-taf
\def\whenwhere{[Monday May 30 / lundi 30 mai, 10:50]\\}
{\Author{Tania Alarcon}{Falconi}{falconi}{1B-C2}\Author{Bertha}{Estrella}{estrella}{1B-C2}\Author{Fernando}{Sempértegui}{sempertegui}{1B-C2}\Author{Magaly}{Koch}{koch}{1B-C2}\Author{Elena}{Naumova}{naumova}{1B-C2}%
\abshead{\absauthor{TANIA ALARCON FALCONI}\absaffil{Civil and Environmental Engineering, Tufts University}, \absauthor{BERTHA ESTRELLA}\absaffil{Corporación Ecuatoriana de Biotecnología, Ecuador}, \absauthor{FERNANDO SEMPÉRTEGUI}\absaffil{Central University, Ecuador}, \absauthor{MAGALY KOCH}\absaffil{Center for Remote Sensing, Boston University}, \absauthor{ELENA NAUMOVA}\absaffil{Friedman School of Nutrition Science and Policy, Tufts University}}
        {Characterization of Seasonality in Longitudinal Clinical Trials\newline
        Caractérisation des saisonnalités dans les essais cliniques longitudinaux}

\absSideBySide{The impact of disease prevention interventions can be affected by characteristic temporal patterns in probability and dose of exposures. Lack of proper accounting for such seasonal dynamics can affect observed disease incidence and lead to misinterpretation of intervention results. We characterized seasonality of respiratory and diarrheal infections in a longitudinal clinical trial of supplementation in children in Ecuador using mixed effects models. We accounted for cohort age effects and associated cross-immunity by considering different temporal components and children's age. We also explored temporal effects of climatic and meteorological parameters (temperature, precipitation, UV radiation, photoperiod, cloud cover) on cohort seasonal behavior.
}{L'impact des interventions de prévention des maladies peut être affecté par des motifs temporels caractéristiques au niveau des probabilités et des doses d'exposition. Le manque de bonne comptabilisation de ces dynamiques saisonnières peut affecter l'incidence de la maladie observée et conduire à une mauvaise interprétation des résultats d'intervention. Nous avons caractérisé la saisonnalité des infections respiratoires et diarrhéiques dans un essai clinique longitudinal de la supplémentation chez les enfants en Équateur en utilisant des modèles à effets mixtes. Nous avons tenu compte des effets de l'âge de la cohorte et de l'immunité croisée associée en considérant différentes composantes temporelles et l'âge des enfants. Nous avons également exploré les effets temporels des paramètres climatiques et météorologiques (température, précipitations, rayonnement UV, photopériode, couverture nuageuse) sur le comportement saisonnier de la cohorte.
}}
%% Talk bma2-kmk
\def\whenwhere{[Monday May 30 / lundi 30 mai, 11:05]\\}
{\Author{Khan Mohammad}{Kaviul}{kaviul}{1B-C2}\Author{Rafal}{Kustra}{kustra}{1B-C2}\Author{Zhengfei}{Chen}{chen}{1B-C2}%
\abshead{\absauthor{KHAN MOHAMMAD KAVIUL}, \absauthor{RAFAL KUSTRA} \& \absauthor{ZHENGFEI CHEN}\absaffil{Dalla Lana School of Public Health, University of Toronto}}
        {Investigation of Change in Methylation in Prostate Cancer Patients after Receiving Treatment\newline
        Étude du changement dans la méthylation chez les patients atteints de cancer de la prostate après avoir reçu un traitement}

\absSideBySide{An experimental study was conducted on 33 prostate cancer patients undergoing chemotherapy. The patients were assigned to a treatment. Based on changes in their prostate-specific-antigen (PSA) responder or non-responder to the treatment was declared.  Methylation status from blood sample was collected before treatment and weekly for seven weeks after treatment. We divided the whole genome into 1744 chromosome regions. We tested whether change in methylation status over time in a specific chromosome region varies in responders and non-responders using linear mixed effect model. Since there were 1744 such regions adjustment was done for multiple comparisons.
}{Une étude expérimentale a été réalisée sur 33 patients atteints d'un cancer de la prostate subissant une chimiothérapie. Les patients ont été assignés à un traitement. En fonction des variations de leur antigène prostatique spécifique (PSA), ils ont été déclarés répondants ou non-répondants au traitement. L'état de méthylation de leur échantillon de sang a été prélevé avant le traitement puis chaque semaine pendant les sept semaines suivant le traitement. Nous avons divisé l'ensemble du génome en 1 744 régions chromosomiques. Nous avons vérifié si le changement dans leur statut de méthylation au fil du temps dans une région spécifique du chromosome varie chez les répondants et les non-répondants à l'aide d'un modèle linéaire à effet mixte. Étant donné qu'il y avait 1 744  régions, un ajustement a été fait pour les comparaisons multiples.
}}
%% Talk bma2-vbd
\def\whenwhere{[Monday May 30 / lundi 30 mai, 11:20]\\}
{\Author{Victoria}{Borg-Debono}{borgdebono}{1B-C2}\Author{Philip J.}{Devereaux}{devereaux}{1B-C2}\Author{Lawrence}{Mbuagbaw}{mbuagbaw}{1B-C2}\Author{Lehana}{Thabane}{thabane}{1B-C2}%
\abshead{\absauthor{VICTORIA BORG-DEBONO}, \absauthor{PHILIP J. DEVEREAUX}, \absauthor{LAWRENCE MBUAGBAW} \& \absauthor{LEHANA THABANE}\absaffil{McMaster University}}
        {Interim Data Sharing: What Interim Results/Information Should Be Shared During the Interim of a Randomized Controlled Trial with Those Responsible for the Conduct of the Trial?\newline
        Partage de données intérimaires : quels résultats/information intérimaires devraient être partagés avec ceux qui mènent l'essai lors d'un essai contrôlé randomisé ?}

\absSideBySide{It is unclear what interim trial information a Data Safety Monitoring Board (DSMB) should share with non-DSMB members during a trial's conduct. To understand the opinions of those involved in clinical trials, an online survey targeted at trialists and statisticians was conducted in 2015. The total response rate was 17.8\%. Preliminary results suggest that the majority of respondents believe that the interim combined event rate should be shared (65\% [95\%CI: 60\% to 71\%]) with various parties responsible for a trial's conduct. Further analyses on the reasons for/against sharing interim information will be presented. Low response rate is a limitation.
}{On ne sait pas quelle information d'un essai intérimaire un comité indépendant de contrôle des données (CICD) devrait partager avec les membres non-CICD pendant l'essai. Afin de comprendre les opinions de ceux impliqués dans les essais cliniques, une enquête en ligne ciblant les spécialistes des essais et les statisticiens a été réalisée en 2015. Le taux de réponse total était de 17,8\%. Les résultats préliminaires suggèrent que la majorité des répondants croient que les taux d'événements combinés intérimaire devraient être partagés (65\% [IC à 95\% : 60\% à 71\%]) avec les différentes parties responsables de la mise en œuvre de l'essai. Des analyses plus poussées sur les raisons pour/contre le partage de l'information intérimaire seront présentées. Le faible taux de réponse est une limite.
}}
%% Talk bma2-ss
\def\whenwhere{[Monday May 30 / lundi 30 mai, 11:35]\\}
{\Author{Shahriar}{Shams}{shams}{1B-C2}\Author{Eleanor}{Pullenayegum}{pullenayegum}{1B-C2}%
\abshead{\absauthor{SHAHRIAR SHAMS}\absaffil{University of Toronto}, \absauthor{ELEANOR PULLENAYEGUM}\absaffil{CHES, The Hospital for Sick Children}}
        {Evaluate the Feasibility of Bayesian Inference as a Means of Reducing Scoring Uncertainty for the EQ-5D Health Utilities\newline
        Évaluer la faisabilité de l'inférence bayésienne pour réduire la cotation de l'incertitude pour l'utilité de santé EQ-5D}

\absSideBySide{\noindent
Health utilities are the quality weights used in estimating quality-adjusted life years, and can be measured using the EQ-5D-3L questionnaire which classifies a respondent into one of 243 health states. The health states are converted to utilities through a prediction equation, developed in a valuation study that directly valued 43 health states. Prediction error is large and is currently ignored. Model mis-specification ($\delta$) explains 84\% of the mean squared prediction error. Incorporating the posterior mean of $\delta$ substantially increased the precision of the valued-states. Furthermore, modelling the correlation structure of $\delta's$ may increase the precision of the 200 unvalued-states.
}{Les utilités de santé sont les pondérations des qualités, utilisées dans l'estimation des années de vie ajustée en fonction de la qualité. Elles peuvent être mesurées au moyen du questionnaire EQ-5D-3L qui classifie un répondant dans l'un des 243 états de santé possibles. Les états sont convertis au moyen d'une équation de prédiction développée dans une étude d'évaluation qui a directement valorisé 43 états de santé. L'erreur de prédiction est grande et n'actuellement pas prise en compte. L'erreur de spécification du modèle ($\delta$) explique 84\ \% de l'erreur quadratique moyenne de prédiction. L'intégration de la moyenne a posteriori de $\delta$ permet d'augmenter substantiellement la précision des états évalués. De plus, la modélisation de la structure de corrélation des $\delta$ pourrait augmenter la précision des 200 états non-évalués.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 1B-C3: Statistical Methods Utilizing Copulas\\M\'ethodes statistiques utilisant des copules}
\begin{center}{\large Chair/Président: Zhenhua Lin (University of Toronto)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 256\end{center}
\label{abs-sid:smu}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk smu-ea
\def\whenwhere{[Monday May 30 / lundi 30 mai, 10:20]\\}
{\Author{Elif}{Acar}{acar}{1B-C3}\Author{Candida}{Geerdens}{geerdens}{1B-C3}\Author{Paul}{Janssen}{janssen}{1B-C3}%
\abshead{\absauthor{ELIF ACAR}\absaffil{University of Manitoba}, \absauthor{CANDIDA GEERDENS} \& \absauthor{PAUL JANSSEN}\absaffil{Hasselt University}}
        {Conditional Copula Models for Right-Censored Clustered Event Time Data\newline
        Modèles de copules conditionnelles pour données historiques d'événements groupés censurés à droite}

\absSideBySide{This work proposes a modelling strategy to infer the impact of a covariate on the dependence structure of right-censored clustered event time data. The joint 
survival function of the event times is modelled using a parametric conditional copula whose parameter depends on a cluster-level covariate. We use a local 
likelihood approach to estimate the form of the copula parameter and outline a generalized likelihood ratio-type test strategy to formally test its constancy. We 
apply the methods to data from the Diabetic Retinopathy Study to assess the impact of disease onset age on the loss of visual acuity.
}{Cette étude propose une stratégie de modélisation pour déterminer l'impact d'une covariable sur la structure de dépendance des données historiques d'événements 
groupés censurés à droite. Cette fonction de survie conjointe des temps d'événements est modelée en utilisant une copule conditionnelle paramétrique dont le 
paramètre dépend d'une covariable au niveau de la grappe. Nous utilisons une approche de vraisemblance locale pour estimer la forme du paramètre de la copule et pour 
décrire une stratégie de test du rapport de vraisemblance généralisée pour tester formellement sa constance. Nous appliquons les méthodes à des données provenant 
de l'étude sur la rétinopathie diabétique pour évaluer l'impact de l'âge d'apparition de la maladie sur la perte d'acuité visuelle.
}}
%% Talk smu-tb
\def\whenwhere{[Monday May 30 / lundi 30 mai, 10:35]\\}
{\Author{Tarik}{Bahraoui}{bahraoui}{1B-C3}%
\abshead{\absauthor{TARIK BAHRAOUI}\absaffil{Université de Sherbrooke}}
        {A Family of Goodness-of-fit Tests for Copulas Based on Characteristic Functions\newline
        Une famille de tests d'adéquation pour des copules fondée sur les fonctions caractéristiques}

\absSideBySide{A general class of rank statistics based on the characteristic function is introduced for testing composite goodness-of-fit hypotheses about multivariate copula 
families. These statistics are defined as $L_2$ weighted functional distances between a nonparametric estimator and a semi-parametric estimator of the characteristic 
function associated to a copula. It is shown that these statistics behave asymptotically as a degenerate $V$-statistic of order 4 and that the limit distribution is, 
up to a constant, a weighted sum of independent chi-square random variables. A parametric bootstrap is suggested. At the end, simulation study and illustrated on a 
multivariate dataset
}{Une classe générale des statistiques de rangs fondée sur la fonction caractéristique est présentée pour tester les hypothèses d'adéquation composées sur les familles 
de copules multivariées. Ces statistiques sont définies en tant que distances fonctionnelles pondérées $L_2$ entre un estimateur non-paramétrique et un estimateur 
semiparamétrique de la fonction caractéristique associée à une copule. Il est démontré que ces statistiques se comportent asymptotiquement comme une statistique-$V$ 
dégénérée d'ordre 4 et que la loi limite est, jusqu'à une constante, une somme pondérée de variables aléatoires indépendantes khi carrée. Un bootstrap 
paramétrique est suggéré. En terminant, une étude de simulation est illustrée avec un ensemble de données multivariées.
}}
%% Talk smu-mb
\def\whenwhere{[Monday May 30 / lundi 30 mai, 10:50]\\}
{\Author{Manoj}{Bahuguna}{bahuguna}{1B-C3}\Author{Ravindra}{Khattree}{khattree}{1B-C3}%
\abshead{\absauthor{MANOJ BAHUGUNA}\absaffil{Oakland University, Rochester, MI}, \absauthor{RAVINDRA KHATTREE}\absaffil{Oakland University,Rochester, MI}}
        {Copula Transformation, Prediction and Exploration\newline
        Transformation avec copule, prédiction et exploration}

\absSideBySide{Copulas have been used in various applications in biomedical sciences and finance. In this talk, we suggest copula as a generic all-purpose transformation which can 
enable one to apply various standard procedures more efficiently and with better statistical properties and results. We evaluate and illustrate various applications 
including those in regression, principal component analysis and factor analysis, where analysis using the copula transformation results in improvement in 
implementation, interpretation as well as prediction. Specifically, emphasis is in introducing the multivariate normality using this all purpose transformation.
}{Les copules ont été utilisées dans plusieurs applications des sciences biomédicales et en finance. Dans cet exposé, nous recommandons la copule comme transformation 
générique tout usage qui peut rendre possible l'application plus efficace de différentes procédures standards avec de meilleurs propriétés statistiques et résultats. 
Nous évaluons et illustrons diverses applications y compris celles en régression, en analyse de composante principale et en analyse factorielle, où l'analyse utilisant 
la transformation avec copule a pour résultat l'amélioration de l'exécution, de l'interprétation ainsi que de la prédiction. Spécifiquement, l'accent est mis sur la 
présentation de la normalité en utilisant cette transformation tout usage.
}}
%% Talk smu-ch
\def\whenwhere{[Monday May 30 / lundi 30 mai, 11:05]\\}
{\Author{Caren}{Hasler}{hasler}{1B-C3}\Author{Radu V.}{Craiu}{craiu}{1B-C3}\Author{Louis-Paul}{Rivest}{rivest}{1B-C3}%
\abshead{\absauthor{CAREN HASLER} \& \absauthor{RADU V. CRAIU}\absaffil{University of Toronto}, \absauthor{LOUIS-PAUL RIVEST}\absaffil{Université Laval}}
        {Vine Copulas for Imputation of Monotone Non-Response\newline
        Copules en vignes pour l'imputation de la non-réponse monotone}

\absSideBySide{We investigate the use of vines to impute for monotone non-response. We use vine copulas and factorize the density of the observed variables into a cascade of bivariate copulas to flexibly model their joint distribution. The structure of the vine depends on the non-response pattern. We build on the work of Aas et al. (2009) and propose a method to estimate the parameters of the bivariate copulas entering into the model. Imputations are carried out by simulating the missing values using the constructed model. We discuss the generalization of our results to more global non-response patterns. Project supported by CANSSI.
}{Nous explorons l'application des copules en vignes pour l'imputation de la non-réponse monotone. Nous considérons des copules en vignes et factorisons la densité des variables observées en une cascade de copules par paire afin de modéliser leur distribution jointe avec une grande flexibilité. Nous nous basons sur les travaux de Aas et al. (2009) et proposons une méthode d'estimation des paramètres du modèle. L'imputation se fait par simulation des observations manquantes au moyen du modèle construit. Nous discutons l'extension de notre travail à des répartitions de données manquantes plus générales. Projet financé par l'INCASS.
}}
%% Talk smu-dl
\def\whenwhere{[Monday May 30 / lundi 30 mai, 11:20]\\}
{\Author{David}{Lee}{lee}{1B-C3}\Author{Harry}{Joe}{joe}{1B-C3}%
\abshead{\absauthor{DAVID LEE} \& \absauthor{HARRY JOE}\absaffil{University of British Columbia}}
        {Multivariate Extreme Value Copulas with Factor and Tree Dependence Structures\newline
        Copules de valeurs extrêmes multivariées avec structures factorielles et arborescentes de dépendance}

\absSideBySide{In multivariate modelling, factor and tree dependence structures are parsimonious assumptions often reasonable in practice, for example in finance where stock prices are driven by common latent factors, or in spatial applications where stations form a tree network. We propose copula extensions for extreme observations that incorporate such structures. These models allow intuitive interpretation of dependence relationships underlying the processes that generate the extremes; this is illustrated through a data example, which also suggests that our proposed models may perform better than a fully unconstrained one.
}{Dans la modélisation multivariée, les structures factorielles et arborescentes de dépendance sont des hypothèses parcimonieuses souvent raisonnables dans la pratique comme dans le domaine de la finance où les cours de l'action sont dirigés par des facteurs latents communs, ou bien dans des applications spatiales où les stations forment un réseau arborescent. Nous proposons des extensions de copules d'observations extrêmes qui incorporent de telles structures. Ces modèles permettent une interprétation intuitive de la relation de dépendance sous-jacente aux processus qui génèrent les extrêmes. Nous donnons un exemple de données qui suggère également que les modèles que nous proposons sont plus efficaces qu'un modèle sans contrainte.
}}
%% Talk smu-yr
\def\whenwhere{[Monday May 30 / lundi 30 mai, 11:35]\\}
{\Author{Yassir}{Rabhi}{rabhi}{1B-C3}\Author{Taoufik}{Bouezmarni}{bouezmarni}{1B-C3}%
\abshead{\absauthor{YASSIR RABHI} \& \absauthor{TAOUFIK BOUEZMARNI}\absaffil{Université de Sherbrooke}}
        {Copula Function Under Biased Sampling and Informative Censoring\newline
        Fonction de la copule dans le cadre d'un échantillonnage biaisé et d'une censure informative}

\absSideBySide{In observational studies, incidence cohort sampling is ideally adopted to study individuals, who have not experienced a disease, from disease onset to a failure event. Logistic or other constraints may however preclude the possibility of recruiting incident cases. A feasible alternative in such circumstances is to sample subjects who have already experienced the disease onset, through cross-sectional sampling. In this presentation, we discuss the estimation of the copula function for right-censored length-biased data. Copula function is known for its use in modeling the dependence structure between two variables. We introduce a nonparametric estimation method for the copula and its functionals.
}{Dans le cadre des études d'observation, on adopte idéalement l'échantillonnage de l'incidence des cohortes pour étudier les individus qui n'ont pas connu de maladie, à partir du début de la maladie jusqu'à un évènement négatif déterminé. Certaines contraintes, d'ordre logistiques ou autres, peuvent cependant empêcher de recruter des cas d'incidents. Une alternative possible, dans de telles circonstances, est d'échantillonner les sujets qui montrent les premières manifestations de la maladie au moyen d'un échantillonnage transversal. Dans cet exposé, nous discutons de l'estimation de la fonction de la copule des données censurées à droite de longueur biaisée. La fonction de la copule est connue pour son utilisation dans la modélisation de la structure de dépendance entre deux variables. Nous présentons une méthode d'estimation non paramétrique pour la copule et ses fonctions.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 1B-C4: Statistical Methods and Applications 1\\M\'ethodes statistiques et applications 1}
\begin{center}{\large Chair/Président: David Spiegelhalter (Cambridge University)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Welch 8G\end{center}
\label{abs-sid:sma3}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk sma3-jc
\def\whenwhere{[Monday May 30 / lundi 30 mai, 10:20]\\}
{\Author{Jingjia}{Chu}{chu}{1B-C4}\Author{Reg}{Kulperger}{kulperger}{1B-C4}\Author{Hao}{Yu}{yu}{1B-C4}%
\abshead{\absauthor{JINGJIA CHU}, \absauthor{REG KULPERGER} \& \absauthor{HAO YU}\absaffil{University of Western Ontario}}
        {A Multivariate Time Series Model with an Additive GARCH Type Structure\newline
        Un modèle de séries temporelles multivarié avec une structure de type GARCH additif}

\absSideBySide{A new class of multivariate time series model with an additive GARCH type structure was proposed, which could be used to capture the common risk among equities. The dynamic conditional covariances between series are aggregated by a common risk term, which has been the key to characterize the conditional correlation. The model ergodicity and stationarity were proved as well as the identifiability theorem in terms of the second moment. A Monte Carlo simulation example will be shown in the talk.
}{Une nouvelle classe de modèles de séries temporelles multivariés avec une structure de type GARCH additif, qui pourrait être utilisée pour capturer le risque commun des actions, a été proposée. Les covariances conditionnelles dynamiques entre les séries sont agrégées par un terme de risque commun, qui est la clé pour caractériser la corrélation conditionnelle. L'ergodicité et la stationnarité du modèle ont été prouvées ainsi que le théorème d'identification en termes du deuxième moment. Un exemple de simulation de Monte-Carlo sera montré dans la présentation.
}}
%% Talk sma3-rh
\def\whenwhere{[Monday May 30 / lundi 30 mai, 10:35]\\}
{\Author{Reyhaneh}{Hosseini}{hosseini}{1B-C4}\Author{Mahmoud}{Zarepour}{zarepour}{1B-C4}%
\abshead{\absauthor{REYHANEH HOSSEINI} \& \absauthor{MAHMOUD ZAREPOUR}\absaffil{University of Ottawa}}
        {A Bayesian Nonparametric Chi-squared Goodness-of-fit Test\newline
        Un test d'ajustement du khi-deux bayésien non paramétrique}

\absSideBySide{The Bayesian nonparametric inference and Dirichlet process are popular tools
in statistical methodologies. In this work, we employ the Dirichlet process in
hypothesis testing to propose a Bayesian nonparametric chi-squared goodness-of-fit test. In our Bayesian nonparametric approach, we consider the Dirichlet
process as the prior for the distribution of data and carry out the test based
on the Kullback-Leibler distance between the updated Dirichlet process and the
hypothesized distribution. We prove that this distance asymptotically converges
to the same chi-squared distribution as the classical test does. Similarly, a
Bayesian nonparametric chi-squared test of independence for a contingency table
is provided.
}{L'inférence bayésienne non paramétrique et le processus de Dirichlet sont des outils populaires dans les méthodes statistiques. Dans ce travail, nous employons le processus de Dirichlet dans un test d'hypothèse afin de proposer un test d'ajustement du khi-deux bayésien non paramétrique. Dans notre approche bayésienne non paramétrique, nous considérons le processus de Dirichlet en tant que préalable à la distribution des données et effectuons le test selon la distance de Kullback-Leibler entre le processus de Dirichlet mis à jour et la distribution hypothétique. Nous montrons que cette distance converge asymptotiquement vers la même distribution du khi-deux que le test classique. De même, un test du khi-deux bayésien non paramétrique de l'indépendance pour un tableau de contingence est fourni.
}}
%% Talk sma3-agn
\def\whenwhere{[Monday May 30 / lundi 30 mai, 10:50]\\}
{\Author{Aurélien Guetsop}{Nangue}{nangue}{1B-C4}\Author{Martin}{Bilodeau}{bilodeau}{1B-C4}%
\abshead{\absauthor{AURÉLIEN GUETSOP NANGUE} \& \absauthor{MARTIN BILODEAU}\absaffil{Université de Montréal}}
        {Approximations to Permutation Tests of Independence Between Two Random Vectors\newline
        Approximation du test de permutation d'indépendance entre deux vecteurs aléatoires}

\absSideBySide{The main result establishes the equivalence of power between the $\alpha$-distance covariance and the Hilbert-Schmidt independence criterion ($HSIC$) tests with the characteristic kernel of a stable probability distribution of index $\alpha$ with a sufficiently small bandwidth. Large-scale simulations reveal the superiority of these tests over other tests using copula. The Pearson type III approximation to the exact permutation distribution yields tests with more accurate type I error rates than the gamma approximation used for $HSIC$. A new method for bandwidth selection in $HSIC$ tests is proposed which improves power  in three simulations, two of which are from machine learning.
}{Le travail établit une équivalence de puissance entre tests basés sur la alpha-distance de covariance et sur le critère d'indépendance d'Hilbert-Schmidt ($HSIC$) avec fonction caractéristique de distribution de probabilité stable d'indice alpha avec paramètre d'échelle suffisamment petit. Les simulations en grandes dimensions montrent la supériorité de ces tests en comparaison à certains tests utilisant les copules. La distribution de Pearson de type III approche la distribution exacte de permutation des tests et donne des erreurs de type I plus précises que l'approximation gamma pour $HSIC$. Des simulations empruntées de l'apprentissage machine montrent l'amélioration de la puissance par notre nouvelle méthode adaptative.
}}
%% Talk sma3-an
\def\whenwhere{[Monday May 30 / lundi 30 mai, 11:05]\\}
{\Author{Aurélien}{Nicosia}{nicosia}{1B-C4}\Author{Thierry}{Duchesne}{duchesne}{1B-C4}\Author{Louis-Paul}{Rivest}{rivest}{1B-C4}\Author{Daniel}{Fortin}{fortin}{1B-C4}%
\abshead{\absauthor{AURÉLIEN NICOSIA}, \absauthor{THIERRY DUCHESNE}, \absauthor{LOUIS-PAUL RIVEST} \& \absauthor{DANIEL FORTIN}\absaffil{Université Laval}}
        {Equivalence Between the Random Walk Model in the Plane and Conditional Logistic Regression in a Multi-State Framework, with Application to the Analysis of Animal Movement\newline
        Équivalence entre un modèle de marche aléatoire dans le plan et une régression logistique conditionnelle dans un cadre multi-états, application à l'analyse du mouvement animalier}

\absSideBySide{In this talk, we introduce a multi-state version of conditional logistic regression that compares the locations that an animal has chosen with randomly sampled locations. The impacts of different sampling procedures are discussed. A hidden process structure enables the modeling of situations where the animal exhibits various choice behaviors. We prove its equivalence with a random walk model in the plane, in which the directional part of the process is an angular regression model. We illustrate the use of the methods by modeling the movement of an animal in Prince Albert National Park (Saskatchewan, Canada).
}{Dans cette présentation, nous introduisons une version multi-états d'un modèle de régression logistique conditionnelle. Le modèle de régression caractérise quelle localisation un animal choisit face à des localisations échantillonnées au hasard. L'impact des différentes procédures d'échantillonnage est discuté.  Une structure de processus cachée permet de modéliser des situations où l'animal manifeste des comportements différents. Nous prouvons son équivalence avec un modèle de marche aléatoire dans le plan, où la partie directionnelle du processus est un modèle de régression angulaire. Nous illustrons son utilisation en modélisant le mouvement d'un animal dans le parc national de Prince Albert (Saskatchewan, Canada).
}}
%% Talk sma3-hz
\def\whenwhere{[Monday May 30 / lundi 30 mai, 11:20]\\}
{\Author{Hossein}{Zareamoghaddam}{zareamoghaddam}{1B-C4}\Author{Serge}{Provost}{provost}{1B-C4}\Author{Syed Ejaz}{Ahmed}{ahmed}{1B-C4}%
\abshead{\absauthor{HOSSEIN ZAREAMOGHADDAM} \& \absauthor{SERGE PROVOST}\absaffil{Western University}, \absauthor{SYED EJAZ AHMED}\absaffil{Brock University}}
        {A Moment-Based Bivariate Density Estimation Methodology for Large Data Sets\newline
        Une méthodologie basée sur les moments pour l'estimation de densités bivariées pour grands ensembles de données}

\absSideBySide{We propose a moment-based methodology for approximating the density function associated with a bivariate distribution, which consists in applying an adjustment involving orthogonal polynomials to an initial density approximation. The use of standard polynomials is then shown to be mathematically equivalent. As well, the proposed technique is extended to the estimation of bivariate density functions, in which case joint sample moments are being utilized. It will be explained that this approach is ideally suited to model `big data', which will be illustrated by applying it to large data sets. Generalizations to multivariate distributions shall also be discussed.
}{Nous proposons une méthodologie basée sur les moments pour l'approximation de la fonction de densité associée à une distribution bivariée, consistant à appliquer un ajustement utilisant des polynômes orthogonaux à une approximation initiale de la densité. L'utilisation de polynômes standards est alors montrée comme un équivalent mathématique. De même, la technique proposée est étendue à l'estimation des fonctions de densité bivariées et dans ce cas, les moments conjoints échantillonnaux sont utilisés. Nous verrons que cette approche convient idéalement à la modélisation des «~données massives~», ce qui sera illustré par son application à de grands ensembles de données. Nous traiterons aussi de généralisations à des distributions multivariées.
}}
%% Talk sma3-bz
\def\whenwhere{[Monday May 30 / lundi 30 mai, 11:35]\\}
{\Author{Bangxin}{Zhao}{zhao}{1B-C4}\Author{Wenqing}{He}{he}{1B-C4}%
\abshead{\absauthor{BANGXIN ZHAO} \& \absauthor{WENQING HE}\absaffil{Western University}}
        {Distance Techniques for High-dimensional Influence Measure\newline
        Techniques de distance pour mesure d'influence à dimensions élevées}

\absSideBySide{In this talk, a new statistic based on Hellinger Distance is developed to evaluate the influence of individual observation on high-dimensional data. We first give the rigorous definition of this statistic which itself is a function of probability mass functions based on marginal correlations, and then explain how this statistic accurately evaluates the influence of individual observations. The asymptotic distribution of the proposed statistic can be found by setting the dimension of the explanatory variable to approach infinity, which shows inference can also be conducted. Simulation and real data analysis are given as illustration of the usefulness of the proposed method.
}{Dans cet exposé, une nouvelle statistique fondée sur la distance de Hellinger est élaborée pour évaluer l'influence de l'observation individuelle sur des données à dimensions élevées. Après avoir rigoureusement défini cette statistique qui est en soi une fonction des fonctions de masse basées sur les corrélations marginales, nous expliquerons ensuite de quelle façon elle évalue avec exactitude l'influence des observations individuelles. On peut dériver la distribution asymptotique de la statistique proposée en posant vers l'infini la dimension des variables explicatives, ce qui montre la possibilité d'une inférence. La simulation et l'analyse de données réelles illustrent l'utilité de la méthode proposée.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 1B-C5: Survey Methodology 1\\M\'ethodologie d'enqu\^ete 1}
\begin{center}{\large Chair/Président: John L. Eltinge (U.S. Bureau of Labor Statistics)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 257\end{center}
\label{abs-sid:sm}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk sm-ja
\def\whenwhere{[Monday May 30 / lundi 30 mai, 10:20]\\}
{\Author{Jessica}{Andrews}{andrews}{1B-C5}\Author{Pierre}{Daoust}{daoust}{1B-C5}\Author{Matei}{Mireuta}{mireuta}{1B-C5}%
\abshead{\absauthor{JESSICA ANDREWS}, \absauthor{PIERRE DAOUST} \& \absauthor{MATEI MIREUTA}\absaffil{Statistics Canada}}
        {Small Domains and Sparse Variables;  Challenges with Creating an Active Collection Strategy\newline
        Petits domaines et variables rares; défis lors de la création d'une stratégie de collection active}

\absSideBySide{Statistics Canada has recently put into place the Integrated Business Statistics Program (IBSP) as a common framework to process its business surveys. One aspect of 
this new system has been to introduce an active collection strategy   to improve efficiency, data quality and lower costs.  The Quality Indicator and Measure of 
Impact (QIMI) collection tool was built to facilitate this. There have, however, been a number of challenges in turning a theoretical tool into a practical system.  
In this paper we explore some of the challenges faced including the impact of misreported values, small domains and sparsely reported variables.
}{Statistique Canada a récemment mis en place le programme intégré de la statistique des entreprises (IBSP) comme cadre commun pour traiter ses enquêtes auprès des 
entreprises. Un aspect de ce nouveau système a été l'introduction d'une stratégie de collection active pour améliorer l'efficacité, la qualité des données et pour réduire 
les coûts. Pour faciliter ceci, l'outil de collection Indicateur de qualité et mesure de l'impact (QIMI) a été développé. Par contre, il y a eu plusieurs défis lors 
de la transformation d'un outil théorique en un système pratique. Dans cet article, nous explorons quelques-uns de ces défis dont l'impact des valeurs erronées, des 
petits domaines et des variables rarement rapportées.
}}
%% Talk sm-sd
\def\whenwhere{[Monday May 30 / lundi 30 mai, 10:35]\\}
{\Author{Susan}{Demedash}{demedash}{1B-C5}\Author{Matthew}{Buckley}{buckley}{1B-C5}%
\abshead{\absauthor{SUSAN DEMEDASH}\absaffil{Statistics Canada}, \absauthor{MATTHEW BUCKLEY}\absaffil{Wilfred Laurier University}}
        {Job Vacancy and Wage Survey Paradata Log Files: A New Realm of Respondent Behavioural Analysis\newline
        Les fichiers de paradonnées de l'Enquête sur les postes vacants et les salaires : un nouveau monde d'analyse comportementale des répondants}

\absSideBySide{The Job Vacancy and Wage Survey collects information from Canadian employers on a number of labour issues, such as job vacancies and hourly wages by occupation.  Started in February 2015, this survey is Statistics Canada's largest business survey with 100,000 business locations surveyed quarterly. The survey is conducted via electronic questionnaire and underlying paradata information on the communication from the respondent's computer to an internal host server was deciphered and analysed. This paper presents paradata results of the early quarters of this survey, which can help improve our questionnaire and reduce response burden.
}{L'Enquête sur les postes vacants et les salaires recueille de l'information reliée au travail auprès des employeurs canadiens.  L'enquête, qui a débuté en février 2015, est la plus vaste enquête de Statistique Canada auprès des entreprises, avec 100 000 emplacements échantillonnés trimestriellement.  L'information est recueillie par questionnaire électronique et les paradonnées sous-jacentes résultant des communications entre les ordinateurs des répondants et nos serveurs internes ont été déchiffrées et étudiées.  L'analyse des paradonnées permet d'améliorer le questionnaire et de réduire le fardeau des répondants.  Cet article présente certains résultats de cette analyse pour les premiers trimestres de l'enquête.
}}
%% Talk sm-mf
\def\whenwhere{[Monday May 30 / lundi 30 mai, 10:50]\\}
{\Author{Michael}{Freiman}{freiman}{1B-C5}\Author{Amy}{Lauger}{lauger}{1B-C5}\Author{Jerome}{Reiter}{reiter}{1B-C5}%
\abshead{\absauthor{MICHAEL FREIMAN} \& \absauthor{AMY LAUGER}\absaffil{U.S. Census Bureau}, \absauthor{JEROME REITER}\absaffil{Duke University}}
        {Two Approaches to Disclosure Avoidance for a Microdata Analysis System at the U.S. Census Bureau\newline
        Deux approches pour prévenir la divulgation dans un système d'analyse des microdonnées au U.S. Census Bureau}

\absSideBySide{The U.S. Census Bureau is developing an online Microdata Analysis System where users request a table or other analysis of Census Bureau data and receive the results without seeing the underlying microdata. We compare two approaches to maintaining the confidentiality of the data: output suppression, where results are withheld if they are deemed too risky, and pre-tabulation perturbation, where the underlying microdata are modified before the analysis is performed. Our research shows severe limitations to a suppression approach, with adequate protection requiring an unreasonably large number of tables to be suppressed. Ongoing research on pre-tabulation methods is discussed.
}{Le U.S. Census Bureau développe un système d'analyse des microdonnées en ligne où les utilisateurs demandent un tableau ou une autre analyse des données du Census Bureau et reçoivent les résultats sans voir les microdonnées sous-jacentes. Nous comparons deux approches pour maintenir la confidentialité des données : suppression de sortie, où les résultats sont retenus s'ils sont jugés trop risqués, et perturbation des pré-tabulations, où les microdonnées sous-jacentes sont modifiées avant l'analyse. Notre recherche montre des limites importantes à une approche de suppression, avec une protection adéquate nécessitant un nombre déraisonnablement grand de tableaux à supprimer. Nous discutons les recherches en cours sur les méthodes de pré-tabulation.
}}
%% Talk sm-zn
\def\whenwhere{[Monday May 30 / lundi 30 mai, 11:05]\\}
{\Author{Zixin}{Nie}{nie}{1B-C5}\Author{Kyle}{Vincent}{vincent}{1B-C5}\Author{Andee}{Cooper-Parks}{cooperparks}{1B-C5}%
\abshead{\absauthor{ZIXIN NIE}, \absauthor{KYLE VINCENT} \& \absauthor{ANDEE COOPER-PARKS}\absaffil{International Justice Mission}}
        {Methodological Challenges Faced in the 2015-2016 Kolkata/Mumbai Commercial Sex Worker Study\newline
        Problèmes méthodologiques rencontrés dans l'étude 2015-2016 sur les travailleurs de l'industrie du sexe à Calcutta et à Mumbai}

\absSideBySide{International Justice Mission (IJM) conducted a study in 2015-2016 to measure the prevalence of minors working in the commercial sex industry of Kolkata and Mumbai, India. To sell minors for sex is illegal, and therefore the population is hidden and transient, resulting in several methodological challenges. We present the methods used to address and account for such issues, along with the results based on the resulting inference procedure, and we discuss how to address these limitations for future studies.
}{La Mission Internationale De Justice a mené une étude en 2015-2016 afin de mesurer la prévalence des mineurs travaillant dans l'industrie du sexe à Calcutta et à Mumbai, en Inde. La vente de services sexuels par des mineurs est illégale. Ainsi la population est cachée et isolée, ce qui engendre plusieurs problèmes méthodologiques. Nous présentons les méthodes utilisées pour aborder et prendre en compte ces problèmes, et nous présentons les résultats basés sur la procédure d'inférence qui en découle. Nous abordons également la façon dont on peut remédier à ces limitations dans les futures études.
}}
%% Talk sm-jo
\def\whenwhere{[Monday May 30 / lundi 30 mai, 11:20]\\}
{\Author{Javier}{Oyarzun}{oyarzun}{1B-C5}%
\abshead{\absauthor{JAVIER OYARZUN}\absaffil{Statistics Canada}}
        {Needle in a Haystack – How to Detect Break in Series: The Capital Expenditures Survey Integrated Business Statistics Program Experience\newline
        Une aiguille dans une botte de foin – La façon de détecter un bris dans une série : l'expérience de l'Enquête sur les dépenses en immobilisations dans le Programme intégré de la statistique des entreprises}

\absSideBySide{Statistics Canada's Capital Expenditures Survey (CES), which collects data on capital and repair expenditures, was integrated into the Integrated Business Statistics Program (IBSP) for 2013. The transition from a stand-alone survey to the CES-IBSP survey introduced several methodological changes that could have potentially introduced a break in annual estimates between 2012 (CES-based) and 2013 (IBSP-based). Using SAS High-Performance Forecasting and along with the Time Series Research and Analysis Centre, a modeling approach was developed to determine whether a break in the series exists. An overview of the methodology used and results using the published 2013 estimates will be presented.
}{L'Enquête sur les dépenses en immobilisations (EDI) de Statistique Canada, qui recueille des données sur les dépenses en immobilisations et réparations, a été intégrée dans le Programme intégré de la statistique des entreprises (PISE) en 2013. La transition d'une enquête indépendante à l'enquête EDI-PISE a entraîné plusieurs changements méthodologiques qui ont pu introduire un bris dans les estimations annuelles entre 2012 (estimations de l'EDI) et 2013 (estimations du PISE). En utilisant «SAS High-Performance Forecasting» et en partenariat avec le Centre de recherche et d'analyse en séries chronologiques, une approche de modélisation a été élaborée afin de déterminer si un bris dans la série existe. Un aperçu de la méthodologie utilisée ainsi que les résultats obtenus en utilisant les estimations publiées de 2013 seront présentés.
}}
%% Talk sm-qw
\def\whenwhere{[Monday May 30 / lundi 30 mai, 11:35]\\}
{\Author{Qian}{Wei}{wei}{1B-C5}\Author{J.N.K.}{Rao}{rao}{1B-C5}%
\abshead{\absauthor{QIAN WEI} \& \absauthor{J.N.K. RAO}\absaffil{Carleton University}}
        {Statistical Inference Based on Survey Weighted Composite Likelihood\newline
        Inférence statistique fondée sur la vraisemblance composite pondérée d'un sondage}

\absSideBySide{Survey weighted composite likelihood (WCL) method (Rao et al. 2013) has been proposed to estimate multilevel model parameters when data are from a complex survey with a multistage sample design. In this paper, we study inferential properties of the WCL method, such as the asymptotic normality of the maximum WCL estimators and likelihood ratio tests. Simulation studies are conducted to evaluate the performance of the proposed test statistics.\\
\\
Key Words: Composite Likelihood, Multilevel Models, Multi-stage Survey Data
}{La méthode de la vraisemblance composite pondérée (VCP) d'un sondage (Rao et coll. 2013) a été proposée pour estimer les paramètres de modèles multiniveaux lorsque les données proviennent d'un sondage complexe avec un plan d'échantillonnage à plusieurs étapes.  Dans cet article, nous étudions les propriétés inférentielles de la méthode VCP, comme la normalité asymptotique des estimateurs VCP maximaux et les tests du rapport des vraisemblances. Des études de simulation sont menées pour évaluer la performance des statistiques de test proposées.\\
\\
Mots-clés~: vraisemblance composite, modèles multiniveaux, données de sondage à plusieurs étapes
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 1C-D1: Case Study 1: Can Google Flu Trends Predict the Frequency and Results of Tests for Influenza and Other Respiratory Illnesses?\\\'Etude de cas  1: Le site Google Flu Trends peut-il pr\'edire la fr\'equence et le r\'esultat des tests pour la grippe et autres maladies respiratoires ?}
\vspace*{5pt}
\begin{center}{\large Organizer and Chair / Responsable et président:  Lisa Lix (University of Manitoba)}
\end{center}
\par (Posters displayed 12:00-17:30. Presenters in attendance 13:30-15:30)\par (Les affiches seront exposées de 12 h à 17 h 30. Les auteurs seront présents de 13 h 30 à 15 h 30.)\vspace{3pt}

\begin{center}\large\bfseries Room/Salle: Thistle Corridor\end{center}
\label{abs-sid:cgf}
\begin{center}{\large\bfseries Abstract/R\'esum\'e}
\end{center}
%% Talk cgf-saswzo
\def\whenwhere{[Monday May 30 / lundi 30 mai, 12:00]\\}
{\Author{Shomoita}{Alam}{alam}{1C-D1}\Author{Shouao}{Wang}{wang}{1C-D1}\Author{Zayd}{Omar}{omar}{1C-D1}%
\medskip\par\textbf{SHOMOITA ALAM, SHOUAO WANG \& ZAYD OMAR} (McGill University)
}
%% Talk cgf-sbmehfaofa
\def\whenwhere{[Monday May 30 / lundi 30 mai, 12:00]\\}
{\Author{Shabnam}{Balamchi}{balamchi}{1C-D1}\Author{Md. Erfanul}{Hoque}{hoque}{1C-D1}\Author{Faisal}{Atakora}{atakora}{1C-D1}\Author{Olawale Fatai}{Ayilara}{ayilara}{1C-D1}%
\medskip\par\textbf{SHABNAM BALAMCHI, MD. ERFANUL HOQUE, FAISAL ATAKORA \& OLAWALE FATAI AYILARA} (University of Manitoba)
}
%% Talk cgf-zhdg
\def\whenwhere{[Monday May 30 / lundi 30 mai, 12:00]\\}
{\Author{Zhoushanyue}{He}{he}{1C-D1}\Author{Danqiao}{Guo}{guo}{1C-D1}%
\medskip\par\textbf{ZHOUSHANYUE HE \& DANQIAO GUO} (University of Waterloo)
}
%% Talk cgf-yltjss
\def\whenwhere{[Monday May 30 / lundi 30 mai, 12:00]\\}
{\Author{Yuanhao}{Lai}{lai}{1C-D1}\Author{Tianpei}{Jiang}{jiang}{1C-D1}\Author{Samira}{Soleymani}{soleymani}{1C-D1}%
\medskip\par\textbf{YUANHAO LAI, TIANPEI JIANG \& SAMIRA SOLEYMANI} (University of Western Ontario)
}
%% Talk cgf-wlhhfmryyz
\def\whenwhere{[Monday May 30 / lundi 30 mai, 12:00]\\}
{\Author{Wei}{Lin}{lin}{1C-D1}\Author{Hongyang}{Hu}{hu}{1C-D1}\Author{Faizan}{Moshin}{moshin}{1C-D1}\Author{Ruoqi}{Yu}{yu}{1C-D1}\Author{Yuxin}{Zou}{zou}{1C-D1}%
\medskip\par\textbf{WEI LIN, HONGYANG HU, FAIZAN MOSHIN, RUOQI YU \& YUXIN ZOU} (University of Toronto)
}
%% Talk cgf-klmmkmksk
\def\whenwhere{[Monday May 30 / lundi 30 mai, 12:00]\\}
{\Author{Kuan}{Liu}{liu}{1C-D1}\Author{Michael}{Moon}{moon}{1C-D1}\Author{Kaviul Mohammad}{Khan}{khan}{1C-D1}\Author{Sangook}{Kim}{kim}{1C-D1}%
\medskip\par\textbf{KUAN LIU, MICHAEL MOON, KAVIUL MOHAMMAD KHAN \& SANGOOK KIM} (University of Toronto)
}
%% Talk cgf-llxzyggl
\def\whenwhere{[Monday May 30 / lundi 30 mai, 12:00]\\}
{\Author{Lin}{Lu}{lu}{1C-D1}\Author{Xu}{Zhao}{zhao}{1C-D1}\Author{Yishan}{Guo}{guo}{1C-D1}\Author{Gemini}{Lam}{lam}{1C-D1}%
\medskip\par\textbf{LIN LU, XU ZHAO, YISHAN GUO \& GEMINI LAM} (University of Toronto)
}
%% Talk cgf-dthlbfss
\def\whenwhere{[Monday May 30 / lundi 30 mai, 12:00]\\}
{\Author{Danny}{Tshitumbu}{tshitumbu}{1C-D1}\Author{Hang}{Lai}{lai}{1C-D1}\Author{Bisma}{Farooqi}{farooqi}{1C-D1}\Author{Suj}{Sriskandarajah}{sriskandarajah}{1C-D1}%
\medskip\par\textbf{DANNY TSHITUMBU, HANG LAI, BISMA FAROOQI \& SUJ SRISKANDARAJAH} (York University)
}
%% Talk cgf-fywz
\def\whenwhere{[Monday May 30 / lundi 30 mai, 12:00]\\}
{\Author{Fahmida}{Yeasmin}{yeasmin}{1C-D1}\Author{Wenyan}{Zhong}{zhong}{1C-D1}%
\medskip\par\textbf{FAHMIDA YEASMIN \& WENYAN ZHONG} (University of Calgary)
}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 1C-D2: Case Study 2: What Predicts Sustainability of Canadian Charities?\\\'Etude de cas 2:  Pouvons-nous pr\'edire la viabilit\'e des organismes de bienfaisance canadiens ?}
\begin{center}{\large Organizer and Chair / Responsable et président:  Lisa Lix (University of Manitoba)}
\end{center}
\par (Posters displayed 12:00-17:30. Presenters in attendance 13:30-15:30)\par (Les affiches seront exposées de 12 h à 17 h 30. Les auteurs seront présents de 13 h 30 à 15 h 30.)\vspace{3pt}

\begin{center}\large\bfseries Room/Salle: Thistle Corridor\end{center}
\label{abs-sid:wps}
\begin{center}{\large\bfseries Abstract/R\'esum\'e}
\end{center}
%% Talk wps-ibslmgjr
\def\whenwhere{[Monday May 30 / lundi 30 mai, 12:00]\\}
{\Author{Isabelle}{Bernard}{bernard}{1C-D2}\Author{Samuel}{Labelle}{labelle}{1C-D2}\Author{Maikel}{Geagea}{geagea}{1C-D2}\Author{Jorge-Enrique}{Ruiz-Trujillo}{ruiztrujillo}{1C-D2}%
\medskip\par\textbf{ISABELLE BERNARD, SAMUEL LABELLE, MAIKEL GEAGEA \& JORGE-ENRIQUE RUIZ-TRUJILLO} (HEC Montréal)
}
%% Talk wps-bchdph
\def\whenwhere{[Monday May 30 / lundi 30 mai, 12:00]\\}
{\Author{Bo}{Chen}{chen}{1C-D2}\Author{Haosui}{Duanmu}{duanmu}{1C-D2}\Author{Peiyang}{He}{he}{1C-D2}%
\medskip\par\textbf{BO CHEN, HAOSUI DUANMU \& PEIYANG HE} (University of Toronto)
}
%% Talk wps-hhxwjmhbjpylbm
\def\whenwhere{[Monday May 30 / lundi 30 mai, 12:00]\\}
{\Author{Huang}{Huang}{huang}{1C-D2}\Author{Xiaoxiao}{Wang}{wang}{1C-D2}\Author{Jin}{Ma}{ma}{1C-D2}\Author{He}{Bian}{bian}{1C-D2}\Author{Jingchun}{Pei}{pei}{1C-D2}\Author{Yisen}{Lin}{lin}{1C-D2}\Author{Banoo}{Madhanagopal}{madhanagopal}{1C-D2}%
\medskip\par\textbf{HUANG HUANG, XIAOXIAO WANG, JIN MA, HE BIAN, JINGCHUN PEI, YISEN LIN \& BANOO MADHANAGOPAL} (University of Toronto)
}
%% Talk wps-wlhhfmryyz
\def\whenwhere{[Monday May 30 / lundi 30 mai, 12:00]\\}
{\Author{Wei}{Lin}{lin}{1C-D2}\Author{Hongyang}{Hu}{hu}{1C-D2}\Author{Faizan}{Moshin}{moshin}{1C-D2}\Author{Ruoqi}{Yu}{yu}{1C-D2}\Author{Yuxin}{Zou}{zou}{1C-D2}%
\medskip\par\textbf{WEI LIN, HONGYANG HU, FAIZAN MOSHIN, RUOQI YU \& YUXIN ZOU} (University of Toronto)
}
%% Talk wps-ttkl
\def\whenwhere{[Monday May 30 / lundi 30 mai, 12:00]\\}
{\Author{Tingbin}{Tan}{tan}{1C-D2}\Author{Kexin}{Luo}{luo}{1C-D2}%
\medskip\par\textbf{TINGBIN TAN \& KEXIN LUO} (University of Western Ontario)
}
%% Talk wps-ytygjgmlgzhq
\def\whenwhere{[Monday May 30 / lundi 30 mai, 12:00]\\}
{\Author{Yanbo}{Tang}{tang}{1C-D2}\Author{Yang Guan Jian}{Guo}{guo}{1C-D2}\Author{Mufan}{Li}{li}{1C-D2}\Author{Gong}{Zhang}{zhang}{1C-D2}\Author{Harris}{Quach}{quach}{1C-D2}%
\medskip\par\textbf{YANBO TANG, YANG GUAN JIAN GUO, MUFAN LI, GONG ZHANG \& HARRIS QUACH} (University of Toronto)
}
%% Talk wps-gwalmp
\def\whenwhere{[Monday May 30 / lundi 30 mai, 12:00]\\}
{\Author{Guanbo}{Wang}{wang}{1C-D2}\Author{Alexander}{Levis}{levis}{1C-D2}\Author{Mengian}{Pang}{pang}{1C-D2}%
\medskip\par\textbf{GUANBO WANG, ALEXANDER LEVIS \& MENGIAN PANG} (McGill University)
}
%% Talk wps-fzxwjyyxjc
\def\whenwhere{[Monday May 30 / lundi 30 mai, 12:00]\\}
{\Author{Fei}{Zuo}{zuo}{1C-D2}\Author{Xiaotian}{Wang}{wang}{1C-D2}\Author{Jiajun}{Yan}{yan}{1C-D2}\Author{Yao}{Xi}{xi}{1C-D2}\Author{Jingjing}{Cao}{cao}{1C-D2}%
\medskip\par\textbf{FEI ZUO, XIAOTIAN WANG, JIAJUN YAN, YAO XI \& JINGJING CAO} (University of Toronto)
}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 1D-I1: Analysis of Big Data\\Analyse des m\'egadonn\'ees}
\begin{center}{\large Organizer and Chair / Responsable et président:  Francois Bellavance (HEC Montreal)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Welch - David Howes\end{center}
\label{abs-sid:abd}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk abd-lb
\def\whenwhere{[Monday May 30 / lundi 30 mai, 13:30]\\}
{\Author{Luke}{Bornn}{bornn}{1D-I1}%
\abshead{\absauthor{LUKE BORNN}\absaffil{Simon Fraser University}}
        {From Pixels to Points: Using Tracking Data to Measure Performance in Professional Sports\newline
        Des pixels aux points~: comment utiliser les données de poursuite pour mesurer la performance dans le sport professionnel}

\absSideBySide{In this talk I will explore how players perform, both individually and as a team, on a basketball court. By blending advanced spatio-temporal models with geography-inspired mapping tools, we are able to understand player skill far better than either individual tool allows. Using optical tracking data consisting of hundreds of millions of observations, I will demonstrate these ideas by characterizing defensive skill and decision making in NBA players.
}{Dans cette présentation, j'explorerai comment les joueurs réagissent, individuellement et en équipe, sur un terrain de basketball. La combinaison de modèles spatiotemporels avancés et d'outils cartographiques inspirés de la géographie nous permet une compréhension de l'habileté du joueur bien meilleure que celle que nous offre l'un ou l'autre de ces outils. Je présenterai ces idées à l'aide de données de poursuite optique consistant en des centaines de millions d'observations, qui permettent de caractériser l'habileté défensive et la prise de décision chez les joueurs de la NBA.
}}
%% Talk abd-ag
\def\whenwhere{[Monday May 30 / lundi 30 mai, 14:00]\\}
{\Author{Ershad}{Banijamali}{banijamali}{1D-I1}\Author{Ali}{Ghodsi}{ghodsi}{1D-I1}%
\abshead{\absauthor{ERSHAD BANIJAMALI} \& \absauthor{ALI GHODSI}\absaffil{University of Waterloo}}
        {Generative Mixture of Networks\newline
        Mélange de réseaux générateur}

\absSideBySide{In this talk I will introduce a generative model by training deep architecture. The model starts with dividing the input data into K clusters and feeding each of them into a separate network. So, there is a big network which consists of K sub-networks. The goal of training each sub-network is to generate samples which have close distribution to the underlying distribution of its assigned training set. Empirical estimation of Maximum Mean Discrepancy (MMD) is used as a measure of distance between these two distributions. Subsequently, an algorithm is employed whose goal is to further train the networks, jointly with updating the clusters of the training set by a non-parametric likelihood estimation. We call this algorithm, \textit{Mixture of Networks}.
}{Dans cette présentation, j'introduirai un modèle générateur créé par entrainement d'une architecture profonde. Pour commencer, les données d'entrée sont divisées en K grappes et intégrées dans des réseaux distincts. Nous nous retrouvons avec un grand réseau composé de K sous-réseaux. L'entrainement de chaque sous-réseau vise à générer des échantillons dont la distribution est proche de la distribution sous-jacente de son ensemble d'entrainement attitré. L'estimation empirique de la divergence moyenne maximale (MMD) est utilisée comme mesure de la distance entre ces deux distributions. Par la suite, un algorithme est utilisé pour continuer l'entrainement des réseaux, tout en mettant à jour les grappes de l'ensemble d'entrainement par une estimation de la vraisemblance non paramétrique. Nous appelons cet algorithme \textit{Mélange de réseaux}.
}}
%% Talk abd-pm
\def\whenwhere{[Monday May 30 / lundi 30 mai, 14:30]\\}
{\Author{Paul D.}{McNicholas}{mcnicholas}{1D-I1}%
\abshead{\absauthor{PAUL D. MCNICHOLAS}\absaffil{McMaster}}
        {Clustering Ultra High-Dimensional Data\newline
        Regroupement de données de très grande dimension}

\absSideBySide{Some approaches to clustering ultra high-dimensional data are considered. Each approach is based on a mixture model, and each component thereof is taken as corresponding to a cluster. Some approaches depend on the assumption of an underlying low-dimensional (latent) space while others also draw on shrinkage. Real data are used for illustration.
}{Nous examinons diverses approches de regroupement de données de très grande dimension. Chaque approche est fondée sur un modèle de mélange, dont chaque composante est considérée comme correspondant à un groupe. Certaines approches reposent sur l'hypothèse d'un espace (latent) à faible dimension sous-jacent, tandis que d'autres ont recours au concept du rétrécissement. Nous illustrons ces approches à l'aide de données réelles.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 1D-I2: New Perspectives in Causal Inference\\Nouvelles perspectives en inf\'erence causale}
\begin{center}{\large Organizer and Chair / Responsable et présidente:  Yeying Zhu (University of Waterloo)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 325\end{center}
\label{abs-sid:npc}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk npc-ph
\def\whenwhere{[Monday May 30 / lundi 30 mai, 13:30]\\}
{\Author{Peisong}{Han}{han}{1D-I2}\Author{Lu}{Wang}{wang}{1D-I2}%
\abshead{\absauthor{PEISONG HAN}\absaffil{University of Waterloo}, \absauthor{LU WANG}\absaffil{University of Michigan}}
        {Multiply Robust Estimation in Causal Inference\newline
        Estimation multiplement robuste en inférence causale}

\absSideBySide{Multiple robustness is a desirable property first established in the missing data literature. Estimators are multiply robust if they are consistent when any 
one of the multiple missingness probability models and multiple data distribution models is correctly specified. We will show how to construct multiply robust 
estimators for the average treatment effect for observational studies with binary treatments. The estimators are consistent if any one of the multiple propensity 
score models and multiple outcome regression models is correctly specified. Our estimation procedure can also achieve desired level of covariate balance between 
treatment and control groups by matching moments of the covariate distributions through reweighting the units in one or both groups.
}{La robustesse multiple est une propriété souhaitable d'abord établie dans la littérature des données manquantes. Des estimateurs sont multiplement robustes s'ils 
sont convergents lorsqu'un des modèles de probabilités de données manquantes multiples ou de loi de données multiples est correctement spécifié. Nous démontrerons 
comment construire des estimateurs multiplement robustes pour l'effet de traitement moyen lors d'études d'observations avec traitements binaires. Les estimateurs 
sont convergents si un des modèles de coefficients de propension multiples ou de régression des résultats multiples est correctement spécifié. Notre procédure d'estimation 
peut aussi atteindre le niveau souhaité de balance des covariables entre le traitement et les groupes témoins en faisant correspondre les moments des lois des covariables 
par la repondération des unités dans un groupe ou dans les deux groupes.
}}
%% Talk npc-wl
\def\whenwhere{[Monday May 30 / lundi 30 mai, 14:00]\\}
{\Author{Wei}{Luo}{luo}{1D-I2}\Author{Yeying}{Zhu}{zhu}{1D-I2}\Author{Debashis}{Ghosh}{ghosh}{1D-I2}%
\abshead{\absauthor{WEI LUO}\absaffil{Baruch College}, \absauthor{YEYING ZHU}\absaffil{University of Waterloo}, \absauthor{DEBASHIS GHOSH}\absaffil{Colorado School of Public Health}}
        {On Estimating Regression-based Causal Effects Using Sufficient Dimension Reduction\newline
        Estimer les effets de causalité basés sur la régression en utilisant une réduction suffisante de la dimension}

\absSideBySide{In many causal inference problems, the parameter of interest is often the regression causal effect, defined as the conditional mean difference in the potential 
outcomes given covariates. This paper discusses how sufficient dimension reduction can be used to assist in its estimation, and proposes a new estimator for the 
regression causal effect inspired by a sufficient dimension reduction method called the minimum average variance estimation. The estimator requires a weaker common 
support condition than the traditional propensity score-based approaches. In addition, it can be easily converted to estimate the average causal effect, where it is 
shown to be asymptotically super efficient. The finite-sample properties of the proposed method are illustrated using simulation studies.
}{Dans plusieurs problèmes d'inférence causale, le paramètre d'intérêt est souvent l'effet de causalité de la régression, défini comme étant la différence de la moyenne 
conditionnelle dans les résultats potentiels compte tenu des covariables. Cet article discute de la façon dont une réduction suffisante de la dimension peut être 
utilisée pour aider à son estimation et propose un nouvel estimateur pour l'effet causal de régression, inspiré d'une méthode de réduction suffisante de la dimension 
appelée estimation à variance moyenne minimale. L'estimateur nécessite une condition de support commun plus faible que dans les approches traditionnelles fondées 
sur les coefficients de propension. De plus, il peut facilement être transformé pour estimer l'effet causal moyen, où il est asymptotiquement super efficace. Les 
propriétés d'échantillons finis de la méthode proposée sont illustrées par des études de simulation.
}}
%% Talk npc-ms
\def\whenwhere{[Monday May 30 / lundi 30 mai, 14:30]\\}
{\Author{Mireille}{Schnitzer}{schnitzer}{1D-I2}\Author{Judith}{Lok}{lok}{1D-I2}\Author{Susan}{Gruber}{gruber}{1D-I2}\Author{Mark}{van der Laan}{vanderlaan}{1D-I2}%
\abshead{\absauthor{MIREILLE SCHNITZER}\absaffil{University of Montreal}, \absauthor{JUDITH LOK} \& \absauthor{SUSAN GRUBER}\absaffil{Harvard University}, \absauthor{MARK VAN DER LAAN}\absaffil{University of California, Berkeley}}
        {Collaborative Double Robustness and the Connection to Data-Adaptive Nuisance Model Selection in Causal Inference\newline
        La double robustesse collaborative et le lien à la sélection de modèles de nuisance par algorithme d'apprentissage adaptatif en inférence causale}

\absSideBySide{In causal inference and censored data methods, double robust estimators such as Targeted Minimum Loss-based estimation require the specification of two model components. Estimation will be consistent under the correct specification of either nuisance component, conditional on a sufficient set of confounding variables. However, this class of estimators also has a collaborative robustness property that allows for consistent effect estimation in a wider class of settings. We describe this class of estimators in single time point and longitudinal exposure settings, describe a cohesive approach to variable selection in causal inference, and illustrate the performance of data-adaptive learning algorithms for the nuisance models.
}{En inférence causale et méthodes pour données censurées, les estimateurs doublement robustes, telle l'estimation par maximum de vraisemblance ciblée, nécessitent la bonne spécification de deux composantes de la vraisemblance. L'estimation est convergente quand au moins une des deux composantes nuisibles est bien spécifiée, conditionnellement sur des facteurs de confusion suffisants. Cependant, cette classe d'estimateurs a aussi la propriété de robustesse collaborative permettant une estimation convergente dans certains cas, même si aucun des modèles n'est bien spécifié. Nous décrivons cette classe d'estimateurs pour des contextes d'exposition à un ou plusieurs points dans le temps, décrivons une approche cohérente pour la sélection de variables en inférence causale et illustrons la performance d'algorithmes d'apprentissage adaptatif pour l'estimation de modèles à composantes nuisibles.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 1D-I3: Small Area Estimation: New Developments\\Estimation pour petits domaines : nouveaux d\'eveloppements}
\begin{center}{\large Chair/Présidente: Karla Fox (Statistics Canada)\protect\\[5pt]
Organizer/Responsable: Mahmoud Torabi (University of Manitoba)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 258\end{center}
\label{abs-sid:sae}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk sae-dmgr
\def\whenwhere{[Monday May 30 / lundi 30 mai, 13:30]\\}
{\Author{M. Giovanna}{Ranalli}{ranalli}{1D-I3}\Author{Gaia}{Bertarelli}{bertarelli}{1D-I3}\Author{Francesco}{Bartolucci}{bartolucci}{1D-I3}\Author{Michele}{D'Alò}{dalo}{1D-I3}\Author{Fabrizio}{Solari}{solari}{1D-I3}%
\abshead{\absauthor{M. GIOVANNA RANALLI}\absaffil{University of Perugia, Italy}, \absauthor{GAIA BERTARELLI} \& \absauthor{FRANCESCO BARTOLUCCI}\absaffil{University of Perugia}, \absauthor{MICHELE D'ALÒ} \& \absauthor{FABRIZIO SOLARI}\absaffil{ISTAT}}
        {Time Series Small Area Estimation for Unemployment Rates using Latent Markov Models\newline
        Estimation pour petites régions de séries chronologiques relatives au taux de chômage à l'aide de modèles de Markov latents}

\absSideBySide{In this work we develop a new area-level Small Area Estimation method using Latent Markov Models (LMMs) in a Bayesian setting. LMMs handle longitudinal data in which the characteristics of interest, and their evolution in time, are represented by a latent process that follows a Markov chain. Small areas are allowed to move between latent states over time. LMMs may be seen as an extension of Markov chain models to control for measurement errors and as a dynamic extension of Latent class models for longitudinal data. Estimation is conducted using a Gibbs sampler with data augmentation. The proposed model is applied to estimate quarterly unemployment rates for Italian Local Labour Market Areas using LFS data from 2004 to 2014.
}{Nous développons une nouvelle méthode d'estimation pour petites régions au niveau de la région, basée sur des modèles de Markov latents (LMM) dans un contexte bayésien. Les LMM permettent de traiter des données longitudinales dans lesquelles les caractéristiques d'intérêt et leur évolution dans le temps sont représentées par un processus latent qui suit une chaîne de Markov. Les petites régions peuvent passer d'un état latent à un autre au fil du temps. Les LMM peuvent être considérés comme une extension des modèles de chaînes de Markov qui tient compte des erreurs de mesure – et comme une extension dynamique des modèles de catégories latentes pour les données longitudinales. Nous effectuons l'estimation à l'aide d'un échantillonneur de Gibbs avec augmentation des données. Nous appliquons ce modèle à l'estimation du taux de chômage trimestriel pour diverses régions du marché du travail italien avec des données de l'EPA de 2004 à 2014.
}}
%% Talk sae-djr
\def\whenwhere{[Monday May 30 / lundi 30 mai, 14:00]\\}
{\Author{J.N.K.}{Rao}{rao}{1D-I3}%
\abshead{\absauthor{J.N.K. RAO}\absaffil{Carleton University}}
        {On Measuring Uncertainty of Small Area Estimators Under Area Level and Unit Level Models\newline
        Mesure de l'incertitude des estimateurs pour petites régions dans les modèles au niveau de la région et de l'unité}

\absSideBySide{Model-based estimators of small area means that borrow strength from related areas through linking models are extensively used because direct area-specific estimators are imprecise due to small sample sizes within areas. We study different methods of estimating mean squared error (MSE) of the model-based estimators and appraise their relative properties. We also study methods of finding confidence intervals on the small area means. We consider both area level and unit level models.
}{On a largement recours à des estimateurs modélisés de la moyenne régionale dont l'efficacité est renforcée au moyen de données empruntées des régions connexes grâce à des modèles de liaison, parce que les estimateurs directs de la moyenne régionale pour une région précise sont imprécis en raison de la petite taille des échantillons régionaux. Nous étudions différentes méthodes d'estimation de l'erreur quadratique moyenne (EQM) des estimateurs modélisés et en évaluons les propriétés respectives. Nous étudions également les méthodes de calcul des intervalles de confiance pour les moyennes régionales. Nous examinons des modèles au niveau de la région et de l'unité.
}}
%% Talk sae-dmt
\def\whenwhere{[Monday May 30 / lundi 30 mai, 14:30]\\}
{\Author{Mahmoud}{Torabi}{torabi}{1D-I3}%
\abshead{\absauthor{MAHMOUD TORABI}\absaffil{University of Manitoba}}
        {Prediction in Small-Area Spatially Correlated Data\newline
        Prédiction dans les données de petites régions spatialement corrélées}

\absSideBySide{In small area estimation, we need to predict characteristics of the sub-populations (areas) based on the coarse scale data. Small area predictors are improved by using (standard) mixed models. However, there are many situations where the characteristics are related to their locations. For example, it is an interest of policy makers (and public) to know the spatial pattern of a rare disease (e.g., chronic disease or cancer). We propose small area models in the class of spatial mixed models to be able to predict characteristics and also to obtain corresponding mean squared prediction error (MSPE) and MSPE estimate. The performance of our proposed approach is evaluated through simulations and by a real application.
}{Dans l'estimation pour petites régions, il faut prédire les caractéristiques des sous-populations (régions) en fonction de données à une échelle moins fine. Les prédicteurs sur petits domaines sont souvent améliorés à l'aide de modèles mixtes (standard). Cependant, dans de nombreuses situations, les caractéristiques sont liées à leur localisation. Par exemple, il est important pour les décideurs (et pour le grand public) de connaitre la répartition géographique d'une maladie rare (maladie chronique ou cancer). Nous proposons des modèles pour petites régions de la classe des modèles spatiaux mixtes capables de prédire des caractéristiques et d'obtenir l'erreur quadratique moyenne de prédiction (EQMP) correspondante et une estimation de l'EQMP. Nous évaluons la performance de cette approche via des simulations et une application réelle.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 1D-I4: Statistical Modelling in Environmental and Health Studies\\Mod\'elisation statistique dans les \'etudes sur l'environnement et la sant\'e}
\begin{center}{\large Chair/Président: Tim Swartz (Simon Fraser University)\protect\\[5pt]
Organizer/Responsable: Paramjit Gill (British Columbia-Okanogan)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Welch 209\end{center}
\label{abs-sid:sme}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk sme-dsl
\def\whenwhere{[Monday May 30 / lundi 30 mai, 13:30]\\}
{\Author{Cindy}{Feng}{feng}{1D-I4}\Author{Mehdi}{Rostamiforooshani}{rostamiforooshani}{1D-I4}%
\abshead{\absauthor{CINDY FENG} \& \absauthor{MEHDI ROSTAMIFOROOSHANI}\absaffil{University of Saskatchewan}}
        {Modeling Spatially Correlated Survival Data: Impact of Misspecification of Correlation Structure on the Parameter Estimates\newline
        Modélisation de données de survie spatialement corrélées: impact des erreurs de spécification d'une structure de corrélation sur les estimations de paramètres}

\absSideBySide{In epidemiological and environmental studies, time to event data are often grouped into clusters (e.g. families, clinical sites and geographical regions, etc.). 
Spatial survival models have been proposed in literature for modeling geographically correlated survival data, which includes a flexible spatially varying baseline 
hazard function to control for unmeasured spatial confounders and to borrow information across geographical units.  The identified spatial pattern may highlight the 
regions requiring attention, which may assist public health professionals in their decision making.  Few studies have been conducted to investigate the consequence 
of ignoring modeling the spatial correlation on the parameter estimates, so we conducted a simulation study to determine how the bias and efficiency in the parameter 
estimates change when misspecifying the correlation structure.
}{Dans les études épidémiologiques et environnementales, les données de durée avant événement sont souvent regroupées en grappes (par exemple les familles, les sites 
cliniques et les régions géographiques, etc.). Les modèles de survie spatiaux ont été présentés dans la littérature pour modeler des données de survie corrélées 
géographiquement, ce qui inclut une fonction du risque de base flexible spatialement variable pour contrôler les facteurs de confusion spatiaux non mesurés et pour 
emprunter de l'information à travers les unités géographiques. Le schéma spatial identifié peut souligner les régions qui requièrent de l'attention, ce qui peut aider 
les professionnels de la santé dans le processus de prise de décision. Peu d'études ayant été menées pour étudier la conséquence d'ignorer la modélisation de la 
corrélation spatiale sur les estimations de paramètres, nous avons décidé de mener une étude de simulation pour déterminer le changement dans le biais et dans 
l'efficacité des estimations de paramètres lorsque la structure de corrélation est mal spécifiée.
}}
%% Talk sme-drm
\def\whenwhere{[Monday May 30 / lundi 30 mai, 14:00]\\}
{\Author{Renjun}{Ma}{ma}{1D-I4}%
\abshead{\absauthor{RENJUN MA}\absaffil{University of New Brunswick}}
        {Mixed Models for Correlated Environmental/Health Data with Left Censoring and Right-Skewness\newline
        Modèles mixtes pour données environnementales/de santé corrélées avec censure à gauche et asymétrie à droite}

\absSideBySide{Environmental and health data are often subject to left censoring at detection limit. Such censored data are usually right-skewed continuous, but with a point mass at the detection limit. Examples of such data are water pollutants in fish, tumor size due to radioactivity treatments, lesion depth related to ultrasound and precipitation. A special case of such data is so-called zero-inflated semi-continuous data where negligible amount is ignored. In this talk, we propose compound Poisson mixed models to characterize both the occurrence of the detection limit and size of the correlated data simultaneously. Our approach is illustrated with applications to real data.
}{Les données environnementales et de santé sont souvent sujettes à une censure à gauche à la limite de détection. Ces données censurées sont généralement asymétriques à droite et continues, mais avec une masse ponctuelle à la limite de détection. C'est le cas par exemple pour les polluants de l'eau dans les poissons, la taille d'une tumeur après traitements radioactifs, la profondeur de la lésion en fonction des ultrasons et des précipitations. Cas spécial de ces données~: les données dites semi-continues à surreprésentation de zéros où une quantité négligeable est ignorée. Dans cette présentation, nous proposons des modèles mixtes de Poisson composés pour caractériser simultanément l'occurrence de la limite de détection et l'importance des données corrélées. Nous illustrons notre approche par des applications sur des données réelles.
}}
%% Talk sme-dgp
\def\whenwhere{[Monday May 30 / lundi 30 mai, 14:30]\\}
{\Author{Gyanendra}{Pokharel}{pokharel}{1D-I4}\Author{Rob}{Deardon}{deardon}{1D-I4}%
\abshead{\absauthor{GYANENDRA POKHAREL} \& \absauthor{ROB DEARDON}\absaffil{University of Calgary}}
        {Gaussian Process Emulator-Based Inference for Spatial Models Infectious Disease Systems\newline
        Inférence basée sur un émulateur à processus gaussien pour les modèles spatiaux de systèmes de maladies infectieuses}

\absSideBySide{Statistical inference for mechanistic models of infectious disease spread is often very computationally expensive. Such models are generally fitted in a Bayesian Markov chain Monte Carlo (MCMC) framework that requires multiple calculation of likelihood function and is often computationally inefficient. This problem is more severe when incorporating large numbers of latent variables. Here, we propose a method of inference based on so-called emulation techniques. The method is again set in a Bayesian MCMC context, but avoids calculation of the computationally expensive likelihood function replacing it with a Gaussian process approximation. We show that such a method offers a significant gain in computation and can be used to infer the model parameters and underlying characteristics of spatial disease systems.
}{L'inférence statistique pour les modèles mécanistes de la propagation de maladies infectieuses est souvent très coûteuse en ressources informatiques. Ces modèles sont généralement ajustés dans un cadre de Monte-Carlo par chaîne de Markov (MCMC) bayésien qui exige des calculs multiples de la fonction de vraisemblance, ce qui est souvent inefficace d'un point de vue computationnel. Ce problème ne fait que s'aggraver en la présence de grands nombres de variables latentes. Ici, nous proposons une méthode d'inférence basée sur des techniques dites d'émulation. Cette méthode s'inscrit aussi dans un contexte de MCMC bayésien, mais elle évite le calcul de la fonction de vraisemblance, si coûteux en ressources, et le remplace par une approximation par processus gaussien. Nous montrons qu'une telle méthode offre un gain computationnel significatif et qu'elle permet d'inférer les paramètres du modèle et les caractéristiques sous-jacentes des systèmes spatiaux de la maladie.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 1D-I5: Imaging Genomics - A New Frontier for Statistical Methodology\\Imagerie g\'enomique -- une nouvelle fronti\`ere pour la m\'ethodologie statistique}
\begin{center}{\large Organizer and Chair / Responsable et président:  Linglong Kong (University of Alberta)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 246\end{center}
\label{abs-sid:iga}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk iga-mg
\def\whenwhere{[Monday May 30 / lundi 30 mai, 13:30]\\}
{\Author{Michele}{Guindani}{guindani}{1D-I5}%
\abshead{\absauthor{MICHELE GUINDANI}\absaffil{University of Texas}}
        {Bayesian Predictive Modeling for Imaging Genetics with Application to Schizophrenia\newline
        Modélisation prédictive bayésienne pour la génétique d'imagerie avec application à la schizophrénie}

\absSideBySide{By combining single-nucleotide polymorphism (SNP) arrays and functional magnetic resonance imaging (fMRI), we propose an integrative Bayesian risk prediction model that allows us to discriminate between individuals with schizophrenia and healthy controls, based on a sparse set of discriminatory regions of interest (ROIs) and SNPs. Inference on a regulatory network between SNPs and ROI intensities (ROI-SNP network) is used in a single modeling framework to inform the selection of the discriminatory ROIs and SNPs. We use simulation studies to assess the performance of our method and apply it to data collected from individuals with schizophrenia and healthy controls. We found our approach to outperform competing methods that do not link the ROI-SNP network to the selection of discriminatory markers.
}{En combinant des réseaux de polymorphismes nucléotidiques (SNP) et l'imagerie par résonance magnétique fonctionnelle (IRMf), nous proposons un modèle bayésien intégratif de prévision du risque qui nous permet de distinguer les individus schizophrènes des contrôles sains, basé sur un ensemble clairsemé de ROI (régions d'intérêt) et de SNP discriminants. Une inférence sur un réseau régulatoire entre les ROI et les SNP est utilisée dans une modélisation unifiée pour informer la sélection de ROI et de SNP discriminants. Nous utilisons des études de simulation pour évaluer la performance de notre méthode, avant de l'appliquer à des données collectées auprès d'individus schizophrènes et de contrôles sains. Nous montrons que notre approche surpasse des méthodes qui ne relient pas le réseau ROI-SNP à la sélection de marqueurs discriminants.
}}
%% Talk iga-fn
\def\whenwhere{[Monday May 30 / lundi 30 mai, 14:00]\\}
{\Author{Farouk}{Nathoo}{nathoo}{1D-I5}\Author{Keelin}{Greenlaw}{greenlaw}{1D-I5}\Author{Mary}{Lesperance}{lesperance}{1D-I5}\Author{Elena}{Szefer}{szefer}{1D-I5}\Author{Jinko}{Graham}{graham}{1D-I5}%
\abshead{\absauthor{FAROUK NATHOO}, \absauthor{KEELIN GREENLAW} \& \absauthor{MARY LESPERANCE}\absaffil{University of Victoria}, \absauthor{ELENA SZEFER} \& \absauthor{JINKO GRAHAM}\absaffil{Simon Fraser University}}
        {A Bayesian Group Sparse Multi-Task Regression Model for Imaging Genomics\newline
        Modèle de régression multitâche bayésienne à groupes clairsemés en génomique de l'imagerie}

\absSideBySide{Advances in technology for brain imaging and genotyping have motivated studies examining the relationships between genetic variation and brain structure. Wang et al. (Bioinformatics, 2012) developed an approach for simultaneous regression parameter estimation and SNP selection based on penalized regression with a group $l_{2,1}$-norm penalty. The group-norm penalty formulation incorporates the biological group structures among SNPs induced from their genetic arrangement and enforces sparsity at the group level. In this paper, we propose a corresponding Bayesian model that allows for full posterior inference for the regression parameters using Gibbs sampling. Properties of our method are investigated using simulation studies and the methodology is applied to a large dataset collected as part of the Alzheimer's Disease Neuroimaging Initiative.
}{Les progrès technologiques en imagerie cérébrale et génotypage ont motivé plusieurs études qui examinent les relations qui existent entre la variation génétique et la structure du cerveau. Wang et al. (Bioinformatics, 2012) ont mis au point une approche pour l'estimation simultanée des paramètres de régression et la sélection de SNP basée sur la régression pénalisée avec une pénalité de groupe de norme $l_{2,1}$. La formulation de pénalité de groupe de norme intègre les structures de groupes biologiques des SNP induites par leur agencement génétique et impose la parcimonie au niveau du groupe. Dans cet article, nous proposons une formulation bayésienne correspondante qui permet une inférence a posteriori complète par échantillonnage de Gibbs. Nous étudions les propriétés de notre méthode à l'aide d'études de simulation et appliquons notre méthodologie à un grand ensemble de données collectées dans le cadre de l'initiative en neuroimagerie de la maladie d'Alzheimer.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 1D-I6: Opportunities for statistical scientists in CIHR programs: a panel discussion\\Opportunit\'es pour les statisticiens dans les programmes IRSC : table ronde}
\begin{center}{\large Chair/Présidente: Mary Thompson (University of Waterloo)}
\end{center}
\par \vspace{3pt}

\begin{center}\large\bfseries Room/Salle: Thistle 242\end{center}
\label{abs-sid:oc}
\begin{center}{\large\bfseries Description}\end{center}
\bigskip
\absSideBySide{The Canadian Institutes of Health Research (CIHR) has recently introduced new open operating grant programs, such as Foundation Scheme and Project Scheme grants for individual researchers or teams working to solve problems, including methodological problems, about health and healthcare use. Other CIHR-funded programs of potential interest to statistical scientists include Collaborative Health Research Projects (joint with NSERC) and funding for training initiatives. The panelists will describe these opportunities from their own perspectives and share their advice on applying to CIHR programs.}{Les Instituts de recherche en santé du Canada (IRSC) ont présenté de nouveaux programmes ouverts de subventions de fonctionnement, comme les subventions – volet Fondation et volet Projet, destinés à des chercheurs ou équipes de recherche s'appliquant à résoudre des problèmes, y compris d'ordre méthodologique, en matière de santé et d'utilisation des soins de santé. D'autres programmes financés par les IRSC d'un intérêt éventuel pour les scientifiques de la statistique comprennent les Projets de recherche concertée sur la santé (conjointement avec le CRSNG) et des subventions de projets en matière de formation. Les panélistes décriront ces possibilités de financement de leur point de vue respectif et expliqueront comment soumettre une demande aux programmes des IRSC.}\bigskip
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 1D-C1: Actuarial Science and Finance 1\\Actuariat et finance 1}
\begin{center}{\large Chair/Président: \'Etienne Marceau (Laval University)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 257\end{center}
\label{abs-sid:asf}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk asf-nb
\def\whenwhere{[Monday May 30 / lundi 30 mai, 13:30]\\}
{\Author{Nicholas}{Beck}{beck}{1D-C1}\Author{Mélina}{Mailhot}{mailhot}{1D-C1}%
\abshead{\absauthor{NICHOLAS BECK}\absaffil{McGill University}, \absauthor{MÉLINA MAILHOT}\absaffil{Concordia University}}
        {A Consistent Estimator to the Orthant-Based Tail Value-at-Risk\newline
        Un estimateur convergent de l'aile de la valeur à risque en fonction de l'orthant}

\absSideBySide{In this paper we address the estimation of multivariate Value-at-Risk (VaR) and Tail
Value-at-Risk (TVaR). We recall definitions for the bivariate lower and upper orthant VaR
and bivariate lower and upper orthant TVaR, presented in Cossette et al. (2013, 2014).
Here, we present estimators for both these measure extended to arbitrary dimension $d>2$
and establish the consistency of our estimator for the lower and upper orthant TVaR in
any dimension. We then demonstrate these results by providing numerical examples that
compare our estimator to theoretical results, from both real and simulated data.
}{Dans cet article nous abordons l'estimation de la valeur à risque multivariée (VaR) et de l'aile de la valeur à risque (TVaR). Nous nous rappelons certaines définitions 
pour la VaR bivariée de l'orthant inférieur et supérieur et pour la TVaR bivariée de l'orthant inférieur et supérieur, présentées dans Cossette et al. (2013, 2014). 
Nous présentons des estimateurs pour ces deux mesures élargies à une dimension arbitraire $d>2$ et nous établissons la convergence de notre estimateur pour la TVaR de 
l'orthant inférieur et supérieur dans n'importe quelle dimension. Nous démontrons ensuite ces résultats par des exemples numériques qui comparent notre estimateur à 
des résultats théoriques, pour des données réelles et simulées.
}}
%% Talk asf-jb
\def\whenwhere{[Monday May 30 / lundi 30 mai, 13:45]\\}
{\Author{Jean-François}{Bégin}{begin}{1D-C1}\Author{Mathieu}{Boudreault}{boudreault}{1D-C1}\Author{Delia Alexandra}{Doljanu}{doljanu}{1D-C1}\Author{Geneviève}{Gauthier}{gauthier}{1D-C1}%
\abshead{\absauthor{JEAN-FRANÇOIS BÉGIN}\absaffil{HEC Montréal}, \absauthor{MATHIEU BOUDREAULT}\absaffil{Université du Québec à Montréal}, \absauthor{DELIA ALEXANDRA DOLJANU} \& \absauthor{GENEVIÈVE GAUTHIER}\absaffil{HEC Montréal}}
        {Credit and Systemic Risks in the Financial Services Sector\newline
        Risques de crédit et systémique dans le secteur financier}

\absSideBySide{The Great Recession has shaken the foundations of the financial industry and led to tighter solvency monitoring of both the banking and insurance industries. To this end, we develop a portfolio credit risk model that includes firm-specific Markov-switching regimes as well as individual stochastic and endogenous recovery rates. Using weekly credit default swap premiums for 35 financial firms, we analyze the credit risk of each of these companies and their statistical linkages, placing special emphasis on the 2005-2012 period.  Moreover, we study the systemic risk affecting both the banking and insurance subsectors.
}{La Grande Récession a ébranlé les bases de l'industrie financière et a mené à un suivi plus serré de la solvabilité des firmes du secteur financier. À cet effet, nous développons un modèle de risque de crédit multivarié qui comprend des régimes propres à chaque firme ainsi qu'un taux de recouvrement stochastique. À l'aide de primes hebdomadaires pour 35 sociétés, nous analysons le risque de crédit de chacune de ces firmes et leurs liens statistiques, mettant un accent particulier sur 2005-2012. Nous étudions aussi le risque systémique affectant à la fois le sous-secteur bancaire et celui de l'assurance.
}}
%% Talk asf-lc
\def\whenwhere{[Monday May 30 / lundi 30 mai, 14:00]\\}
{\Author{Lichen}{Chen}{chen}{1D-C1}\Author{Adam}{Kolkiewicz}{kolkiewicz}{1D-C1}\Author{Tony}{Wirjanto}{wirjanto}{1D-C1}%
\abshead{\absauthor{LICHEN CHEN}, \absauthor{ADAM KOLKIEWICZ} \& \absauthor{TONY WIRJANTO}\absaffil{University of Waterloo}}
        {Bayesian Estimation of GARCH Models with General Parameter Constraints\newline
        Estimation bayésienne des modèles GARCH avec contraintes sur le paramètre général}

\absSideBySide{We discuss Bayesian estimation of GARCH models with general parameter constraints for non-negative conditional variance. The general parameter constraints are important for capturing long-memory type behaviour of financial asset return volatilities together with their short-term behaviour. However, the shapes of the parameter space and the likelihood surface under these constraints often pose computational problems to standard estimation algorithms. In our work we explain how Bayesian MCMC methods and model reparameterization can help us solve these difficulties.
}{Nous discutons l'estimation bayésienne des modèles GARCH avec des contraintes sur le paramètre général pour la variance conditionnelle non-négative. Les contraintes sur le paramètre général sont importantes pour saisir le comportement de type longue mémoire des volatilités du retour sur l'actif financier ainsi que leur comportement à court terme. Cependant, les formes de l'espace des paramètres et la surface de vraisemblance sous ces contraintes posent souvent des problèmes de calcul aux algorithmes standards d'estimation. Dans notre travail, nous expliquons comment les méthodes bayésiennes MCMC et le reparamétrage du modèle peuvent nous aider à résoudre ces difficultés.
}}
%% Talk asf-wj
\def\whenwhere{[Monday May 30 / lundi 30 mai, 14:15]\\}
{\Author{Wenjun}{Jiang}{jiang}{1D-C1}\Author{Jiandong}{Ren}{ren}{1D-C1}%
\abshead{\absauthor{WENJUN JIANG} \& \absauthor{JIANDONG REN}\absaffil{Western University}}
        {Optimal Reinsurance Minimizing the Risk of Joint Party\newline
        Réassurance optimale minimisant le risque de la partie commune}

\absSideBySide{Optimal reinsurance problem has been studied for a long time. By imposing restrictions on ceded function, analytical solutions can be obtained under certain risk measures. However, most studies are from the viewpoint of insurer or reinsurer. In this paper, we consider the bivariate Value-at-Risk of joint party, insurer and reinsurer, and take full use of geometric approach to obtain the optimal form of reinsurance with ceded function in $C^1$ and $C^2$ [Chi and Tan~(2011)]. We further illustrate the solution is also Pareto-optimal and derive the optimal reinsurance parameters.
}{Le problème de réassurance optimale a été étudié depuis longtemps. En imposant des restrictions sur la fonction cédée, les solutions analytiques peuvent être obtenues sous certaines mesures du risque. Cependant, la plupart des études sont du point de vue de l'assureur ou du réassureur. Dans cet article, nous considérons la valeur à risque bivariée de la partie commune, assureur et réassureur, et utilisons pleinement l'approche géométrique pour obtenir la forme optimale de la réassurance avec fonction cédée en $C^1$ et $C^2$ [Chi et Tan~(2011)]. De plus, nous illustrons que la solution est également Pareto optimale et dérivons les paramètres de réassurance optimale.
}}
%% Talk asf-ll
\def\whenwhere{[Monday May 30 / lundi 30 mai, 14:30]\\}
{\Author{Luyao}{Lin}{lin}{1D-C1}\Author{Li}{Chen}{chen}{1D-C1}\Author{Yi}{Lu}{lu}{1D-C1}\Author{Gary}{Parker}{parker}{1D-C1}%
\abshead{\absauthor{LUYAO LIN}, \absauthor{LI CHEN}, \absauthor{YI LU} \& \absauthor{GARY PARKER}\absaffil{Simon Fraser University}}
        {Analysis of Survivorship Life Insurance Portfolio with Stochastic Interest Rates\newline
        Analyse d'un portefeuille d'assurance de survie avec taux d'intérêt stochastique}

\absSideBySide{A general portfolio of joint survivorship life insurance contracts is studied in a stochastic interest rate environment with dependent mortality model. Two methods are used to derive the first two moments of the projected prospective loss random variable. The distribution function of the present value of future losses at given valuation time is derived and approximated for a homogeneous portfolio. The effects of the mortality dependence, the portfolio size and the policy type, as well as the impact of investment strategies on the riskiness of portfolios of survivorship life insurance policies are analyzed by means of moments and probability distributions.
}{Nous étudions un portefeuille général de contrats d'assurance de survie au moyen d'un modèle de mortalité dépendant dans un contexte de taux d'intérêt stochastique. Nous utilisons deux méthodes afin d'obtenir les deux premiers moments de la variable prospective de perte aléatoire. Nous dérivons la fonction de répartition de la valeur actuelle des futures pertes à un moment donné de l'évaluation et nous en faisons une approximation afin d'obtenir un portefeuille homogène. Nous analysons les effets de la dépendance de la mortalité, de la taille du portefeuille et du type de police, ainsi que l'incidence des stratégies d'investissement sur le niveau de risque des portefeuilles de polices d'assurance de survie au moyen de distributions des moments et de probabilités.
}}
%% Talk asf-yw
\def\whenwhere{[Monday May 30 / lundi 30 mai, 14:45]\\}
{\Author{Ying}{Wang}{wang}{1D-C1}\Author{Jun}{Cai}{cai}{1D-C1}%
\abshead{\absauthor{YING WANG} \& \absauthor{JUN CAI}\absaffil{University of Waterloo}}
        {Fair Premium for Reinsurance Contracts\newline
        Primes équitables des contrats de réassurance}

\absSideBySide{Fair reinsurance premium principles considering risks of both insurer and reinsurer are introduced. Scholars usually investigate the optimal ceded function if the reinsurance premium is given. In this paper, quadratic and identity loss functions are applied to quantify the risks of both insurer and reinsurer. And two novel reinsurance premium principles are proposed. In addition, the properties for these two kinds of premium principles are studied.
}{Des principes de primes équitables en cas de réassurance, compte tenu des risques encourus à la fois par l'assureur et le réassureur sont présentés ici.  En général, les chercheurs examinent la fonction de cession optimale, lorsque la prime de réassurance est connue. Dans cet article, les fonctions de perte quadratiques et d'identité sont appliquées pour quantifier les risques de l'assureur et du réassureur. Deux nouveaux principes de primes de réassurance sont proposés. De plus, les propriétés de ces deux types de principes de primes sont étudiées.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 1D-C2: Biostatistics: Methods and Applications 2\\Biostatistique : m\'ethodes et applications 2}
\begin{center}{\large Chair/Président: Joseph Beyene (McMaster University)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 259\end{center}
\label{abs-sid:bma}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk bma-yj
\def\whenwhere{[Monday May 30 / lundi 30 mai, 13:30]\\}
{\Author{Yang}{Jian}{jian}{1D-C2}\Author{Wu}{Jingjing}{jingjing}{1D-C2}\Author{Li}{Haocheng}{haocheng}{1D-C2}%
\abshead{\absauthor{YANG JIAN}, \absauthor{WU JINGJING} \& \absauthor{LI HAOCHENG}\absaffil{University of Calgary}}
        {Minimum Hellinger Distance Estimation for Linear Regression Model\newline
        Méthode d'estimation de la distance minimale de Hellinger pour un modèle de régression linéaire}

\absSideBySide{     Minimum Hellinger Distance estimation (MHDE) has been shown an appealing method of estimation for discrete data when the assumed model is suspected to be true. In this presentation, we first introduce Minimum Hellinger Distance (MHD) as well as the MHDE. Then, we will derive the MHDE for linear regression model. Some properties of the estimator are discussed and a comparison with MLE is carried out through Monte Carlo simulation studies. Lastly, we will apply this method to a breast cancer data set and demonstrate its implementation and efficiency in estimation.
}{L'estimation de la distance minimale de Hellinger (EDMH) se veut une méthode attrayante d'estimation pour les données discrètes lorsque le modèle supposé est soupçonné être le vrai. Dans cette présentation, nous présentons d'abord la distance minimale de Hellinger (DMH) et l'EDMH. Ensuite, nous allons dériver l'EDMH pour le modèle de régression linéaire. Certaines propriétés de l'estimateur sont discutées et une comparaison avec l'EMV est réalisée à l'aide d'études de simulation de Monte Carlo. Enfin, nous allons appliquer cette méthode à un jeu de données sur le cancer du sein et démontrer sa mise en œuvre et son efficacité dans l'estimation.
}}
%% Talk bma-el
\def\whenwhere{[Monday May 30 / lundi 30 mai, 13:45]\\}
{\Author{Erin}{Lundy}{lundy}{1D-C2}\Author{Charmaine}{Dean}{dean}{1D-C2}%
\abshead{\absauthor{ERIN LUNDY} \& \absauthor{CHARMAINE DEAN}\absaffil{University of Western Ontario}}
        {Analyzing Heaped Counts and Longitudinal Presence/Absence Data in Joint Zero-inflated Poisson Regression Models\newline
        Analyse de données de dénombrement arrondies et de données longitudinales de présence et d'absence dans des modèles conjoints de régression de Poisson à surreprésentation de zéros}

\absSideBySide{Recurrent event data where a fraction of subjects are not at-risk for an event are frequently seen in longitudinal studies. In many settings, an aggregate count of the number of self-reported events over an observation period is recorded. Self-reported counts are often subject to heaping which yields a distorted distribution for the observed counts and therefore may bias estimation. Alternatively, the presence/absence of events between shorter periodic assessments may be recorded. Motivated by a major study of criminal behaviour, we compare the analysis of aggregate heaped count data and longitudinal presence/absence data using joint zero-inflated Poisson regression models.
}{Dans les études longitudinales, des données d'événements récurrents où une fraction des sujets n'est pas à risque pour un événement sont fréquemment observées. Dans de nombreuses situations, on enregistre des données de dénombrement agrégé du nombre d'événements autodéclarés sur une certaine période d'observation. Les données de dénombrement autodéclarées sont souvent arrondies, ce qui a une incidence sur la répartition des données observées et ce qui peut biaiser l'estimation. Par ailleurs, la présence ou l'absence d'événements entre des évaluations périodiques plus courtes peuvent être enregistrées. Motivés par une grande étude sur le comportement criminel, nous comparons l'analyse des données arrondies de dénombrement agrégé et les données de présence et d'absence au moyen de modèles de régression de Poisson à surreprésentation de zéros.
}}
%% Talk bma-rb
\def\whenwhere{[Monday May 30 / lundi 30 mai, 14:00]\\}
{\Author{Rachid}{Bentoumi}{bentoumi}{1D-C2}\Author{Mayer}{Alvo}{alvo}{1D-C2}\Author{Mhamed}{Mesfioui}{mesfioui}{1D-C2}%
\abshead{\absauthor{RACHID BENTOUMI} \& \absauthor{MAYER ALVO}\absaffil{Université d'Ottawa}, \absauthor{MHAMED MESFIOUI}\absaffil{Université du Québec à Trois-Rivières}}
        {Information Gain under Length-biased Sampling\newline
        Gain en information sous l'échantillonnage à biais de longueur}

\absSideBySide{In epidemiological studies, subjects with disease (prevalent cases) differ from newly diseased (incident) cases. Methods for regression analyses have recently been 
proposed to measure the potential effects of covariates on survival. We propose to extend the measure of dependence based on information gain in the context of 
length-biased sampling. This will require development of an estimator for the covariate distribution. We will assess the asymptotic properties of the measure of 
dependence and estimated information gain and illustrate the methods using both simulation and application to the Canadian Study on Health.
}{Dans les études épidémiologiques, les sujets souffrant de la maladie (cas prévalents) diffèrent des cas nouvellement malades (incidents). Des méthodes pour les 
analyses de régression ont récemment été proposées pour mesurer les effets potentiels des covariables sur la survie. Nous proposons d'élargir la mesure de dépendance 
fondée sur le gain en information dans le contexte d'échantillonnage à biais de longueur. Ceci exigera le développement d'un estimateur pour la loi de la covariable. 
Nous évaluerons les propriétés asymptotiques de la mesure de dépendance et du gain en information estimé et nous illustrerons les méthodes en utilisant à la fois la 
simulation et l'application à l'étude canadienne sur la santé.
}}
%% Talk bma-eg
\def\whenwhere{[Monday May 30 / lundi 30 mai, 14:15]\\}
{\Author{Elnaz}{Ghadimi}{ghadimi}{1D-C2}\Author{Arusharka}{Sen}{sen}{1D-C2}%
\abshead{\absauthor{ELNAZ GHADIMI} \& \absauthor{ARUSHARKA SEN}\absaffil{Concordia University}}
        {Multivariate Cure Rate Estimation under Random Censoring\newline
        Estimation multivariée du taux de guérison sous censure aléatoire}

\absSideBySide{We considered a non-parametric multivariate cure rate estimator under random censoring. Individuals can be cured or fail. The event of interest is defined as death. Our data have censoring at the end of follow-up. Sen and Stute proposed a multivariate Kaplan-Meier estimator via mass-shifting method. We use the tail of the Sen-Stute estimator as an estimator of cure rate under random censoring. Taking a cue from Maller and Zhou without using martingale theory, the asymptotic normality of univariate and multivariate cure rate estimator is established and its variance estimator is obtained. The results are illustrated using simulation and real data.
}{Nous avons considéré un estimateur multivarié non-paramétrique du taux de guérison sous censure aléatoire. Les individus peuvent être guéris ou non. L'événement d'intérêt est défini comme étant le décès. Nos données sont censurées à la fin du suivi. Sen et Stute ont proposé un estimateur multivarié de Kaplan-Meier via la méthode de déplacement de la masse. Nous utilisons la queue de l'estimateur Sen-Stute comme estimateur du taux de guérison sous censure aléatoire. En s'inspirant de Maller et Zhou sans utiliser la théorie des martingales, la normalité asymptotique de l'estimateur univarié et multivarié du taux de guérison est établie et son estimateur de la variance est obtenu. Les résultats sont illustrés à l'aide de simulations et de données réelles.
}}
%% Talk bma-rsk
\def\whenwhere{[Monday May 30 / lundi 30 mai, 14:30]\\}
{\Author{Regina S.}{Kampo}{kampo}{1D-C2}\Author{Ashley}{Bonner}{bonner}{1D-C2}\Author{Joseph}{Beyene}{beyene}{1D-C2}%
\abshead{\absauthor{REGINA S. KAMPO}, \absauthor{ASHLEY BONNER} \& \absauthor{JOSEPH BEYENE}\absaffil{McMaster University}}
        {Assessing the Influence of Non-adherence on Fixed-Effect Meta-Analysis for a Continuous Outcome: A Simulation Study\newline
        Évaluer l'influence de la non-adhérence sur la méta-analyse à effets fixes pour un résultat continu : une étude de simulation}

\absSideBySide{A traditional meta-analysis assumes complete adherence for an intervention.  For some interventions such as exercise, change of diet etc., adherence might be a challenge that may compromise inferences. We investigated the influence of non-adherence on meta-analysis of a continuous outcome through simulations. We varied several key parameters and assessed the effect of non-adherence on estimation and hypothesis test properties. The findings from the simulation studies show that, the properties of estimation and hypothesis perform well under complete adherence but are not optimal as non-adherence is observed. Thus, researchers must treat non-adherence cautiously before using results in decision making.
}{Une méta-analyse traditionnelle suppose l'adhérence complète pour une intervention. Pour certaines interventions telles que l'exercice, le changement de régime alimentaire, etc., l'adhérence pourrait représenter un défi pouvant compromettre les inférences. Nous avons étudié l'influence de la non-adhérence sur la méta-analyse d'un résultat continu à l'aide de simulations. Nous avons fait varier plusieurs paramètres clés et avons évalué l'effet de la non-adhérence sur les propriétés de l'estimation et du test d'hypothèse. Les résultats des études de simulation montrent que les propriétés de l'estimation et de l'hypothèse fournissent de bons résultats sous adhérence complète, mais ne sont pas optimales lorsque la non-adhérence est observée. Ainsi, les chercheurs doivent traiter la non-adhérence avec prudence avant d'utiliser les résultats dans le processus décisionnel.
}}
%% Talk bma-yx
\def\whenwhere{[Monday May 30 / lundi 30 mai, 14:45]\\}
{\Author{Yuying}{Xie}{xie}{1D-C2}\Author{Yeying}{Zhu}{zhu}{1D-C2}\Author{Cecilia A.}{Cotton}{cotton}{1D-C2}\Author{Pan}{Wu}{wu}{1D-C2}%
\abshead{\absauthor{YUYING XIE}, \absauthor{YEYING ZHU} \& \absauthor{CECILIA A. COTTON}\absaffil{University of Waterloo}, \absauthor{PAN WU}\absaffil{Value Institute, Christiana Care Health System}}
        {A Model Averaging Approach for Estimating Propensity Scores by Optimizing Balance\newline
        Approche par modèle moyen pour l'estimation de scores de propension en optimisant l'équilibre}

\absSideBySide{Many approaches, including traditional parametric modelling and machine learning techniques, have been proposed to estimate propensity scores. We proposes a new model averaging approach to propensity score estimation in which parametric and nonparametric estimates are combined to achieve covariate balance. Simulation studies are conducted across different scenarios varying in the degree of misspecification in the treatment model. It shows that the proposed method produces less bias and smaller standard error than existing approaches. This approach is applied to a real data set in evaluating the causal effect of formula or mixed feeding versus exclusive breastfeeding on a child's BMI Z-score.
}{De nombreuses approches, y compris la modélisation paramétrique traditionnelle et les techniques d'apprentissage automatique, ont été proposées pour estimer les scores de propension. Nous proposons une nouvelle approche par modèle moyen pour l'estimation des scores de propension, approche qui combine des estimations paramétriques et non paramétriques pour équilibrer les covariables. Des études de simulation sont menées selon divers scénarios dans lesquels le degré d'erreur de spécification du modèle de traitement varie. Nous voyons que contrairement aux approches existantes, il y a moins de biais et des erreurs types moins importantes avec la méthode proposée. Celle-ci est appliquée à un ensemble de données réelles afin d'évaluer l'effet causal de l'usage de formules ou d'une alimentation mixte pour les bébés comparativement au recours exclusif à l'allaitement sur le score-Z de l'IMC de l'enfant.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 1D-C3: Statistical Methods and Applications 2\\M\'ethodes statistiques et applications 2}
\begin{center}{\large Chair/Président: William Aeberhard (Dalhousie University)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 255\end{center}
\label{abs-sid:sma4}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk sma4-lbhs
\def\whenwhere{[Monday May 30 / lundi 30 mai, 13:30]\\}
{\Author{Latifa Ben Hadj}{Slimene}{slimene}{1D-C3}\Author{Éric}{Marchand}{marchand}{1D-C3}\Author{Theodoros}{Nicoleris}{nicoleris}{1D-C3}%
\abshead{\absauthor{LATIFA BEN HADJ SLIMENE} \& \absauthor{ÉRIC MARCHAND}\absaffil{Université de Sherbrooke}, \absauthor{THEODOROS NICOLERIS}\absaffil{University of the Aegean, Greece}}
        {Inference of a Constrained Parameter in Presence of an Uncertain Constraint\newline
        Inférence d'une contrainte paramétrique en présence d'une contrainte incertaine}

\absSideBySide{We consider a statistical model $X \sim  f_{\theta}$ and the problem of estimating $\theta$ under the parametric constraint $\theta \in C \subset \mathbb{R}^p$, with situations where uncertainty resides in the parametric constraint and where we take a simple hierarchical Bayes approach to describe the uncertainty relative to $C$ and to $\theta$.
We will focus on the case of a lower bound constraint. We provide various examples.  For estimating a positive normal mean under squared error loss, we obtain hierarchical Bayes estimators which are also minimax.  Finally, extensions to predictive density estimation are provided.
}{On considère le modèle statistique $X \sim f_{\theta}$ et le problème de l'estimation de $\theta$ sous la contrainte paramétrique $\theta \in C \in \mathbf{R}^p$, où l'incertitude réside dans la contrainte paramétrique et où nous prenons une approche simple de Bayes hiérarchique pour décrire l'incertitude par rapport à C et $\theta$. On se concentre sur le  cas d'une contrainte sur la borne inférieure. 
On fournit divers exemples. Pour estimer une moyenne normale positive sous la perte quadratique, on obtient les estimateurs de Bayes hiérarchiques qui sont aussi minimax. Enfin, les extensions à l'estimation par densité prédictive sont aussi prévues.
}}
%% Talk sma4-ar
\def\whenwhere{[Monday May 30 / lundi 30 mai, 14:00]\\}
{\Author{Adam}{Rahman}{rahman}{1D-C3}\Author{Wayne}{Oldford}{oldford}{1D-C3}%
\abshead{\absauthor{ADAM RAHMAN} \& \absauthor{WAYNE OLDFORD}\absaffil{University of Waterloo}}
        {What Makes a Scatterplot Interesting?\newline
        Qu'est-ce qui rend un nuage de points pertinent?}

\absSideBySide{An important question that arises in modern data analytics is how to detect (and ultimately create) interesting structure in a set of points. Tools such as the minimum spanning tree and Delaunay triangulation can help to detect the presence of interesting structure. Using these concepts, Wilkinson and Wills introduced scagnostics, which uses the tools available in graph theory to numerically summarize the structure of a scatterplot in two dimensions. We will consider some of the limitations of scagnostics, propose new measures that capture interesting structure that escapes current scagnostics, and consider the possibilities of generalization to higher dimensions.
}{Une question importante qui surgit dans les analyses de données modernes concerne la façon de détecter (et finalement créer) une structure pertinente dans un ensemble de points. Des outils, tels que l'arbre recouvrant de longueur minimum et la triangulation de Delaunay, peuvent aider à détecter la présence d'une structure pertinente. Au moyen de ces concepts, Wilkinson et Wills ont introduit la librairie R scagnostic, qui utilise les outils disponibles dans la théorie des graphes pour résumer numériquement la structure d'un nuage de points en deux dimensions. Nous considérerons certaines limitations de scagnostic, et proposerons de nouvelles mesures qui permettent de capturer une structure pertinente qui échappe actuellement à scagnostic. Nous considérerons les possibilités de généralisation à des dimensions plus élevées.
}}
%% Talk sma4-yw
\def\whenwhere{[Monday May 30 / lundi 30 mai, 14:15]\\}
{\Author{Yilei}{Wu}{wu}{1D-C3}\Author{Yingli}{Qin}{qin}{1D-C3}\Author{Mu}{Zhu}{zhu}{1D-C3}%
\abshead{\absauthor{YILEI WU}, \absauthor{YINGLI QIN} \& \absauthor{MU ZHU}\absaffil{University of Waterloo}}
        {Large Covariance Matrix Estimation - A Factor Model Approach\newline
        Estimation d'une grande matrice de covariance – une approche par modèle factoriel}

\absSideBySide{Estimating high-dimensional covariance matrix is a challenging and important problem. Motivated by lasso-type estimators in the literature, we reduce the complexity of the estimation problem by imposing a factor model structure for the population covariance matrix. We prove the consistency of our estimator under moderate conditions on the eigenvalues of the population covariance matrix.  Our covariance matrix estimate can be used in various statistical procedures, such as discriminant analysis.
}{L'estimation d'une grande matrice de covariance est une préoccupation importante. Motivés par les estimateurs de type Lasso présentés dans la littérature, nous réduisons la complexité du problème que pose l'estimation en imposant une structure de modèle factoriel pour la matrice de covariance de la population. Nous prouvons également la convergence de notre estimateur sous des conditions modérées pour les valeurs propres de la matrice de covariance de la population. Notre estimation d'une matrice de covariance peut être utilisée pour diverses procédures statistiques, comme l'analyse discriminante.
}}
%% Talk sma4-xl
\def\whenwhere{[Monday May 30 / lundi 30 mai, 14:30]\\}
{\Author{Xin}{Liu}{liu}{1D-C3}\Author{Wenqing}{He}{he}{1D-C3}%
\abshead{\absauthor{XIN LIU} \& \absauthor{WENQING HE}\absaffil{Western University}}
        {Improve Performance of Support Vector Machine Classifiers with Data Adaptive Kernel\newline
        Amélioration de l'efficacité du classificateur d'une machine à vecteurs de support avec noyau adaptatif aux données}

\absSideBySide{In this talk, a new way to enhance the performance of an SVM classifier is presented. The initial kernel function is conformally re-scaled in an adaptive way so that the separation between two classes can be effectively enlarged, based on the prior knowledge obtained from the conventional SVM. The modified classifier takes into consideration the distribution of the support vectors in the feature space, and the correlation between voxels will be dealt with by selecting only limited numbers of parameters properly. Improvement of prediction accuracy from this data-dependent SVM is shown with numerical studies.
}{Nous présentons une nouvelle façon d'améliorer l'efficacité du classificateur d'une machine à vecteurs de support . La fonction initiale du noyau est pondérée de façon conforme et adaptative afin d'élargir efficacement la séparation entre deux classes, en fonction des connaissances préalables acquises à partir de la machine à vecteurs de support classique. Le classificateur modifié prend en considération la répartition des vecteurs de support dans l'espace des attributs, et la corrélation entre les voxels sera traitée en sélectionnant adéquatement seulement un certain nombre de paramètres. Au moyen d'études numériques, nous montrons l'amélioration de l'exactitude de la prédiction à partir de cette machine à vecteurs de support dépendante des données.
}}
%% Talk sma4-ns
\def\whenwhere{[Monday May 30 / lundi 30 mai, 14:45]\\}
{\Author{Nathaniel}{Stevens}{stevens}{1D-C3}\Author{Christine M.}{Anderson-Cook}{andersoncook}{1D-C3}%
\abshead{\absauthor{NATHANIEL STEVENS}\absaffil{University of San Francisco}, \absauthor{CHRISTINE M. ANDERSON-COOK}\absaffil{Los Alamos National Laboratory}}
        {Comparing the Reliability of Related Populations with the Probability of Agreement\newline
        Comparaison de la fiabilité des populations apparentées et de la probabilité d'accord}

\absSideBySide{Combining information between different populations to improve precision, simplify future predictions or improve underlying understanding of relationships can be advantageous when considering the reliability of several related sets of systems. Using the probability of agreement to help quantify the similarities of populations can help to give a realistic assessment of whether the systems have reliabilities that are sufficiently similar for practical purposes to be treated as a homogeneous combined population. The new method is described and illustrated with an example involving two generations of a complex system where the reliability is modeled using either a logistic or probit regression model.
}{La combinaison de l'information entre différentes populations visant à améliorer la précision, à simplifier les prédictions futures ou à améliorer la compréhension sous-jacente des relations peut être un avantage lorsqu'on considère la fiabilité de plusieurs ensembles apparentés de systèmes. L'utilisation de la probabilité d'adéquation pour aider à quantifier les similitudes des populations peut contribuer à évaluer de façon réaliste si les systèmes ont des fiabilités qui sont suffisamment semblables, à des fins pratiques, pour être traitées comme une population homogène combinée. La méthode proposée est décrite et illustrée par un exemple impliquant deux générations d'un système complexe où la fiabilité est modélisée au moyen d'un modèle de régression logistique ou probit.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 1D-C4: Statistical Theory\\Th\'eorie statistique 1}
\begin{center}{\large Chair/Président: Jianfeng Yao (University of Hong Kong)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Welch 8G\end{center}
\label{abs-sid:st}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk st-hd
\def\whenwhere{[Monday May 30 / lundi 30 mai, 13:30]\\}
{\Author{Haosui}{Duanmu}{duanmu}{1D-C4}\Author{Daniel}{Roy}{roy}{1D-C4}%
\abshead{\absauthor{HAOSUI DUANMU} \& \absauthor{DANIEL ROY}\absaffil{University of Toronto}}
        {Nonstandard Complete Class Theorem\newline
        Théorème de classe complète non standard}

\absSideBySide{For finite parameter spaces under finite loss, there is a close link between optimal frequentist decision procedures and Bayesian procedures: every admissible procedure is Bayes. Using nonstandard analysis, we introduce the notion of a hyperfinite statistical decision problem and study the class of nonstandard Bayesian decision procedure-namely, those whose average risk with respect to some prior is within an infinitesimal of the optimal Bayes risk. We give some sufficient regularity conditions on standard statistical decision problems that imply every admissible procedure is nonstandard Bayes, and conditions such that nonstandard Bayes procedures are in fact Bayes ones.
}{Pour les espaces de paramètres finis sous la perte finie, il existe un lien étroit entre les procédures de décision fréquentistes optimales et les procédures bayésiennes : chaque procédure admissible est bayésienne. En utilisant une analyse non standard, nous introduisons la notion d'un problème statistique de décision hyperfini et nous étudions la classe de la procédure de décision bayésienne non standard, à savoir celle dont le risque moyen préalable est dans une infinitésimale du risque optimal de Bayes. Nous donnons des conditions suffisantes de régularité sur les problèmes de décision statistique standard qui impliquent que chaque procédure admissible est non standard Bayes et des conditions telles que les procédures de Bayes non standards sont en fait celles de Bayes.
}}
%% Talk st-rkk
\def\whenwhere{[Monday May 30 / lundi 30 mai, 13:45]\\}
{\Author{Romain Kadje}{Kenmogne}{kenmogne}{1D-C4}\Author{François}{Perron}{perron}{1D-C4}%
\abshead{\absauthor{ROMAIN KADJE KENMOGNE} \& \absauthor{FRANÇOIS PERRON}\absaffil{Université de Montréal}}
        {Density of the Ratio of Two Normal Random Variables\newline
        Densité du rapport de deux variables normalement distribuées}

\absSideBySide{The random vector $(X, Y)$ has a multinormal distribution and we are looking for the distribution of the ratio $X / Y$. This issue was addressed in a long paper (T. PHAM GIA et al.) often cited on Google Scholar. 
The purpose of this work is to significantly reduce the length of the proofs and to explain how to numerically solve estimation problems in a Bayesian setting. 
We show that the density of $X / Y$ is a mixture of new densities belonging to a family. We discuss the properties of this family. Some convergence results are given.
}{Le vecteur aléatoire $(X,Y)$ est de loi multinormale et on cherche la loi du rapport $X / Y$. Ce sujet a été abordé dans un long article (T. PHAM-GIA et coll.) souvent cité sur Google Scholar. 
Le but de ce travail est de raccourcir considérablement la longueur des démonstrations et d'expliquer comment résoudre numériquement des problèmes d'estimation dans un cadre bayésien. 
Nous montrons que la densité de $X / Y$ est un mélange de nouvelles densités appartenant à une certaine famille. Nous discutons des propriétés de cette famille. Des résultats de convergence sont donnés.
}}
%% Talk st-fcl
\def\whenwhere{[Monday May 30 / lundi 30 mai, 14:00]\\}
{\Author{Félix Camirand}{Lemyre}{lemyre}{1D-C4}\Author{Taoufik}{Bouezmarni}{bouezmarni}{1D-C4}\Author{Jean-François}{Quessy}{quessy}{1D-C4}%
\abshead{\absauthor{FÉLIX CAMIRAND LEMYRE} \& \absauthor{TAOUFIK BOUEZMARNI}\absaffil{Université de Sherbrooke}, \absauthor{JEAN-FRANÇOIS QUESSY}\absaffil{Université du Québec à Trois-Rivières}}
        {Nonparametric Measures of Local Causality and Tests of Non-Causality in Time Series\newline
        Mesures non paramétriques de la causalité locale et tests de non causalité pour des séries temporelles}

\absSideBySide{To study the causal relationships in a process $(Y_t,Z_t)_{t\in \mathbb{Z}}$, a widely-used approach is to consider the Granger causality. In the case of Markovian processes, the notion is based on the joint distribution of $(Y_t,Z_{t-1})$ given $Y_{t-1}$. The Granger causality measures proposed so far are global, which means that if the relationship between $Y_t$ and $Z_{t-1}$ changes with the value of $Y_{t-1}$, this will not be captured. To circumvent this limitation,  \textit{local} Granger causality indices based on the conditional copula of $(Y_t,Z_{t-1})$ given $Y_{t-1}=x$ are here proposed, and the asymptotic behavior of nonparametric estimators is obtained for $\alpha$-mixing processes.
}{Pour étudier la dépendance entre $(Y_t,Z_t)_{t \in \mathbb{Z}}$, on s'intéresse souvent à la causalité de Granger. Dans le cas de processus Markoviens, cette notion se base sur la distribution de $(Y_t,Z_{t-1})$ sachant $Y_{t-1}$. À ce jour, les mesures de la causalité de Granger sont globales, en ce sens où si la dépendance entre $Y_t$ et $Z_{t-1}$ change en fonction des valeurs de $Y_{t-1}$, alors ces changements ne pourront être détectés. Pour contourner ce problème, des mesures de causalité basées sur la copule conditionnelle de $(Y_t,Z_{t-1})$ sachant $Y_{t-1}=x$ seront ici proposées, et leur normalité asymptotique est établie pour des processus $\alpha$-mélangeants.
}}
%% Talk st-vv
\def\whenwhere{[Monday May 30 / lundi 30 mai, 14:15]\\}
{\Author{Victor}{Veitch}{veitch}{1D-C4}\Author{Daniel}{Roy}{roy}{1D-C4}%
\abshead{\absauthor{VICTOR VEITCH} \& \absauthor{DANIEL ROY}\absaffil{University of Toronto}}
        {Models and Inference for Sparse Random Graphs Using Exchangeable Random Measures\newline
        Modèles et inférence pour des graphes aléatoires épars à l'aide de mesures aléatoires échangeables}

\absSideBySide{We introduce a class of random graphs that meets many of the desiderata for a foundation for statistical analysis of real-world networks. The class of random graphs is defined by a probabilistic symmetry: invariance of the distribution of each graph to an arbitrary relabelings of its vertices. We interpret a symmetric simple point process on $\mathbb{R}_+^2$ as the edge set of a random graph, and formalize the probabilistic symmetry as joint exchangeability of the point process. We give a representation theorem for the class of random graphs satisfying this symmetry.
}{Nous présentons une classe de graphes aléatoires qui répond à bon nombre des désidératas pour établir le fondement d'une analyse statistique des réseaux réels. La classe de graphes aléatoires se définit par une symétrie probabiliste~: l'invariance de la distribution de chaque graphe au réétiquettage arbitraire de ses sommets. Nous interprétons un processus ponctuel symétrique sur $\mathbb{R}_+^2$ comme l'ensemble des arêtes d'un graphe aléatoire et formalisons la symétrie probabiliste comme l'échangeabilité conjointe du processus ponctuel. Nous proposons un théorème de représentation pour la classe de graphes aléatoires qui répond à cette symétrie.
}}
%% Talk st-jy
\def\whenwhere{[Monday May 30 / lundi 30 mai, 14:30]\\}
{\Author{Jun}{Yang}{yang}{1D-C4}\Author{Daniel}{Roy}{roy}{1D-C4}%
\abshead{\absauthor{JUN YANG} \& \absauthor{DANIEL ROY}\absaffil{University of Toronto}}
        {Meta-Bayesian Analysis\newline
        Méta-analyse bayésienne}

\absSideBySide{The ``optimality'' of the Bayesian approach to inference does not hold when the model is misspecified. As essentially every statistical model is misspecified, this raises the question: what is a prior? We formalize the problem of choosing a (surrogate) prior as a Bayesian decision theory task, and develop theory and algorithms for choosing optimal priors. The resulting theory, which we call meta-Bayesian analysis, gives priors a pragmatic interpretation and has some surprising consequences. For example, the optimal prior may depend on the number of data points you plan to observe, and the number of predictions you expect to make.
}{Le caractère optimal de l'approche bayésienne à l'inférence ne tient pas la route en cas d'erreur de spécification du modèle. Comme la spécification de tout modèle statistique est essentiellement erronée, une question se pose~: qu'est-ce qu'une loi a priori? Nous formalisons le problème du choix d'une loi a priori (de substitution) comme une tâche de théorie de décision bayésienne et mettons au point une théorie et des algorithmes pour le choix de lois a priori optimales. Ladite théorie que nous appelons méta-analyse bayésienne donne une interprétation pragmatique aux lois a priori et mène à certaines conséquences étonnantes. Par exemple, une loi a priori optimale peut dépendre du nombre de données que vous prévoyez observer et des prévisions que vous comptez faire.
}}
%% Talk st-zz
\def\whenwhere{[Monday May 30 / lundi 30 mai, 14:45]\\}
{\Author{Zhiyang}{Zhou}{zhou}{1D-C4}\Author{Runchu}{Zhang}{zhang}{1D-C4}%
\abshead{\absauthor{ZHIYANG ZHOU}\absaffil{Simon Fraser University}, \absauthor{RUNCHU ZHANG}\absaffil{Nankai University and Northeast Normal University}}
        {A Generalized General Minimum Lower Order Confounding Criterion for Non-regular Designs\newline
        Critère généralisé de l'amalgame général minimal d'ordre inférieur pour des plans non réguliers}

\absSideBySide{We extend the work of Zhang et al. [Statistica Sinica 18, 1689--1705] for non-regular designs and propose two new concepts, i.e., the generalized aliasing effect-number pattern (\textit{G}$_2$-AENP) and generalized general minimum lower order confounding (\textit{G}$_2$-GMC). It proves that (i) isomorphic designs have the same \textit{G}$_2$-AENP and (ii) the generalized minimum aberration (GMA) and minimum moment aberration (MMA) can both be treated as ones that optimize functions over the \textit{G}$_2$-AENP. That is, the \textit{G}$_2$-GMC criterion is more sensitive in the recognition of non-isomorphic designs.
}{Nous généralisons les travaux de Zhang et coll. [Statistica Sinica 18, 1689--1705] pour les plans non réguliers et proposons deux nouveaux concepts, c.-à-d. la suite numérique à effet de repliement de spectre généralisé (\textit{G}$_2$-AENP) et l'amalgame général minimal d'ordre inférieur (\textit{G}$_2$-GMC). Cela prouve (i) que les concepts isomorphiques ont le même \textit{G}$_2$-AENP et (ii) que l'aberration minimale généralisée (AMG) et l'aberration minimale de moments (AMM) peuvent toutes deux être traitées comme celles qui optimisent les fonctions sur le\textit{G}$_2$-AENP. En fait, le critère \textit{G}$_2$-GMC est plus sensible dans la reconnaissance de plans non isomorphiques.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 1D-C5: Statistics Education\\\'Education en statistique}
\begin{center}{\large Chair/Président: Alison Gibbs (University of Toronto)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 256\end{center}
\label{abs-sid:se}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk se-jb
\def\whenwhere{[Monday May 30 / lundi 30 mai, 13:30]\\}
{\Author{Jeremy}{Balka}{balka}{1D-C5}%
\abshead{\absauthor{JEREMY BALKA}\absaffil{University of Guelph}}
        {A Traditional Introductory Statistics Course with Extensive Video Support\newline
        Un cours d'introduction aux statistiques traditionnelles avec un vaste support vidéo}

\absSideBySide{In an introductory statistics course, I supply complete course notes, more than 100 accompanying videos, and exercise sets with full solutions.  In a sense, the 
course has developed into an online course with lectures as a bonus.  In this talk I will discuss my motivation for setting up the course in this way, some of the 
pros and cons of this approach, and some of my experiences teaching the course.
}{Dans un cours d'introduction aux statistiques, je donne des notes de cours complètes, plus de 100 vidéos d'accompagnement, et des ensembles d'exercices avec solutions 
complètes. En un sens, le cours est devenu un cours en ligne avec conférences en prime. Dans cette présentation, je discuterai de la raison pour laquelle j'ai créé le 
cours de cette façon, quelques pour et contre de cette approche, et un peu de mon expérience lors de l'enseignement de ce cours.
}}
%% Talk se-db
\def\whenwhere{[Monday May 30 / lundi 30 mai, 13:45]\\}
{\Author{Dunham}{Bruce}{bruce}{1D-C5}\Author{Melissa}{Lee}{lee}{1D-C5}\Author{Gaitri}{Yapa}{yapa}{1D-C5}%
\abshead{\absauthor{DUNHAM BRUCE}, \absauthor{MELISSA LEE} \& \absauthor{GAITRI YAPA}\absaffil{University of British Columbia}}
        {Investigating How Students Interact with Simulation-based Applets\newline
        Étude de la manière dont les étudiants interagissent avec les applets basés sur la simulation}

\absSideBySide{The use of applets in helping students grasp statistical concepts has attracted much interest in the statistical education community. There is evidence such tools may improve student learning, but there is little research on how students engage with applets or how they should be designed. As part of the development of a set of new applets, we have interviewed students and observed how they interact with the applets. Suggestions for how to conduct such research will be discussed, along with some provisional findings.
}{L'utilisation d'applets pour aider les étudiants à saisir les concepts statistiques a suscité beaucoup d'intérêt dans le milieu de l'éducation en statistique. Il existe des preuves que de tels outils peuvent améliorer l'apprentissage des élèves, mais il existe peu de recherches sur la façon dont les élèves participent avec les applets ou comment ceux-ci doivent être conçus. Dans le cadre de l'élaboration d'un ensemble de nouveaux applets, nous avons interrogé les étudiants et observé comment ils interagissaient avec les applets. Des suggestions sur la façon de mener une telle recherche seront discutées, avec quelques conclusions provisoires.
}}
%% Talk se-sk
\def\whenwhere{[Monday May 30 / lundi 30 mai, 14:00]\\}
{\Author{Sohee}{Kang}{kang}{1D-C5}%
\abshead{\absauthor{SOHEE KANG}\absaffil{University of Toronto Scarborough}}
        {Project-Based Statistics Learning Through Wikispace\newline
        Apprentissage des statistiques fondé sur le projet à l'aide de Wikispace}

\absSideBySide{``Learn by doing'' is the type of experience that educators strive to facilitate for students. Project based learning is an instructional strategy in which students work cooperatively to create a product.
A final project in upper year statistics courses provides students with opportunities to build 21st century learning skills such as collaboration, communication, critical thinking, and the use of technology.  In this talk, I share the classroom experience of using ``Wikispace'' to create a wiki-like page as a final product for my fourth year course, Multivariate Analysis.  How to create and maintain projects will be demonstrated along with students' products.
}{« Apprendre en faisant » est le type d'expérience que les éducateurs recherchent pour aider les étudiants. L'apprentissage fondé sur le projet est une stratégie d'enseignement dans laquelle les étudiants travaillent en collaboration pour créer un produit.
Un projet final dans les cours de statistique de dernière année fournit aux élèves des occasions d'acquérir des compétences d'apprentissage du 21e siècle telles que la collaboration, la communication, la pensée critique et l'utilisation de la technologie. Dans cet exposé, je partage l'expérience en classe de l'utilisation de « Wikispace » pour créer une page wiki-like comme produit final de mon cours de quatrième année sur l'analyse multivariée. Nous montrerons comment créer et maintenir des projets ainsi que les produits des élèves.
}}
%% Talk se-amm
\def\whenwhere{[Monday May 30 / lundi 30 mai, 14:15]\\}
{\Author{Anne Michele}{Millar}{millar}{1D-C5}%
\abshead{\absauthor{ANNE MICHELE MILLAR}\absaffil{Mount Saint Vincent University}}
        {P-values: Love them -  Hate them - Leave them. Implications for Statistical Education.\newline
        Valeurs de p : aimez-les - détestez-les - laissez-les. Implications en éducation en statistique}

\absSideBySide{The vast majority of our introductory statistics text books teach significance tests as the definitive method for statistical inference. In light of the ban on p-values by some journals, and the ASA's recent statement on p-values, should we be updating this approach? Great strides have been made in statistical education, stressing the teaching of concepts rather than calculations, introducing students to real world data from the beginning of the course. Many of our students still leave the first course with very fuzzy ideas about inference. We rarely see non-frequentist approaches taught in our introductory courses. Is it time for change?
}{La grande majorité de nos manuels d'introduction à la statistique enseignent les tests de signification en tant que méthode définitive pour l'inférence statistique. À la lumière de l'interdiction sur les valeurs p par certains journaux et de la récente déclaration de l'ASA sur les valeurs p, devrions-nous mettre à jour cette approche ? De grands progrès ont été réalisés en éducation en statistique, en insistant sur l'enseignement des concepts plutôt que sur les calculs, en initiant les élèves aux données du monde réel au début du cours. Un grand nombre de nos étudiants quittent encore le premier cours avec des idées très floues au sujet de l'inférence. Nous voyons rarement des approches non-fréquentistes enseignées dans nos cours d'introduction. Le temps est-il venu pour un changement ?
}}
%% Talk se-lm
\def\whenwhere{[Monday May 30 / lundi 30 mai, 14:30]\\}
{\Author{Lori}{Murray}{murray}{1D-C5}%
\abshead{\absauthor{LORI MURRAY}\absaffil{University of Western Ontario}}
        {Five Years of Blending\newline
        Cinq années de mélange}

\absSideBySide{A blended course format is a combination of in-class and online learning. Although the unique format has been in use for several years, for many it is still a new concept. We have been using blended learning in an introductory statistics course at the University of Western Ontario since 2012. In this talk, I will give my experience as the instructor, highlighting some of the advantages and pitfalls that I have encountered along the way.
}{Un format de cours hybride est un mélange d'apprentissage en classe et en ligne. Pour beaucoup d'entre nous, c'est encore un nouveau concept, bien que ce format unique ait été utilisé depuis plusieurs années. Depuis 2012, nous avons donné un enseignement hybride dans un cours d'introduction à la statistique à l'Université de Western Ontario. Dans cet exposé, je vous ferai part de mon expérience à titre d'enseignant et je décrirai certains des avantages et des inconvénients de ce genre d'enseignement.
}}
%% Talk se-nt
\def\whenwhere{[Monday May 30 / lundi 30 mai, 14:45]\\}
{\Author{Nathan}{Taback}{taback}{1D-C5}\Author{Alison}{Gibbs}{gibbs}{1D-C5}%
\abshead{\absauthor{NATHAN TABACK} \& \absauthor{ALISON GIBBS}\absaffil{University of Toronto}}
        {Students' Attitudes Towards Statistics in Online versus Flipped Classrooms\newline
        L'attitude des étudiants à l'égard des statistiques dans les cours en ligne par rapport aux classes inversées}

\absSideBySide{Technology has allowed instructors to experiment with different course delivery methods including flipped and fully online courses.  In the Fall of 2015 at the University of Toronto, our large (N=1,413) introductory course was taught in both flipped and fully online formats, with some discipline-specific sections. To investigate if students' attitudes towards statistics differ depending on course delivery method, students were asked to complete  pre and post course surveys  (SATS-36) measuring attitudes towards statistics. We'll report on a comparison of attitude components between flipped and online sections, and discipline and non-discipline specific sections of the course, adjusted for baseline student covariates.
}{La technologie a permis aux enseignants d'essayer diverses méthodes de prestation de cours, y compris les cours en ligne et les classes inversées. À l'automne de 2015 à l'Université de Toronto, notre volumineux cours d'introduction (N=1413) a été donné dans l'un et l'autre format, avec toutefois des sections propres à la discipline. Pour savoir si l'attitude des étudiants à l'égard des statistiques différait en fonction de la méthode d'enseignement, des sondages préalables et subséquents au cours ont été menés auprès d'eux (SATS-36) afin de mesurer ce facteur. Nous ferons état d'une comparaison des aspects de leur attitude en fonction des sections enseignées en classe inversée et dans le cours en ligne ainsi qu'entre les sections propres ou non à la discipline, avec ajustement pour les covariables de base des étudiants.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 1E-I1: Advances in Longitudinal Data Analysis\\Progr\`es r\'ecents en analyse de donn\'ees longitudinales}
\begin{center}{\large Chair/Président: Yogendra Chaubey (Concordia University)\protect\\[5pt]
Organizer/Responsable: Sanjoy Sinha (Carleton University)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Welch 209\end{center}
\label{abs-sid:ald}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk ald-kd
\def\whenwhere{[Monday May 30 / lundi 30 mai, 15:30]\\}
{\Author{Karelyn}{Davis}{davis}{1E-I1}%
\abshead{\absauthor{KARELYN DAVIS}\absaffil{Health Canada}}
        {Longitudinal Analysis in Health Studies with Censored Observations\newline
        Analyse longitudinale d'études sur la santé avec observations censurées}

\absSideBySide{In performing statistical analysis of national health studies, statisticians often encounter data that is correlated either due to a natural clustering or 
is collected longitudinally. In particular, when concerned with contaminant levels in humans (e.g. mercury, lead), laboratory data may be collected in 
such a fashion so as to determine significant patterns in vulnerable populations such as pregnant women and children. However, such contaminant 
levels may be so low as to be below the laboratory limit of detection and are thus classified as non-detects. In this presentation, analyses encountered 
from a Health Canada study are discussed which extend linear mixed models to incorporate parameter constraints and analyses of left-censored observations.
}{Lors d'analyses statistiques d'études nationales sur la santé, les statisticiens font souvent face à des données qui sont corrélées soit en raison d'une 
classification naturelle ou d'une collecte longitudinale. Notamment, lorsque préoccupés par les concentrations de contaminants chez les humains 
(par exemple, mercure, plomb), les données de laboratoire peuvent être recueillies de manière à déterminer des tendances significatives chez les groupes plus 
vulnérables tels que les femmes enceintes et les enfants. Par contre, ces concentrations de contaminants peuvent être assez faibles pour être sous la limite 
de détection du laboratoire et ainsi être classifiées en tant que concentration non détectée. Lors de cette présentation, des analyses provenant d'une étude de 
Santé Canada seront discutées. Ces analyses élargissent les modèles linéaires mixtes pour incorporer des contraintes de paramètres et des analyses d'observations 
censurées à gauche.
}}
%% Talk ald-as
\def\whenwhere{[Monday May 30 / lundi 30 mai, 16:00]\\}
{\Author{Abdus}{Sattar}{sattar}{1E-I1}\Author{Sanjoy}{Sinha}{sinha}{1E-I1}%
\abshead{\absauthor{ABDUS SATTAR}\absaffil{Case Western Reserve University}, \absauthor{SANJOY SINHA}\absaffil{Carleton University}}
        {Joint Modeling of Longitudinal and Survival Data With a Covariate Subject to Limit of Detection\newline
        Modélisation conjointe de données longitudinales de survie avec une covariable soumise à une limite de détection}

\absSideBySide{We develop and study innovative methods for jointly modeling longitudinal and time-to-event data with a covariate or biomarker subject to the limit of detection 
(LOD). We investigate the gain in efficiency from joint models over separate models when we attempt to estimate the effect of an endogenous time-dependent 
covariate on survival times. We also investigate the effects of misspecified random effects distributions on the likelihood inference. Our extensive simulation 
study indicates that if the assumed random effects distributions deviate from the true distributions to a large extent, the maximum likelihood method can produce 
systematically biased estimators. We present an application of the proposed method using a large clinical trial data from the Genetic and Inflammatory Markers of 
Sepsis (GenIMS) study.
}{Nous développons et étudions des méthodes novatrices pour modéliser conjointement des données longitudinales et de temps d'événement avec une covariable ou un 
biomarqueur soumis à une limite de détection (LOD). Nous examinons le gain en efficacité des modèles conjoints sur les modèles séparés lorsque nous tentons d'estimer 
l'effet d'une covariable endogène avec une dépendance temporelle sur les temps de survie. Nous examinons aussi les effets des lois des effets aléatoires mal spécifiés 
sur l'inférence de vraisemblance. Notre vaste étude de simulation indique que si la loi des effets aléatoires présumée dévie en grande partie de la vraie loi, la 
méthode de maximum de vraisemblance peut produire systématiquement des estimateurs biaisés. Nous présentons une application de la méthode proposée avec des données 
d'un vaste essai clinique de l'étude sur les marqueurs génétiques et inflammatoires du sepsis (GenIMS).
}}
%% Talk ald-ss
\def\whenwhere{[Monday May 30 / lundi 30 mai, 16:30]\\}
{\Author{Sanjoy}{Sinha}{sinha}{1E-I1}%
\abshead{\absauthor{SANJOY SINHA}\absaffil{Carleton University}}
        {Methods for Longitudinal Data With Nonignorable Missing Responses and Outliers\newline
        Méthodes pour données longitudinales avec réponses manquantes et valeurs aberrantes ne pouvant être ignorées}

\absSideBySide{We encounter missing data in many longitudinal studies. When data are nonignorably missing, it is important to analyze the data by incorporating a missing data 
model into the observed data likelihood function. It is well-known that the maximum likelihood (ML) estimators are generally sensitive to potential outliers in 
the data. I propose and explore a robust method, which is developed in the framework of the ML estimation and is useful for downweighting any influential 
observations when estimating the model parameters. The empirical properties of the robust estimators are studied in simulations. The proposed method is also 
illustrated in an example using actual longitudinal data on CD4 counts obtained from clinical trials of HIV-infected patients.
}{Plusieurs études longitudinales contiennent des données manquantes. Lorsque la non-réponse est non-ignorable, il est important d'analyser les données 
en incorporant un modèle de données manquantes dans la fonction de vraisemblance des données observées. Il est bien connu que les estimateurs de maximum de 
vraisemblance (ML) sont habituellement sensibles aux valeurs aberrantes potentielles dans les données. Je propose et explore une méthode robuste qui est 
développée selon le cadre de l'estimation ML et qui est utile pour réduire la pondération d'observations qui ont de l'influence lors de l'estimation des paramètres 
du modèle. Les propriétés empiriques des estimateurs robustes sont étudiées dans des simulations. La méthode proposée est aussi illustrée au moyen d'un
exemple utilisant des données longitudinales réelles sur le CD4 provenant d'essais cliniques sur des patients infectés par le VIH.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 1E-I2: Advances in Regression Tree Modelling\\Progr\`es r\'ecents en mod\'elisation par les arbres de r\'egression}
\begin{center}{\large Organizer and Chair / Responsable et président:  Matthew Pratola (Ohio State University)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 242\end{center}
\label{abs-sid:art}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk art-hc
\def\whenwhere{[Monday May 30 / lundi 30 mai, 15:30]\\}
{\Author{Hugh}{Chipman}{chipman}{1E-I2}\Author{Robert}{McCulloch}{mcculloch}{1E-I2}\Author{Matthew}{Pratola}{pratola}{1E-I2}\Author{Edward}{George}{george}{1E-I2}%
\abshead{\absauthor{HUGH CHIPMAN}\absaffil{Acadia University}, \absauthor{ROBERT MCCULLOCH}\absaffil{University of Chicago}, \absauthor{MATTHEW PRATOLA}\absaffil{Ohio State University}, \absauthor{EDWARD GEORGE}\absaffil{University of Pennsylvania}}
        {Dispersion Modelling with an Ensemble of Trees\newline
        Modèle de dispersion avec ensemble d'arbres}

\absSideBySide{Bayesian additive regression trees (BART) is a flexible and scalable supervised learning model that offers accurate assessment of uncertainty via credible intervals.
 It makes the strong assumption of iid errors. Even when error variance is nonconstant, BART can still give accurate point predictions. However, credible intervals 
are unlikely to remain accurate or useful.  We develop a novel heteroscedastic BART model to alleviate these concerns. This is achieved through the introduction of 
Bayesian Multiplicative Trees, which model the variance component of BART as a function of the predictors. We implement the approach and demonstrate it in several 
examples.
}{Les arbres additifs de régression bayésiennes (BART) forment un modèle d'apprentissage supervisé flexible et évolutif qui offre une évaluation précise de l'incertitude 
via des intervalles de crédibilité. Ce modèle se base sur la forte hypothèse des erreurs iid.  Même lorsque l'erreur de la variance est non constante, BART peut donner des 
prédictions ponctuelles précises. Par contre, il est improbable que les intervalles de crédibilité demeurent précis ou utiles. Nous développons un nouveau modèle 
BART hétéroscédastique pour dissiper ces préoccupations. Cela est possible par l'introduction d'arbres multiplicatifs bayésiens, qui modélisent la composante de 
variance en tant que fonction des prédicteurs. Nous employons et démontrons cette approche à travers plusieurs exemples.
}}
%% Talk art-tl
\def\whenwhere{[Monday May 30 / lundi 30 mai, 16:00]\\}
{\Author{Tom}{Loughin}{loughin}{1E-I2}\Author{Andrew}{Henrey}{henrey}{1E-I2}%
\abshead{\absauthor{TOM LOUGHIN} \& \absauthor{ANDREW HENREY}\absaffil{Simon Fraser University}}
        {Robust Adaptively Pruned Random Forests Using Likelihood-Based Trees\newline
        Forêts aléatoires adaptatives élaguées et robustes utilisant des arbres fondés sur la vraisemblance}

\absSideBySide{Random Forests for regression are typically constructed using standard regression trees. These trees make splits that minimize squared error, which implicitly 
assumes homoscedasticity. They are not robust against heteroscedasticity, and this can be passed on to the forests.  As heteroscedasticity is prevalent in a 
substantial fraction of real datasets, random forests may underperform frequently. Furthermore, they are inefficient at fitting mean functions that are partially 
flat.  We present a likelihood-based version of regression trees that explicitly model both the mean and variance, and use these trees as base learners in our 
random forest. We also develop a fast pruning algorithm based on information criteria that improves the fit to partially flat mean functions.
}{Les forêts aléatoires de régression sont généralement construites à l'aide d'arbres de régression standards. Ces arbres font des divisions qui minimisent l'erreur carrée, 
ce qui suppose implicitement une homoscédasticité. Ils ne sont pas robustes en présence d'hétéroscédasticité, ce qui peut se transférer aux forêts. Comme l'hétéroscédasticité 
est courante dans une part substantielle des données réelles, les forêts aléatoires peuvent fréquemment être peu efficaces. De plus, ils sont inefficaces pour ajuster 
les fonctions moyennes qui sont partiellement planes. Nous présentons une version des arbres de régression basée sur la vraisemblance qui modélise 
explicitement à la fois la moyenne et la variance, et qui utilise ces arbres comme base d'apprentissage dans notre forêt aléatoire. Nous développons aussi un 
algorithme rapide d'élagage fondé sur des critères d'information qui améliore l'ajustement avec les fonctions moyennes partiellement planes.
}}
%% Talk art-dr
\def\whenwhere{[Monday May 30 / lundi 30 mai, 16:30]\\}
{\Author{Daniel}{Roy}{roy}{1E-I2}%
\abshead{\absauthor{DANIEL ROY}\absaffil{University of Toronto}}
        {Mondrian Processes and their Statistical Applications\newline
        Les processus Mondrian et leurs applications statistiques}

\absSideBySide{Mondrian processes are a class of continuous time stochastic processes that induce a random hierarchical partition of a product space.  In this talk, I will survey 
Mondrian processes and their application to a number of statistical problems, including network modeling and efficient online classification and regression via 
Mondrian forests.
}{Les processus Mondrian sont une classe de processus stochastiques en temps continu qui induisent une partition aléatoire hiérarchique d'un espace produit. Durant cet
exposé, je passerai en revue les processus Mondrian et leurs applications à un certain nombre de problèmes statistiques, y compris les modèles de réseau et la 
classification efficace en ligne et la régression via les forêts Mondrian.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 1E-I3: An Introduction to the Joint Modelling of Longitudinal and Survival Data\\Introduction \`a la mod\'elisation conjointe des donn\'ees longitudinales et de survie}
\begin{center}{\large Organizer and Chair / Responsable et président:  Fernando Camacho (Damos Inc.)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Welch - David Howes\end{center}
\label{abs-sid:aij}
\begin{center}{\large\bfseries Abstract/R\'esum\'e}
\end{center}
%% Talk aij-gy
\def\whenwhere{[Monday May 30 / lundi 30 mai, 15:30]\\}
{\Author{Grace Y.}{Yi}{yi}{1E-I3}%
\abshead{\absauthor{GRACE Y. YI}\absaffil{University of Waterloo}}
        {Introduction of Joint Modeling of Longitudinal and Survival Data\newline
        Présentation d'une modélisation conjointe de données longitudinales et de survie}

\absSideBySide{Longitudinal data and survival data frequently arise together in biomedical studies, clinical trials and observational studies. These data are often associated. Separate analyses of longitudinal data and survival data may lead to inefficient or biased results. Joint models of longitudinal and survival data, on the other hand, incorporate all information simultaneously, and provide valid and efficient inferences. Over the last two decades, joint modeling of longitudinal and survival data has received increasing attention. A number of modeling strategies and inferential procedures have been developed in the literature. In this talk, I will give an introductory overview and discuss a broad range of issues on joint modeling.
}{Les données longitudinales et les données de survie apparaissent souvent ensemble dans les études biochimiques, les essais cliniques et les études d'observation, et sont souvent associées. Les analyses séparées de données longitudinales et de données de survie pourraient causer des estimations inefficaces ou biaisées. D'autre part, les modèles conjoints de données longitudinales et de données de survie intègrent toute l'information simultanément et fournissent des inférences valides et efficaces. Au cours des vingt dernières années, la modélisation conjointe de données longitudinales et de données de survie a suscité une attention croissante. Un certain nombre de stratégies et de procédures d'inférence ont été conçues dans la littérature. Dans cet exposé, je donnerai un aperçu et je discuterai des divers problèmes liés à la modélisation conjointe de ce type de données.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 1E-I4: Missing Data in Survey Sampling\\Donn\'ees manquantes en \'echantillonnage}
\begin{center}{\large Chair/Président: Michael Hidiroglou (Statistics Canada)\protect\\[5pt]
Organizer/Responsable: Wes Yung (Statistics Canada)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 258\end{center}
\label{abs-sid:mds}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk mds-sc
\def\whenwhere{[Monday May 30 / lundi 30 mai, 15:30]\\}
{\Author{Song}{Cai}{cai}{1E-I4}\Author{J.N.K.}{Rao}{rao}{1E-I4}%
\abshead{\absauthor{SONG CAI} \& \absauthor{J.N.K. RAO}\absaffil{Carleton University}}
        {Empirical Likelihood Inference Based on Estimation Equations for Complex Survey with Data Missing Completely At Random\newline
        Inférence de vraisemblance empirique fondée sur les équations d'estimation pour sondage complexe avec données manquantes aléatoirement}

\absSideBySide{We propose an empirical likelihood (EL) method for constructing confidence intervals for population parameters defined by estimation equations (EEs) with 
data from complex surveys that are missing completely at random. Instead of imputing the original data, we impute the EEs using fractional imputation with 
fixed number of draws. We then construct an EL ratio based on the imputed EEs and show that this ratio has a chi-bar-square limiting distribution in general. 
We also show that, when no missing data are present, the proposed EL ratio has a simple chi-square limiting distribution under PPS sampling with replacement. 
Moreover, we propose a proper bootstrap procedure to approximate the limiting distribution of the EL ratio for constructing confidence intervals for parameters 
of interest.
}{Nous proposons une méthode de vraisemblance empirique (EL) pour la construction d'intervalles de confiance pour des paramètres d'une population définis par 
des équations d'estimation (EEs) avec des données manquantes aléatoirement provenant de sondages complexes. Au lieu d'imputer les données originales, nous imputons
les EEs en utilisant une imputation fractionnaire avec un nombre fixé de tirages. Nous construisons ensuite un rapport EL basé sur les EEs attribués et démontrons 
que ce rapport a généralement une loi limite khi bar carré. Nous démontrons aussi que, lorsqu'il n'y a aucune donnée manquante, le rapport EL proposé a une simple 
loi limite khi carré selon l'échantillonnage PPT avec remise. De plus, nous proposons une procédure bootstrap pour approximer la loi limite du rapport EL 
pour la construction d'intervalles de confiance pour des paramètres d'intérêt.
}}
%% Talk mds-dh
\def\whenwhere{[Monday May 30 / lundi 30 mai, 16:00]\\}
{\Author{David}{Haziza}{haziza}{1E-I4}\Author{Éric}{Lesage}{lesage}{1E-I4}%
\abshead{\absauthor{DAVID HAZIZA}\absaffil{Université de Montréal}, \absauthor{ÉRIC LESAGE}\absaffil{INSEE}}
        {Properties of the Instrument Vector Calibration Estimator in the Presence of Unit Nonresponse\newline
        Propriétés de l'estimateur par calage instrumental en présence de non-réponse totale}

\absSideBySide{In recent years, instrument vector calibration has received a lot of attention in the literature in the context of unit nonresponse in surveys.  In this paper, 
we lay out the conditions under which the instrument vector calibration estimator is consistent. Even when the conditions are met, the resulting estimator may 
suffer from variance amplification. When the conditions are not met, the estimator may suffer from both  bias and variance amplification. Results from a  
simulation study will be presented.
}{Ces dernières années, le calage instrumental a reçu beaucoup d'attention dans la littérature dans le contexte de la non-réponse totale dans les enquêtes. 
Dans cet article, nous exposons les conditions selon lesquelles l'estimateur par calage instrumental est convergent. Même lorsque les conditions 
sont remplies, l'estimateur résultant peut subir une amplification de la variance. Lorsque les conditions ne sont pas remplies, l'estimateur peut souffrir d'une amplification
du biais et d'une amplification de la variance. Des résultats provenant d'une étude de simulation seront présentés.
}}
%% Talk mds-xs
\def\whenwhere{[Monday May 30 / lundi 30 mai, 16:30]\\}
{\Author{Xichen}{She}{she}{1E-I4}\Author{Changbao}{Wu}{wu}{1E-I4}%
\abshead{\absauthor{XICHEN SHE} \& \absauthor{CHANGBAO WU}\absaffil{University of Waterloo}}
        {Fully Efficient Joint Fractional Imputation for Incomplete Bivariate Ordinal Responses\newline
        Imputation fractionnaire conjointe pleinement efficace pour des réponses bivariées ordinales incomplètes}

\absSideBySide{In medical studies and many fields of social sciences, pairwise ordinal responses are one of the common data formats collected for analysis. It is often that one 
or both of the responses are subject to missingness. In this paper, we propose a fully efficient joint fractional imputation procedure, which creates a single 
complete dataset with fractional weights. We show that this synthetic dataset not only leads to valid inference for simple parameters such as marginal proportions 
but also preserves the correlation structure, which further leads to valid association measure estimation. Theoretical results on inference procedures based on 
the fractionally imputed dataset are presented, and finite sample performances of the methods with comparisons to existing methods are investigated through 
simulation studies.
}{Dans les études médicales ainsi que dans plusieurs domaines des sciences sociales, les réponses ordinales par paires sont un des formats de données communs recueillis 
pour analyse. Fréquemment, une réponse ou même parfois les deux réponses sont manquantes. Dans cet article, nous proposons une procédure d'imputation 
fractionnaire conjointe pleinement efficace qui crée un ensemble de données complet unique avec des pondérations fractionnaires. Nous démontrons que cet ensemble de 
données synthétique ne conduit pas seulement à une inférence valide pour des paramètres simples tels que des proportions marginales mais préserve également la structure 
de la corrélation, ce qui conduit à une estimation de mesures d'association valide. Des résultats théoriques des procédures d'inférence basées sur les ensembles de 
données fractionnaires imputées sont présentés, et les performances de ces méthodes sur des échantillons finis avec comparaisons avec des méthodes existantes 
sont examinées au moyen d'études de simulation.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 1E-I5: Probability and Statistical Problems and Results Arising from Neural Models\\Probabilit\'es et probl\`emes statistiques et r\'esultats d\'ecoulant de mod\`eles neuronaux}
\begin{center}{\large Organizer and Chair / Responsable et présidente:  Priscilla Greenwood (University of British Columbia)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 246\end{center}
\label{abs-sid:psp}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk psp-al
\def\whenwhere{[Monday May 30 / lundi 30 mai, 15:30]\\}
{\Author{Andre}{Longtin}{longtin}{1E-I5}\Author{Stephen}{Clarke}{clarke}{1E-I5}\Author{Len}{Maler}{maler}{1E-I5}%
\abshead{\absauthor{ANDRE LONGTIN}, \absauthor{STEPHEN CLARKE} \& \absauthor{LEN MALER}\absaffil{University of Ottawa}}
        {Fisher Information Analysis of Neural Focussing Point Processes\newline
        Analyse d'information de Fisher de processus de point focal neuronal}

\absSideBySide{The existence of a focus in a sensory system suggests that the neural activity acquires a specific feature around the focal point. Such a feature is currently unknown. This activity should be altered when the stimulus either looms towards or recedes away from the focal point. We first develop Fisher information theory for non-Poisson firing time point process data. We find that there is a distance at which this information is maximized during looming and receding object motions. This distance is in fact chosen by certain animals to view an object. Strikingly, this maximum occurs at a bifurcation between tonic firing and bursting patterns in the neurone's dynamics where the point process becomes clustered.
}{L'existence d'un point focal dans un système sensoriel suggère que l'activité neuronale acquiert une caractéristique spécifique autour de celui-ci. Cette caractéristique est actuellement inconnue. Cette activité devrait être modifiée lorsque le stimulus s'approche ou s'éloigne du point focal. Nous commençons par développer une théorie d'information de Fisher pour les données de points ponctuels de temps de décharge non Poisson. Nous trouvons qu'il existe une distance à laquelle cette information est maximisée pendant le rapprochement et l'éloignement d'un objet. Cette distance est en fait choisie par certains animaux pour visualiser un objet. Remarquablement, ce maximum se présente à une bifurcation entre les modes de décharge et de rafale tonique dans la dynamique du neurone où le processus ponctuel devient regroupé.
}}
%% Talk psp-pr
\def\whenwhere{[Monday May 30 / lundi 30 mai, 16:00]\\}
{\Author{Peter}{Rowat}{rowat}{1E-I5}\Author{Priscilla (Cindy)}{Greenwood}{greenwood}{1E-I5}%
\abshead{\absauthor{PETER ROWAT}\absaffil{University of California at San Diego}, \absauthor{PRISCILLA (CINDY) GREENWOOD}\absaffil{University of British Columbia}}
        {Firing Patterns of a Stochastic Neural Model\newline
        Modes de décharge d'un modèle neuronal stochastique}

\absSideBySide{Neurons in the medial entorhinal cortex exhibit an hexagonal grid-like spatial pattern of spike rates that has been proposed to represent a neural code for the navigational ability of rats and other mammals. Such a spatial pattern can be simulated from a network of neurons with both excitatory and inhibitory connections. We use a simple stochastic model of a stellate cell, with input from a simulated spatial firing field, to explore the spiking patterns produced when an influential current, Ih, is varied.
}{Les neurones du cortex entorhinal médian présentent une configuration spatiale en grille hexagonale de taux d'impulsions qui pourrait constituer un code neuronal pour la capacité de navigation des rats et autres mammifères. Une telle configuration spatiale peut être simulée à partir d'un réseau de neurones avec des connexions excitatrices et inhibitrices. Nous utilisons un modèle stochastique simple d'une cellule stellaire, avec des données d'un champ d'impulsions spatiales simulé, pour explorer les configurations d'impulsions produites lorsqu'un courant influent, Ih, est modifié.
}}
%% Talk psp-lw
\def\whenwhere{[Monday May 30 / lundi 30 mai, 16:30]\\}
{\Author{Lawrence}{Ward}{ward}{1E-I5}\Author{Priscilla (Cindy)}{Greenwood}{greenwood}{1E-I5}%
\abshead{\absauthor{LAWRENCE WARD} \& \absauthor{PRISCILLA (CINDY) GREENWOOD}\absaffil{University of British Columbia}}
        {Quasicycles and Quasipatterns in the Brain?\newline
        Quasicycles et quasimotifs dans le cerveau?}

\absSideBySide{Quasicycles are damped oscillations sustained by noise. Similarly, quasipatterns are damped spatial patterns sustained by noise. We will describe (1) one model that exhibits quasicycles, (2) an approximation that indicates that the model's behaviour comprises a stochastic rotation process and an Ornstein-Uhlenbeck amplitude process, (3) application of the approximation to gamma-band oscillations in living brains, and (4) production of quasipatterns in spatial arrays of quasicycle oscillators.
}{Les quasicycles sont des oscillations affaiblies mais soutenues par le bruit. Similairement, les quasimotifs sont des motifs spatiaux affaiblis également soutenus par le bruit. Nous discuterons (1) un modèle qui exhibe des quasicycles, (2) une approximation qui indique que le comportement du modèle comprend une rotation stochastique et un processus d'amplitude Ornstein-Uhlenbeck, (3) une application de l'approximation aux oscillations de bande-gamma dans les cerveaux vivants, et (4) la production de quasimotifs dans les rangs spatiaux des oscillateurs quasicycliques.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 1E-I6: Statistical Education Section Presidential Invited Address\\Allocution du l'invit\'e du pr\'esident du Groupe d'\'education en statistique}
\begin{center}{\large Chairs/Présidentes: Peggy Ng (York University) and/et Bethany White (University of Western Ontario)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 325\end{center}
\label{abs-sid:ses}
\begin{center}{\large\bfseries Abstract/R\'esum\'e}
\end{center}
%% Talk ses-ds
\def\whenwhere{[Monday May 30 / lundi 30 mai, 15:30]\\}
{\Author{David}{Spiegelhalter}{spiegelhalter}{1E-I6}%
\abshead{\absauthor{DAVID SPIEGELHALTER}\absaffil{Cambridge University}}
        {The Ups and (Many) Downs of Trying to Be a 'Public Statistician'\newline
        Les hauts et les (nombreux) bas de la vie d'un «~statisticien public~»}

\absSideBySide{Statisticians can have special insights into the numbers in the news, but don't tend to have much of a public role. I've got a unique job in trying to improve the way that stats and risk are discussed in society, although this is not an easy task. I will relate both positive and negative experiences from radio, TV, print and online, covering topics such as climate change, alcohol, polls, sky-diving, and sex. You can share the panic at being asked unanswerable questions live on radio, and the joys of doing well in Winter Wipeout due to careful study of the statistics.
}{Les statisticiens peuvent avoir des idées très utiles sur les chiffres qui font les nouvelles, mais on ne leur demande que rarement de jouer un rôle public. J'occupe un poste assez unique où je tente d'améliorer la discussion publique des statistiques et des risques – ce qui n'est pas toujours facile. J'aborderai des expériences positives et négatives tirées de la radio, de la télévision, de la presse écrite et en ligne sur des sujets aussi variés que le changement climatique, l'alcool, les sondages, le saut en parachute et la sexualité. Vous partagerez la panique que je ressens lorsqu'on me pose en direct à la radio des questions sans réponse possible et la joie que j'ai eue à réussir à Winter Wipeout après en avoir soigneusement étudié les statistiques.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 1E-C1: Biostatistics: Causal Inference and Measurement Error\\Biostatistique : inf\'erence causale et erreur de mesure}
\begin{center}{\large Chair/Président: Mireille Schnitzer (Universit\'e de Montr\'eal)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 255\end{center}
\label{abs-sid:bci}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk bci-ab
\def\whenwhere{[Monday May 30 / lundi 30 mai, 15:30]\\}
{\Author{Asma}{Bahamyirou}{bahamyirou}{1E-C1}\Author{Mireille}{Schnitzer}{schnitzer}{1E-C1}\Author{Lucie}{Blais}{blais}{1E-C1}%
\abshead{\absauthor{ASMA BAHAMYIROU}, \absauthor{MIREILLE SCHNITZER} \& \absauthor{LUCIE BLAIS}\absaffil{Université de Montréal}}
        {Comparison of Causal Inference Approaches for Estimating the Effect of Exposure to Inhaled Corticosteroids on Mean Birth Weight and Gestational Age\newline
        Comparaison des approches d'inférence causale pour l'estimation de l'effet de l'exposition aux corticostéroïdes inhalés sur le poids moyen à la naissance et sur l'âge gestationnel}

\absSideBySide{Regression and propensity score methods to deal with confounder adjustment for the estimation of a treatment effect can produce biased estimates. Semiparametric 
methods such as Targeted Minimum Loss-Based Estimation (TMLE) can be used to reduce dependence on parametric model specification. In a large covariate space, 
Collaborative TMLE (C-TMLE) can further improve upon mean squared error. This work aims to apply adaptive methods to estimate the causal effect of exposure to 
inhaled corticosteroids on mean birth weight and gestational age in pregnant women with mild asthma. We compare Inverse Probability of Treatment Weighting, TMLE and 
C-TMLE, all implemented with Super Learner
}{Les méthodes de score de propension et de régression pour traiter l'ajustement sur des facteurs de confusion pour l'estimation de l'effet d'un traitement peuvent produire des 
estimations biaisées. Des méthodes semiparamétriques telles que l'estimation ciblée de la perte minimale (TMLE) peuvent être utilisées pour réduire la dépendance à la 
spécification du modèle paramétrique. Dans un grand espace de covariables, la TMLE collaborative (C-TMLE) peut améliorer encore davantage l'erreur carrée moyenne. Cette 
étude a pour but l'application de méthodes adaptives pour estimer l'effet causal de l'exposition aux corticostéroïdes inhalés sur le poids moyen à la naissance et 
sur l'âge gestationnel chez les femmes enceintes souffrant d'asthme léger. Nous comparons la pondération de la probabilité inverse du traitement, la TMLE et C-TMLE, toutes 
mises en oeuvre avec Super Learner.
}}
%% Talk bci-lg
\def\whenwhere{[Monday May 30 / lundi 30 mai, 15:45]\\}
{\Author{Leila}{Golparvar}{golparvar}{1E-C1}\Author{David A.}{Stephens}{stephens}{1E-C1}\Author{Robert W.}{Platt}{platt}{1E-C1}%
\abshead{\absauthor{LEILA GOLPARVAR}, \absauthor{DAVID A. STEPHENS} \& \absauthor{ROBERT W. PLATT}\absaffil{McGill University}}
        {Causal Structure Learning and Propensity Score Adjustment\newline
        Apprentissage de structure causale et ajustement par score de propension}

\absSideBySide{Mathematical representation of causal dependencies among a set of variables via graphs has been in the statistical literature for more than a century. Predicting the effect of manipulations from non-experimental data involves two steps: first, discovery of causal structures, second identification and estimation of causal parameters. We adopted a frequentist constraint-based approach to discover the causal graph, and use the PC algorithm for causal discovery. To explore the use of the PC algorithm in selection of confounders, we conduct a Monte Carlo simulation study. Results show that PC algorithm works very well when the sample size is moderate to large.
}{La représentation mathématique des dépendances de causalité entre un ensemble de variables via des graphiques apparaît dans la littérature statistique depuis plus d'un siècle. Prédire l'effet des manipulations à partir de données non-expérimentales comporte deux étapes : premièrement, la découverte des structures causales, deuxièmement, l'identification et l'estimation des paramètres de causalité. Nous avons adopté une approche basée sur la contrainte fréquentiste pour découvrir le graphique de causalité et utilisons l'algorithme PC pour la découverte causale. Pour explorer l'utilisation de l'algorithme PC dans le choix des facteurs confondants, nous menons une étude de simulation de Monte-Carlo. Les résultats montrent que l'algorithme PC fonctionne très bien lorsque la taille de l'échantillon est modérée à grande.
}}
%% Talk bci-kr
\def\whenwhere{[Monday May 30 / lundi 30 mai, 16:00]\\}
{\Author{Kamal}{Rai}{rai}{1E-C1}\Author{Martin}{Lysy}{lysy}{1E-C1}\Author{Joel A.}{Dubin}{dubin}{1E-C1}%
\abshead{\absauthor{KAMAL RAI}, \absauthor{MARTIN LYSY} \& \absauthor{JOEL A. DUBIN}\absaffil{University of Waterloo}}
        {Bayesian Inference for Stochastic PK/PD Models\newline
        Inférence bayésienne pour les modèles stochastiques pharmacocinétiques et pharmacodynamiques}

\absSideBySide{Differential equations (DEs) occupy a central role in the modeling of pharmacokinetic/pharmacodynamic (PK/PD) processes. To estimate the parameters of these equations, a statistical approach might augment the deterministic DE with a stochastic simulation model and attempt to solve the corresponding inverse problem. We present a Bayesian methodology and its software implementation for PK/PD parameter inference, accounting for three sources of stochastic variability: (1) measurement error, (2) within-subject process fluctuations, and (3) between-subject random effects. Through numerical experiments with a small-sample PK study, we explore the effect of increased model complexity on the bias-variance trade-off.
}{Les équations différentielles occupent un rôle central dans la modélisation de processus pharmacocinétiques et pharmacodynamiques. Afin d'estimer les paramètres de ces équations, une approche statistique peut augmenter les équations différentielles déterministes au moyen d'un modèle de simulation stochastique, et peut tenter de résoudre le problème inverse correspondant. Nous présentons une méthodologie bayésienne et la mise en œuvre du logiciel connexe pour l'inférence de paramètres pharmacocinétiques et pharmacodynamiques, en tenant compte de trois sources de variabilité stochastique : 1) l'erreur de mesure, 2) les fluctuations du processus intra-sujet et 3) les effets aléatoires inter-sujets. Nous explorons l'effet de la complexité accrue du modèle sur le compromis entre le biais et la variance au moyen d'expériences numériques avec une étude pharmacocinétique sur un petit échantillon.
}}
%% Talk bci-ds
\def\whenwhere{[Monday May 30 / lundi 30 mai, 16:15]\\}
{\Author{Di}{Shu}{shu}{1E-C1}\Author{Grace Y.}{Yi}{yi}{1E-C1}%
\abshead{\absauthor{DI SHU} \& \absauthor{GRACE Y. YI}\absaffil{University of Waterloo}}
        {Estimation of Causal Parameters in the Presence of Measurement Error\newline
        Estimation de paramètres causaux en présence d'erreur de mesure}

\absSideBySide{Odds ratio, risk ratio and risk difference are widely used measures for comparing the performance of two treatments. In observational studies where confounders are inevitable, obtaining these measures with causal interpretation is of increasing interest. To reduce the confounding bias, many methods have been proposed, assuming that there is no measurement error. However, measurement error unavoidably occurs for various reasons. It is well known that ignoring measurement error effects can result in severely biased parameter estimates. In this talk, I'll discuss estimating causal parameters with measurement error. Numerical studies will be presented to assess the performance of proposed methods.
}{Les rapports de cotes, les rapports de risques et les différences de risques sont des mesures largement utilisées pour comparer l'efficacité de deux traitements. Dans le cadre d'études d'observation où les facteurs de confusion sont inévitables, obtenir ces mesures avec une interprétation causale suscite un intérêt croissant. Afin de réduire le biais des facteurs de confusion, de nombreuses méthodes ont été proposées en supposant qu'il n'y ait pas d'erreur de mesure. Cependant, les erreurs de mesure se produisent inévitablement, pour diverses raisons. Il est bien connu que le fait de ne pas tenir compte des effets d'erreurs de mesure peut biaiser sérieusement les estimations des paramètres. Dans cet exposé, nous discuterons des paramètres causaux ainsi que de l'erreur de mesure. Nous présenterons des études numériques afin d'évaluer l'efficacité des méthodes proposées.
}}
%% Talk bci-dt
\def\whenwhere{[Monday May 30 / lundi 30 mai, 16:30]\\}
{\Author{Denis}{Talbot}{talbot}{1E-C1}\Author{Amanda M.}{Rossi}{rossi}{1E-C1}\Author{Simon L.}{Bacon}{bacon}{1E-C1}\Author{Juli}{Atherton}{atherton}{1E-C1}\Author{Geneviève}{Lefebvre}{lefebvre}{1E-C1}%
\abshead{\absauthor{DENIS TALBOT}\absaffil{Université Laval}, \absauthor{AMANDA M. ROSSI}\absaffil{McGill University}, \absauthor{SIMON L. BACON}\absaffil{Concordia University}, \absauthor{JULI ATHERTON} \& \absauthor{GENEVIÈVE LEFEBVRE}\absaffil{Université du Québec à Montréal}}
        {A Graphical Perspective of Marginal Structural Models when Estimating the Causal Relationships Between Physical Activity, Blood Pressure, and Mortality\newline
        Une perspective graphique aux modèles structuraux marginaux pour l'estimation des relations causales entre l'activité physique, la pression artérielle et la mortalité}

\absSideBySide{Estimating causal effects requires important prior subject-matter knowledge and, sometimes, sophisticated statistical tools. Marginal structural models (MSMs) are a relatively new class of causal models that effectively deals with the estimation of the effects of time-varying exposures. MSMs have traditionally been embedded in the counterfactual framework to causal inference. In this talk, we use the causal graph framework to enhance the implementation of MSMs. We illustrate our approach using data from a prospective cohort study, the Honolulu Heart Program.
}{L'estimation d'effets causaux requiert une importante connaissance \textit{a priori} du domaine d'application et, parfois, l'utilisation d'outils statistiques sophistiqués. Les modèles structuraux marginaux (MSMs) constituent une classe de modèles causaux relativement nouvelle qui permet l'estimation des effets d'une exposition variant dans le temps. Ces modèles sont habituellement imbriqués dans le paradigme contrefactuel pour l'inférence causale. Afin de faciliter l'implantation des MSMs, nous proposons de considérer le paradigme graphique à l'inférence causale. Nous illustrons notre approche à l'aide des données d'une étude de cohorte prospective, le Honolulu Heart Program.
}}
%% Talk bci-yz
\def\whenwhere{[Monday May 30 / lundi 30 mai, 16:45]\\}
{\Author{Yeying}{Zhu}{zhu}{1E-C1}\Author{Jennifer}{Savage}{savage}{1E-C1}\Author{Debashis}{Ghosh}{ghosh}{1E-C1}%
\abshead{\absauthor{YEYING ZHU}\absaffil{University of Waterloo}, \absauthor{JENNIFER SAVAGE}\absaffil{Pennsylvania State University}, \absauthor{DEBASHIS GHOSH}\absaffil{Colorado School of Public Health}}
        {A Kernel-Based Approach to Covariate Adjustment for Causal Inference\newline
        Méthode par noyaux pour l'ajustement de covariables à des fins d'inférence causale}

\absSideBySide{In matching, an important goal is to achieve balance in the covariates among the treatment groups. In this article, we introduce the concept of distributionally balance preserving which requires the distribution of the covariates to be the same in different treatment groups.  Meanwhile, we propose a new balance measure called kernel distance, which is the empirical estimate of the probability metric defined in the reproducing kernel Hilbert spaces. Compared to the traditional balance metrics, the kernel distance measures the difference in the two multivariate distributions instead of the difference in the finite moments of the distributions.
}{En appariement, équilibrer les covariables parmi les groupes de traitement est un objectif important. Cet article présente le concept de préservation distributionnelle de l'équilibre, ce qui exige une distribution identique des covariables dans différents groupes de traitement. Entre-temps, nous proposons une nouvelle mesure de l'équilibre appelée distance par noyaux, qui se veut une estimation empirique de la métrique de la probabilité définie dans les espaces de Hilbert à noyau reproduisant. Comparativement aux mesures traditionnelles de l'équilibre, la distance par noyaux mesure la différence dans les deux distributions multivariées plutôt que celle dans les moments finis des distributions.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 1E-C2: Biostatistics: Methodological Innovation 2\\Biostatistique : innovation m\'ethodologique 2}
\begin{center}{\large Chair/Président: Andrea Benedetti (McGill University)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 256\end{center}
\label{abs-sid:bmi2}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk bmi2-ab
\def\whenwhere{[Monday May 30 / lundi 30 mai, 15:30]\\}
{\Author{Ashley}{Bonner}{bonner}{1E-C2}\Author{Joseph}{Beyene}{beyene}{1E-C2}%
\abshead{\absauthor{ASHLEY BONNER} \& \absauthor{JOSEPH BEYENE}\absaffil{McMaster University}}
        {Inference for Model Parameters in Sparse Canonical Correlation using Resampling Techniques\newline
        Inférence des paramètres des modèles pour la corrélation canonique rare à l'aide de techniques de rééchantillonnage}

\absSideBySide{Sparse canonical correlation analysis (sCCA) is a multivariate method that uses latent variables to explore sparse and complex associations between high-dimensional data types. Currently, there are limited inferential strategies to accompany the estimated loading values that researchers depend on to determine whether, and to what extent, each variable contributes to the latent relationship. Through simulation, we evaluate the performance of re-sampling techniques, including the non-parametric bootstrap, to explore distributional properties and create confidence intervals for sCCA loading values. Under certain data conditions, re-sampling may offer a means to infer some level of confidence in the complex relationships returned from sCCA.
}{L'analyse de corrélation canonique rare (sCCA) est une méthode multivariée qui utilise des variables latentes pour explorer les associations rares et complexes entre les types de données de grande dimension. Actuellement, il existe des limites dans les stratégies inférentielles pour accompagner les valeurs de chargement estimées dont dépendent les chercheurs pour déterminer si, et dans quelle mesure, chaque variable contribue à la relation latente. À l'aide de simulations, nous évaluons la performance des techniques de rééchantillonnage, y compris le bootstrap non-paramétrique, pour explorer les propriétés de distribution et bâtir des intervalles de confiance pour les valeurs de chargement sCCA. Sous certaines conditions des données, le rééchantillonnage peut offrir un moyen de déduire un certain niveau de confiance dans les relations complexes renvoyées par la sCCA.
}}
%% Talk bmi2-oe
\def\whenwhere{[Monday May 30 / lundi 30 mai, 15:45]\\}
{\Author{Osvaldo}{Espin-Garcia}{espingarcia}{1E-C2}\Author{Radu V.}{Craiu}{craiu}{1E-C2}\Author{Shelley B.}{Bull}{bull}{1E-C2}%
\abshead{\absauthor{OSVALDO ESPIN-GARCIA}\absaffil{University of Toronto and Lunenfeld-Tanenbaum Research Institute}, \absauthor{RADU V. CRAIU}\absaffil{University of Toronto}, \absauthor{SHELLEY B. BULL}\absaffil{University of Toronto and Lunenfeld-Tanenbaum Research Institute}}
        {Two-Phase Designs for Joint Quantitative-Trait-Dependent and GWAS-SNP-Dependent Sampling in Post-GWAS Regional Sequencing\newline
        Plans à deux phases pour trait quantitatif dépendant et échantillonnage GWAS-SNP conjoints dans le séquençage régional post-GWAS}

\absSideBySide{We evaluate two-phase designs to follow up findings from genomewide association study (GWAS) when regional sequencing in the entire cohort is too expensive. We develop a novel EM-estimation under a semiparametric maximum likelihood formulation tailored for post-GWAS inference. A GWAS-SNP serves as an auxiliary covariate in inferring association between a sequence variant and a normally distributed quantitative trait (QT). We perform simulations to quantify efficiency and power of QT-SNP joint sampling under alternative sample allocations. A joint allocation balanced on GWAS-SNP genotype and extreme-QT strata yields power improvements compared to marginal QT or SNP-sampling counterparts.
}{Nous évaluons les plans à deux phases pour assurer le suivi des résultats de l'étude d'association pangénomique (GWAS) lorsque le séquençage régional de l'ensemble de la cohorte est trop cher. Nous développons une nouvelle estimation EM sous une formulation du maximum de vraisemblance semiparamétrique sur mesure pour l'inférence post-GWAS. Un GWAS-SNP sert de covariable auxiliaire dans la déduction d'une association entre une variante de séquence et un trait quantitatif (CQ) normalement distribué. Nous effectuons des simulations pour quantifier l'efficacité et la puissance de l'échantillonnage conjoint CQ-SNP dans le cadre de répartitions d'échantillons alternatifs. Une répartition conjointe équilibrée sur le génotype GWAS-SNP et les strates extrêmes-QT donne des améliorations de puissance par rapport à CQ marginal ou à ses homologues SNP-échantillonnage.
}}
%% Talk bmi2-jx
\def\whenwhere{[Monday May 30 / lundi 30 mai, 16:00]\\}
{\Author{Jingxiong}{Xu}{xu}{1E-C2}\Author{Laurent}{Briollais}{briollais}{1E-C2}\Author{Wei}{Xu}{xu}{1E-C2}%
\abshead{\absauthor{JINGXIONG XU}\absaffil{University of Toronto}, \absauthor{LAURENT BRIOLLAIS}\absaffil{Lunenfeld-Tanenbaum Research Institute; University of Toronto}, \absauthor{WEI XU}\absaffil{Princess Margaret Cancer Centre: University of Toronto}}
        {Bayesian Statistical Approaches for Genetic Association with Next Generation Sequencing (NGS) Data\newline
        Approches statistiques bayésiennes pour l'association génétique avec des données de séquençage de prochaine génération (SPG)}

\absSideBySide{The discovery of rare variants is becoming a major challenge in genetic association studies and could help elucidating the genetic basis of common diseases. Because rare variants occur too infrequently in the general population, single-variant association tests lack power in NGS analyses. We developed several joint test statistics using a Bayes Factor approach to assess the evidence of association between a set of rare variants located on same chromosomal region and a cancer outcome. Our simulation studies show the advantages of the Bayesian approach compared to two popular approaches: the burden test and the sequence kernel association test (SKAT).
}{La découverte de variantes rares prend de l'importance pour les études d'associations génétiques et pourrait aider à élucider le fondement génétique de maladies communes. Comme les variantes rares sont peu fréquentes dans la population en général, les tests d'association à variante unique ont peu de puissance dans les analyses SPG. Nous avons élaboré plusieurs statistiques de tests conjointes à l'aide d'une approche fondée sur le facteur de Bayes pour évaluer les indices d'une association entre un ensemble de variantes rares situé dans une même zone chromosomique et l'apparition d'un cancer. Nos études de simulation montrent les avantages de l'approche bayésienne comparativement à deux approches populaires~: le test de charge et le test d'association à noyau de séquences (SKAT).
}}
%% Talk bmi2-qy
\def\whenwhere{[Monday May 30 / lundi 30 mai, 16:15]\\}
{\Author{Qing}{Yu}{yu}{1E-C2}\Author{Guohua}{Yan}{yan}{1E-C2}\Author{Renjun}{Ma}{ma}{1E-C2}%
\abshead{\absauthor{QING YU}, \absauthor{GUOHUA YAN} \& \absauthor{RENJUN MA}\absaffil{University of New Brunswick}}
        {A Bayesian Poisson Mixed Modelling Approach to Zero-Inflated Cox Proportional Hazard Models\newline
        Approche bayésienne Poisson de modélisation mixte pour les modèles à risques proportionnels de Cox à surreprésentation de zéros}

\absSideBySide{In Cox proportional hazard models, frailty can be introduced to incorporate individual susceptibility to a disease or an event of interest; however, some individuals may be immune or insusceptible to certain diseases. We use compound Poisson distributed frailties, which has a spike at zero, to characterize the heterogeneity in susceptibility among individuals. This approach allows us to accommodate both zero and positive susceptibilities in an integral way. We extend the Poisson modelling approach proposed by Ma et al. (Biometrika, 2003) to estimation of our model. Our proposed approach is illustrated with a simulated dataset and a real life example.
}{Dans les modèles à risques proportionnels de Cox, un facteur de fragilité peut être introduit afin d'incorporer la susceptibilité individuelle à une maladie ou à un événement d'intérêt; toutefois, certaines personnes peuvent être immunisées contre une maladie ou y être insusceptibles. Nous utilisons des fragilités distribuées selon une Poisson composée, qui a un pic à zéro, pour caractériser l'hétérogénéité de la susceptibilité chez les individus. Cette approche nous permet de prendre en compte les susceptibilités nulles et positives. Nous étendons l'approche de modélisation de type Poisson proposée par Ma et coll. (Biometrika, 2003) à l'estimation de notre modèle. L'approche que nous proposons est illustrée par un ensemble de données simulées et un exemple réel.
}}
%% Talk bmi2-tf
\def\whenwhere{[Monday May 30 / lundi 30 mai, 16:30]\\}
{\Author{Tian}{Feng}{feng}{1E-C2}\Author{Joseph}{Beyene}{beyene}{1E-C2}\Author{Jemila S.}{Hamid}{hamid}{1E-C2}%
\abshead{\absauthor{TIAN FENG}, \absauthor{JOSEPH BEYENE} \& \absauthor{JEMILA S. HAMID}\absaffil{McMaster University}}
        {A Bayesian Method for Handling Missing Data in the Growth Curve Model\newline
        Une méthode bayésienne pour traiter les données manquantes dans le modèle de courbe de croissance}

\absSideBySide{The growth curve model (GCM) is a generalized multivariate analysis of variance (GMANOVA) 
model. The standard GCM has several desirable properties when data is complete, but is 
problematic when there are missing observations. We propose a Bayesian method for handling 
missing data in GCM using the Gibbs sampling algorithm. An extensive simulation study is 
carried out to compare the proposed method with the expectation-maximization (EM) algorithm. 
The Bayesian method is found to be generally useful, and gives promising results compared 
with the EM algorithm. Classical datasets are used to illustrate the performance of the 
proposed method.
}{Le modèle de courbe de croissance (MCC) est un modèle d'analyse multivariée généralisée de la variance (GMANOVA). Le MCC standard possède plusieurs propriétés souhaitables lorsque des données sont complètes, mais est problématique en présence d'observations manquantes. Nous proposons une méthode bayésienne pour traiter les données manquantes dans le MCC en utilisant l'algorithme d'échantillonnage de Gibbs. Une vaste étude par simulation est réalisée pour comparer la méthode proposée à l'algorithme espérance-maximisation (EM). La méthode bayésienne se révèle être généralement utile et donne des résultats prometteurs par rapport à l'algorithme EM. Les jeux de données classiques sont utilisés pour illustrer la performance de la méthode proposée.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 1E-C3: Biostatistics: Survival and Current Status Data\\Biostatistique : donn\'ees de survie et d'\'etat actuel}
\begin{center}{\large Chair/Président: Cindy Feng (University of Saskatchewan)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 259\end{center}
\label{abs-sid:bsc}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk bsc-lh
\def\whenwhere{[Monday May 30 / lundi 30 mai, 15:30]\\}
{\Author{Longlong}{Huang}{huang}{1E-C3}\Author{Xuewen}{Lu}{lu}{1E-C3}\Author{Karen}{Kopciuk}{kopciuk}{1E-C3}%
\abshead{\absauthor{LONGLONG HUANG} \& \absauthor{XUEWEN LU}\absaffil{University of Calgary}, \absauthor{KAREN KOPCIUK}\absaffil{Alberta Health Services, University of Calgary}}
        {A Group Bridge Approach for Component Selection in Nonparametric Accelerated Failure Time Additive Regression Model\newline
        Une approche de pont groupé pour la sélection des composantes dans un modèle de régression additif non paramétrique du temps de défaillance accéléré}

\absSideBySide{We propose a nonparametric accelerated failure time additive regression model whose covariates have nonparametric effects on the survival time. The proposed model can be fitted to high-dimensional censored data. B-splines are used to approximate the nonparametric components. A group bridge penalized variable selection approach based on the inverse probability-of-censoring weighted least-squares is developed to select nonparametric components. Simulation studies indicate that the proposed method is able to distinguish the nonzero components from the zero components and estimate the nonzero components simultaneously even with relatively high censoring rates. Real data analysis illustrates the application of the proposed method to survival data.
}{Nous proposons un modèle de régression additif non paramétrique du temps de défaillance accéléré dont les covariables ont des effets non paramétriques sur le temps de survie. Le modèle proposé peut être ajusté à des données censurées de grande dimension. Les B-splines sont utilisées pour approximer les composantes non paramétriques. Une approche de sélection des variables de pont groupé pénalisé basée sur l'inverse de la probabilité de censures des moindres carrés pondérés est développée pour sélectionner les composantes non paramétriques. Des études de simulation montrent que la méthode proposée peut distinguer les composantes non nulles à partir des composantes nulles et estimer les composantes non nulles simultanément même avec des taux de censure relativement élevés. L'analyse de données réelles illustre l'application de la méthode proposée aux données de survie.
}}
%% Talk bsc-sk
\def\whenwhere{[Monday May 30 / lundi 30 mai, 15:45]\\}
{\Author{Saima}{Khosa}{khosa}{1E-C3}\Author{Shahedul}{Khan}{khan}{1E-C3}%
\abshead{\absauthor{SAIMA KHOSA} \& \absauthor{SHAHEDUL KHAN}\absaffil{University of Saskatchewan}}
        {A Generalization of Log-logistic Distribution with Application in Survival Analysis\newline
        Une généralisation de la distribution log-logistique avec application à l'analyse de survie}

\absSideBySide{The log-logistic distribution has wide applications in analyzing survival data. The model is closed under both multiplication of failure time and proportionality of odds. However, it is not a proportional hazard (PH) model. In survival analysis, PH models play a pivotal role in many applications. So in this study, we propose a generalization of the log-logistic distribution that falls into the PH family. A comparison between the generalized log-logistic and the Cox PH model reveals that the generalized log-logistic PH model performs reasonably well in analyzing different types of time-to-event data.
}{La distribution log-logistique a de larges applications dans l'analyse des données de survie. Le modèle est fermé à la fois sous la multiplication du temps de défaillance et la proportionnalité des cotes. Cependant, ce n'est pas un modèle de risque proportionnel (RP). Dans l'analyse de survie, les modèles de RP jouent un rôle central dans de nombreuses applications. Ainsi dans cette étude, nous proposons une généralisation de la distribution log-logistique qui tombe dans la famille de RP. Une comparaison entre le log-logistique généralisé et le modèle de Cox RP révèle que le modèle généralisé log-logistique de RP performe assez bien dans l'analyse des différents types de données de temps d'événement.
}}
%% Talk bsc-sl
\def\whenwhere{[Monday May 30 / lundi 30 mai, 16:00]\\}
{\Author{Shanshan}{Lu}{lu}{1E-C3}\Author{Xuewen}{Lu}{lu}{1E-C3}\Author{Jingjing}{Wu}{wu}{1E-C3}%
\abshead{\absauthor{SHANSHAN LU}, \absauthor{XUEWEN LU} \& \absauthor{JINGJING WU}\absaffil{University of Calgary}}
        {Exploring the Varying Covariate Effects in Partially Linear Proportional Odds Models with Current Status Data\newline
        Exploration des effets variables de covariables dans des modèles à cotes proportionnelles partiellement linéaires avec des données sur le statut actuel}

\absSideBySide{We consider a partially linear proportional odds model with current status data. This model enables one to examine the extent to which some covariates interact nonlinearly with an exposure variable, while other covariates present linear effects. B-spline approach and sieve maximum likelihood estimation method are used to get an integrated estimate for the linear coefficients, the varying-coefficient functions and the baseline function. The proposed parameter estimators are proved to be consistent and asymptotically normal, and the estimators for the nonparametric functions achieve the optimal rate of convergence. Simulation studies and a real data analysis are used for assessment and illustration.
}{Nous nous intéressons à un modèle à cotes proportionnelles partiellement linéaire avec des données sur le statut actuel. Ce modèle permet d'examiner dans quelle mesure certaines covariables interagissent de façon non linéaire avec une variable d'exposition alors que d'autres covariables présentent des effets linéaires. Nous utilisons l'approche de la courbe B-Spline et la méthode par tamisage du maximum de vraisemblance afin d'obtenir une estimation intégrée pour les coefficients linéaires, les fonctions de coefficient variable et la fonction de base. Nous démontrons que les estimateurs des paramètres sont convergents et asymptotiquement normaux et que les estimateurs des fonctions non-paramétriques atteignent le taux optimal de convergence. Nous utilisons des études de simulation et une analyse de données réelles pour l'évaluation et l'illustration.
}}
%% Talk bsc-yd
\def\whenwhere{[Monday May 30 / lundi 30 mai, 16:15]\\}
{\Author{Yuan}{Dong}{dong}{1E-C3}\Author{Xuewen}{Lu}{lu}{1E-C3}\Author{Radhey}{Singh}{singh}{1E-C3}%
\abshead{\absauthor{YUAN DONG} \& \absauthor{XUEWEN LU}\absaffil{University of Calgary}, \absauthor{RADHEY SINGH}\absaffil{University of Waterloo}}
        {Efficient Estimation of Varying-Coefficient Partially Linear Proportional Hazards Models  with Current Status Data\newline
        Estimation efficace des modèles de risques proportionnels à coefficients linéaires partiellement variables à l'aide de données sur l'état actuel}

\absSideBySide{We consider a semiparametric varying-coefficient proportional hazards model with current status data. This mode enables one to assess possibly nonlinear effect of a certain covariate on the hazard rate. We apply B-splines to approximate both the unknown baseline hazard function and the varying-coefficient function. Sieve maximum likelihood estimation method is used for estimation. The rate of convergence of the estimators for the two unknown smooth functions is obtained and the estimator for the unknown parameter is shown to be asymptotically efficient and normally distributed. Simulation studies and data analysis are conducted to examine the finite-sample properties of the proposed estimators.
}{Nous considérons un modèle semi-paramétrique de risques proportionnels à coefficients variables avec des données sur l'état actuel. Ce mode permet d'évaluer l'effet éventuellement non-linéaire d'une certaine covariable sur le taux de risque. Nous appliquons B-splines pour approximer à la fois la fonction inconnue de risque de base et la fonction à coefficients variables. La méthode d'estimation du maximum de vraisemblance de Sieve est utilisée pour l'estimation. Le taux de convergence des estimateurs pour les deux fonctions lisses inconnues est obtenu et il est montré que l'estimateur pour le paramètre inconnu est asymptotiquement efficace et distribué normalement. Des études de simulation et une analyse des données sont menées pour étudier les propriétés des échantillons de taille finie des estimateurs proposés.
}}
%% Talk bsc-oaj
\def\whenwhere{[Monday May 30 / lundi 30 mai, 16:30]\\}
{\Author{Omidali Aghababaei}{Jazi}{jazi}{1E-C3}\Author{Masoud}{Asgharian}{asgharian}{1E-C3}\Author{Abbas}{Khalili}{khalili}{1E-C3}%
\abshead{\absauthor{OMIDALI AGHABABAEI JAZI}, \absauthor{MASOUD ASGHARIAN} \& \absauthor{ABBAS KHALILI}\absaffil{McGill University}}
        {A Comparison of the Penalized Estimation Methods in the Cox Model for Right-censored Length-biased Data\newline
        Une comparaison des méthodes d'estimation pénalisées dans le modèle de Cox pour les données de longueur biaisée censurées à droite}

\absSideBySide{Length-biased data commonly arises in epidemiological studies when prevalent sampling is used for recruiting cohort subjects. Several estimation methods have been proposed for parameters in different survival models in the presence of length-bias in the last decade. In this talk, we will focus on variable selection and discuss how to penalize those methods with different penalty functions. We will compare the resulting estimators for the parameters in the Cox model in terms of their efficiency and employ the selected method to analyze a real right-censored length-biased data.
}{Les données de longueur biaisée surviennent généralement dans les études épidémiologiques lorsque l'échantillonnage prévalent est utilisé pour le recrutement des sujets de la cohorte. Plusieurs méthodes d'estimation ont été proposées pour les paramètres des différents modèles de survie en présence de longueur biaisée au cours de la dernière décennie. Dans cet exposé, nous allons nous concentrer sur la sélection des variables et discuter de la façon de sanctionner ces méthodes à l'aide de différentes fonctions de pénalité. Nous allons comparer les estimateurs résultants pour les paramètres du modèle de Cox selon leur efficacité et employer la méthode choisie pour analyser de vraies données de longueur biaisée censurées à droite.
}}
%% Talk bsc-dl
\def\whenwhere{[Monday May 30 / lundi 30 mai, 16:45]\\}
{\Author{Dongdong}{Li}{li}{1E-C3}\Author{Joan}{Hu}{hu}{1E-C3}%
\abshead{\absauthor{DONGDONG LI} \& \absauthor{JOAN HU}\absaffil{Simon Fraser University}}
        {Modelling and Analysis of Bivariate Event-Times with Informative Censoring\newline
        Modélisation et analyse des temps d'événements bivariés avec censure de l'information}

\absSideBySide{In attempt to evaluate risk of cancer patient to cardiovascular disease, we consider analysis of bivariate event-times with informative censoring. Our particular attention is to the inherent challenges to modelling association between the two event-times and their dependence with the censoring time. We explore various models and develop inference procedures under proposed models. The research is motivated and will be illustrated using records of cardiovascular disease related hospitalization from a cohort of breast cancer patients together with their cancer registry information. This is joint work with Professor Joan Hu.
}{Pour tenter d'évaluer le risque de cancer chez des patients atteints de maladie cardiovasculaire, nous considérons l'analyse des temps d'événements bivariés avec censure d'information. Nous portons une attention particulière aux défis liés à la modélisation de l'association entre deux temps d'événements et leur dépendance avec la censure de temps. Nous explorons divers modèles et élaborons des procédures d'inférence dans le cadre des modèles proposés. Cette recherche est motivée et illustrée par des données de dossiers d'hospitalisation sur les maladies cardiovasculaires provenant d'une cohorte de patients atteints de cancer du sein, ainsi que par les données contenues dans leur dossier d'information sur leur cancer. Ces travaux ont été réalisés conjointement avec la Professeure Joan Hu.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 1E-C4: Statistical Methods and Applications 3\\M\'ethodes statistiques et applications 3}
\begin{center}{\large Chair/Président: Anand N. Vidyashankar (George Mason University)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Welch 8G\end{center}
\label{abs-sid:sma5}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk sma5-rh
\def\whenwhere{[Monday May 30 / lundi 30 mai, 15:30]\\}
{\Author{Rojiar}{Haddadian}{haddadian}{1E-C4}\Author{Liqun}{Wang}{wang}{1E-C4}\Author{Yuliya}{Martsynyuk}{martsynyuk}{1E-C4}%
\abshead{\absauthor{ROJIAR HADDADIAN}, \absauthor{LIQUN WANG} \& \absauthor{YULIYA MARTSYNYUK}\absaffil{University of Manitoba}}
        {Simulation-based Estimation in Regression Models with Ordinal Response Variable and Mismeasured Covariates\newline
        Estimation fondée sur la simulation dans les modèles de régression avec variable de réponse ordinale et covariables mesurées incorrectement}

\absSideBySide{A common problem in regression analysis is that some covariates are measured with errors. We present
a simulation-based approach for the inference in the cumulative logit model for ordinal response variable and mismeasured covariates. We propose a simulation-based approach to overcome the computational difficulty of minimizing an objective function which involves multiple integrals and thus obtaining estimators for unknown parameters. This method does not require parametric assumptions for the distributions of the unobserved covariates and error components. Consistency and asymptotically normality for the proposed estimator are derived under some regularity conditions. Finally, the methodology is illustrated through simulation studies.
}{Un problème commun dans l'analyse de régression est l'erreur de mesure de certaines covariables. Nous présentons une approche fondée sur la simulation pour l'inférence dans le modèle logit cumulatif pour des variables explicatives ordinales et des covariables mesurées incorrectement. Nous proposons une approche fondée sur la simulation pour surmonter la difficulté de calcul de la minimisation d'une fonction objective qui inclut des intégrales multiples et ainsi obtenir des estimateurs pour les paramètres inconnus. Cette méthode ne nécessite pas d'hypothèses paramétriques sur les distributions des covariables non observées et sur les composantes d'erreur. La cohérence et la normalité asymptotique de l'estimateur proposé sont dérivées sous certaines conditions de régularité. Enfin, la méthodologie est illustrée à l'aide d'études de simulation.
}}
%% Talk sma5-rm
\def\whenwhere{[Monday May 30 / lundi 30 mai, 15:45]\\}
{\Author{Ren}{Mingchen}{mingchen}{1E-C4}\Author{Alex}{de Leon}{deleon}{1E-C4}\Author{Yan}{Ying}{ying}{1E-C4}%
\abshead{\absauthor{REN MINGCHEN}, \absauthor{ALEX DE LEON} \& \absauthor{YAN YING}\absaffil{University of Calgary}}
        {Likelihood Estimation of the Gaussian Copula Distribution for Mixed Data via PX-MCEM\newline
        Estimation de vraisemblance de la distribution de copules gaussiennes pour des données mixtes au moyen de l'algorithme EM de Monte-Carlo par expansion de paramètres}

\absSideBySide{I employ the Gaussian copula distribution (GCD) to construct a joint distribution for mixed variables, where I adopt latent variable description of the binary variables. The model generalizes the grouped continuous model (GCM) or the multivariate probit model for binary data and its extension, the conditional GCM (CGCM), to mixed data. I used the parameter-expanded EM algorithm (PX-MCEM) to estimate the parameters. I also carried out a simulation study to investigate the finite-sample properties of the estimates. This is a joint work with A. R. de Leon and Y. Yan.
}{J'utilise la distribution des copules gaussiennes pour construire une distribution conjointe pour des variables mixtes, où j'adopte une description des variables binaires par des variables latentes. Le modèle généralise le modèle continu regroupé, le modèle probit multivarié pour les données binaires ainsi que son extension, le modèle continu regroupé conditionnel, au cas de données mixtes. J'ai utilisé l'algorithme EM avec expansion de paramètres pour estimer les paramètres et effectué une étude de simulation pour étudier les propriétés de l'échantillon de taille finie des estimations. Ceci est un travail conjoint avec A. R. de Leon et Y. Yan.
}}
%% Talk sma5-bn
\def\whenwhere{[Monday May 30 / lundi 30 mai, 16:00]\\}
{\Author{Bouchra}{Nasri}{nasri}{1E-C4}\Author{Taoufik}{Bouezmarni}{bouezmarni}{1E-C4}\Author{André}{St-Hilaire}{sthilaire}{1E-C4}%
\abshead{\absauthor{BOUCHRA NASRI}\absaffil{INRS-ETE}, \absauthor{TAOUFIK BOUEZMARNI}\absaffil{Université de Sherbrooke (UdeS)}, \absauthor{ANDRÉ ST-HILAIRE}\absaffil{Institut national de recherche scientifique (INRS-ETE)}}
        {Copula-Based Quantile Regression and Inference\newline
        Inférence pour un modèle de régression sur les quantiles basé sur des copules}

\absSideBySide{In this work, we introduce a new approach for nonlinear quantile regression modelling based on the copula function. The main idea of this approach is to describe the conditional quantile by using the link between the conditional distribution of the variable of interest given the covariate with the copulas function and the marginal distribution. The quantile estimation will be done by estimating the parametres of the copula function and marginal distributions. Parametric and a semi-parametric estimators for the quantile functions are proposed and their asymptotic normality are established. Simulation results show the performance of theses proposed estimators.
}{Nous présentons une nouvelle approche pour la modélisation par régression non-linéaire des quantiles, basée sur la fonction de copules. L'idée principale derrière cette approche est de décrire le quantile conditionnel grâce au lien existant entre la loi de la variable d'intérêt conditionnelle à la covariable, la fonction de copules et la distribution marginale. L'estimation du quantile utilise l'estimation des paramètres de la fonction de copules et des distributions marginales. Nous proposons des estimateurs paramétriques et semi-paramétriques des fonctions de quantiles et établissons leur normalité asymptotique. Les résultats de simulations montrent l'efficacité des estimateurs proposés.
}}
%% Talk sma5-mc
\def\whenwhere{[Monday May 30 / lundi 30 mai, 16:15]\\}
{\Author{Monsur}{Chowdhury}{chowdhury}{1E-C4}\Author{Saumen}{Mandal}{mandal}{1E-C4}%
\abshead{\absauthor{MONSUR CHOWDHURY} \& \absauthor{SAUMEN MANDAL}\absaffil{University of Manitoba}}
        {Optimal Designs for Maximum Likelihood Estimation\newline
        Conceptions optimales pour l'estimation du maximum de vraisemblance}

\absSideBySide{There are many problems in statistics, which demand the calculation of one or more optimizing probability distributions. We consider one such problem in which we determine maximum likelihood estimates of cell probabilities under the hypothesis of marginal homogeneity in square contingency tables. This is a maximization problem subject to satisfying several constraints. We first formulate the Lagrangian function and then transform the problem to that of maximizing some functions of the cell probabilities simultaneously. Finally, we apply the methodologies in some real data sets. The methodologies are quite flexible and could be applied to a wide class of optimization problems.
}{Plusieurs problèmes en statistique exigent le calcul d'une ou plusieurs distributions de probabilité optimisante. Nous considérons un tel problème dans lequel nous déterminons les estimations du maximum de vraisemblance des probabilités de cellule sous l'hypothèse d'homogénéité marginale dans les tableaux de contingence carrés. Il s'agit d'un problème de maximisation sous réserve de satisfaire à plusieurs contraintes. Nous formulons d'abord la fonction lagrangienne puis transformons le problème en celui de la maximisation simultanée de certaines fonctions de probabilités de cellules. Enfin, nous appliquons les méthodologies à certains jeux de données réelles. Les méthodes sont assez flexibles et pourraient être appliquées à une grande classe de problèmes d'optimisation.
}}
%% Talk sma5-mp
\def\whenwhere{[Monday May 30 / lundi 30 mai, 16:30]\\}
{\Author{Maysum}{Panju}{panju}{1E-C4}%
\abshead{\absauthor{MAYSUM PANJU}\absaffil{University of Waterloo}}
        {Topic Modelling in Natural Language Processing\newline
        Modélisation de sujet dans le traitement du langage naturel}

\absSideBySide{Documents written in natural language typically are not just structured sequences of randomly selected words. The underlying distribution determining which words might be found in a given document typically depends on some latent factors describing the overall domain of the document, which we can refer to as ``topics''. Using statistical methods, these hidden topics can be discovered and identified based on the words that are observed across various documents. In this talk, we'll explore a high-level overview of some of these methods, which ultimately allow machines to infer the central topics and concepts that shape a body of text.
}{En général, les documents écrits en langage naturel ne sont pas seulement des séquences structurées de mots choisis au hasard. La distribution sous-jacente qui détermine quels mots peuvent être trouvés dans un document donné dépend de certains facteurs latents décrivant le domaine principal du document, que l'on peut appeler « sujets ». Grâce à des méthodes statistiques, ces sujets cachés peuvent être découverts et identifiés en fonction des mots qui sont observés dans divers documents. Dans cet exposé, nous explorerons une vue d'ensemble de haut niveau de certaines de ces méthodes, qui permettent à des machines d'inférer les sujets et concepts centraux formant le corps d'un texte.
}}
%% Talk sma5-my
\def\whenwhere{[Monday May 30 / lundi 30 mai, 16:45]\\}
{\Author{Mamadou}{Yauck}{yauck}{1E-C4}\Author{Louis-Paul}{Rivest}{rivest}{1E-C4}%
\abshead{\absauthor{MAMADOU YAUCK} \& \absauthor{LOUIS-PAUL RIVEST}\absaffil{Laval University}}
        {Prediction of Population Sizes in Open-Population Capture-Recapture Models\newline
        Prédiction des tailles de populations dans les modèles de capture-recapture pour population ouverte}

\absSideBySide{Capture-recapture models are used for the estimation of demographic parameters of a wildlife population. For the open-population experiment, we have models such as the Jolly-Seber approach (1963) and the robust-design of Kendall-Pollock-Brownie (1965). This presentation focuses on estimates for population sizes, seen as a prediction problem. We are mainly interested in the robust-design experiment and provide a better predictor than the Kendall-Pollock-Brownie and Rivest-Daigle (2004) estimators. We develop an empirical version of our new predictor, and provide an estimate for the prediction error.
}{Les modèles de capture-recapture sont utilisés pour estimer les paramètres démographiques de populations animales. Plusieurs approches sont disponibles pour les populations ouvertes, par exemple le modèle de Jolly et Seber (1963) et le design robuste de Kendall, Pollock et Brownie (1995). Cet exposé porte sur l'estimation des tailles de populations, considérée comme un problème de prédiction. Nous nous intéressons au cas du design robuste et proposons un prédicteur, meilleur que les estimateurs de Kendall et Pollock et de Rivest et Daigle (2004). Nous présentons ensuite une version empirique du nouveau prédicteur, et traitons enfin de l'estimation de l'erreur de prédiction.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 1E-C5: Statistics in Action\\La statistique en action}
\begin{center}{\large Chair/Président: Yi Yang (McGill University)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 257\end{center}
\label{abs-sid:sa}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk sa-db
\def\whenwhere{[Monday May 30 / lundi 30 mai, 15:30]\\}
{\Author{Devan}{Becker}{becker}{1E-C5}\Author{W. John}{Braun}{braun}{1E-C5}%
\abshead{\absauthor{DEVAN BECKER}\absaffil{University of Western Ontario}, \absauthor{W. JOHN BRAUN}\absaffil{University of British Columbia}}
        {Aerial Firefighting Fatigue Project\newline
        Projet sur la fatigue dans les services aériens de lutte contre les incendies}

\absSideBySide{In this ongoing project, we are attempting to find indicators of pilot fatigue in aerial firefighters using raw flight data (location, altitude, speed, yaw, pitch, 
roll, etc. recorded each second). We are applying Shewhart control charts to detect unusual flights based on summaries of the data. Each flight is different, but 
control charts are meant for homogeneous data. We must either find homogenous processes within the heterogeneous data or extend the methodology. This is still an 
open problem, but successful completion might impact fatigue regulations for non-typical flights and aid future applications of control charts with non-homogeneous 
data.
}{Dans ce projet en cours, nous tentons de trouver des indicateurs de la fatigue des pilotes chez les pompiers de l'air en utilisant des données de vol brutes (emplacement, 
altitude, vitesse, lacet, tangage, roulis, etc. enregistrés chaque seconde). Nous appliquons les graphiques de contrôle Shewhart pour détecter les vols inhabituels 
en se basant sur les résumés des données. Chaque vol est différent mais les graphiques de contrôles sont prévus pour des données homogènes. Nous devons donc soit trouver des 
processus homogènes à l'intérieur de ces données hétérogènes ou nous devons élargir la méthodologie. Ceci est toujours un problème ouvert, mais une conclusion réussie 
pourrait avoir une incidence sur les règlements portant sur la fatigue pour les vols non-typiques et pourrait aider l'application future des graphiques de contrôle à 
des données non-homogènes.
}}
%% Talk sa-pb
\def\whenwhere{[Monday May 30 / lundi 30 mai, 15:45]\\}
{\Author{Pierre-Jérôme}{Bergeron}{bergeron}{1E-C5}\Author{Samuel}{Archibald}{archibald}{1E-C5}%
\abshead{\absauthor{PIERRE-JÉRÔME BERGERON}\absaffil{PJB Consulting}, \absauthor{SAMUEL ARCHIBALD}\absaffil{Université du Québec à Montréal}}
        {Valar Morghulis: A Survival Analysis of  'A Song of Ice and Fire'\newline
        Valar morghulis: une analyse de survie des livres du Trône de fer}

\absSideBySide{George R. R. Martin's {\it A Song of Ice and Fire} is an ongoing series of fantasy novels with a large cast of characters, and numerous, sometimes shocking deaths. Readers do not know what to expect in terms of the eventual resolution of the story's many threads. We apply here the methods of survival analysis to see if we can discern patterns that could inform the literary analysis of the text. We discuss data selection, the choice of variables and units of measurements in defining event times in fiction. We then compare survival between different groups of characters.
}{La série des livres du {\it Trône de fer} de George R.R. Martin comporte un grand nombre de personnages qui souvent meurent, parfois de manière choquante. Les lecteurs ne savent pas à quoi s'attendre quant au dénouement des nombreuses intrigues de l'histoire. On applique ici des méthodes d'analyse de survie pour voir s'il existe des schémas permettant d'éclairer l'analyse littéraire du texte. On discute de la sélection des données, du choix des variables et des unités de mesure pour définir des temps d'évènements dans la fiction. On compare ensuite la survie de différents groupes de personnages.
}}
%% Talk sa-dd
\def\whenwhere{[Monday May 30 / lundi 30 mai, 16:00]\\}
{\Author{Debbie}{Dupuis}{dupuis}{1E-C5}\Author{Marco}{Bee}{bee}{1E-C5}\Author{Luca}{Trapin}{trapin}{1E-C5}%
\abshead{\absauthor{DEBBIE DUPUIS}\absaffil{HEC Montreal}, \absauthor{MARCO BEE}\absaffil{University of Trento}, \absauthor{LUCA TRAPIN}\absaffil{IMT Institute for Advanced Studies Lucca}}
        {Realized Peaks over Threshold\newline
        «Realized Peaks over Threshold»}

\absSideBySide{Recent contributions to the financial econometrics literature exploit high-frequency data to improve models for daily asset returns. In this talk, we present a new class of dynamic extreme value models that profit from high-frequency data when estimating the tails of daily asset returns. Our Realized Peaks-Over-Threshold approach provides estimates for the tails of the time-varying conditional return distribution, and remains applicable even when data are non-stationary. An in-sample fit to S\&P 500 index returns shows that high-frequency data contain information on daily extreme returns beyond that included in low frequency data.
}{Les contributions récentes à la littérature de l'économétrie financière exploitent les données à haute fréquence afin d'améliorer les modèles pour les rendements quotidiens d'actifs.  Dans cet exposé, nous présentons une nouvelle classe de modèles dynamiques pour valeurs extrêmes qui bénéficient des données à haute fréquence lors de l'estimation des queues des rendements quotidiens d'actifs. Notre approche «Realized Peaks-Over-Threshold» reste applicable lorsque les données sont non stationnaires. Un ajustement «in-sample» aux rendements de l'indice SP500 montre que
les données à haute fréquence contiennent des informations sur les rendements quotidiens extrêmes 
au-delà de celles incluses dans les données à basse fréquence.
}}
%% Talk sa-nm
\def\whenwhere{[Monday May 30 / lundi 30 mai, 16:15]\\}
{\Author{Nicholas}{Mitsakakis}{mitsakakis}{1E-C5}\Author{Karen}{Bremner}{bremner}{1E-C5}\Author{Murray}{Krahn}{krahn}{1E-C5}%
\abshead{\absauthor{NICHOLAS MITSAKAKIS}\absaffil{University of Toronto}, \absauthor{KAREN BREMNER}\absaffil{University Health Network}, \absauthor{MURRAY KRAHN}\absaffil{THETA Collaborative}}
        {Investigating Structural Independence in Health Utility Instruments Using Graphical Models\newline
        Étude de l'indépendance structurelle des instruments d'utilité de santé au moyen de modèles graphiques}

\absSideBySide{In health economics cost effectiveness analyses rely on accurate estimation of health utility, a single global measure of health related quality of life (HRQoL). Health utilities are often measured with the use of questionnaire-type instruments, containing a number of items describing specific domains of HRQoL. The construction of these instruments relies on multi-attribute utility theory, assuming structural independence among the attributes. This property is rarely tested empirically. Here we applied discrete graphical models to multiple patient datasets to test the structural independence of a prostate cancer-specific instrument, Patient-Oriented Prostate Utility Scale (PORPUS-U). Results reveal undesired interdependence among items in PORPUS-U.
}{En économie de la santé, les analyses de rapports de coût-efficacité sont fondées sur une estimation précise de l'utilité de la santé,  une mesure globale unique de la qualité de vie liée à la santé. Les utilités de santé sont souvent mesurées au moyen d'instruments de type questionnaires, contenant un certain nombre d'éléments décrivant des domaines précis de qualité de vie liés à la santé. La conception de ces instruments repose sur la théorie de l'utilité multi-attributs, qui suppose l'indépendance structurelle des attributs. Cette hypothèse est rarement testée manière empirique. Dans cette étude, nous avons appliqué des modèles graphiques discrets à plusieurs jeux de données de patients pour vérifier l'indépendance structurelle d'un instrument propre au cancer de la prostate, à savoir l'échelle d'utilité orientée sur les patients atteints du cancer de la prostate (PORPUS-U). Les résultats révèlent une interdépendance non souhaitée entre les éléments de l'échelle d'utilité orientée sur les patients atteints du cancer de la prostate.
}}
%% Talk sa-dr
\def\whenwhere{[Monday May 30 / lundi 30 mai, 16:30]\\}
{\Author{David}{Riegert}{riegert}{1E-C5}\Author{David}{Thomson}{thomson}{1E-C5}\Author{Aaron}{Springford}{springford}{1E-C5}%
\abshead{\absauthor{DAVID RIEGERT}, \absauthor{DAVID THOMSON} \& \absauthor{AARON SPRINGFORD}\absaffil{Queen's University}}
        {Statistical Estimation of Transfer Functions to Examine the Relationship Between Geomagnetism and Power Systems\newline
        Estimation statistique des fonctions de transfert pour examiner les relations entre le géomagnétisme et les réseaux électriques}

\absSideBySide{Geomagnetic disturbances disrupt ground-based systems in a profound manner; increasing the rate of corrosion in pipelines, damaging equipment in power transmission systems, and interfering with communications. We will focus specifically on the relationship between Earth's magnetic field and induced currents in power lines. In order to describe this relationship, transfer functions were estimated using current methods which are robust to outliers. We discuss the shortcomings of such current methods in the context of a Northwest US power system.
}{Les perturbations géomagnétiques interrompent considérablement les systèmes terrestres. Elles augmentent le taux de corrosion dans les pipelines, endommagent l'équipement dans les réseaux de transport d'énergie et interfèrent avec les réseaux de communication. Nous mettrons particulièrement l'accent sur la relation entre le champ magnétique de la terre et les courants induits dans les lignes électriques. Nous décrirons cette relation par l'estimation des fonctions de transfert au moyen des méthodes actuelles qui sont robustes aux valeurs aberrantes. Nous discuterons des faiblesses de ces méthodes actuelles dans le contexte d'un réseau électrique du nord-ouest des États-Unis.
}}
%% Talk sa-ps
\def\whenwhere{[Monday May 30 / lundi 30 mai, 16:45]\\}
{\Author{Pavel}{Slavchev}{slavchev}{1E-C5}\Author{Russell}{Steele}{steele}{1E-C5}\Author{Ian}{Shrier}{shrier}{1E-C5}\Author{Ashley}{Naimi}{naimi}{1E-C5}\Author{Nathalie}{Auger}{auger}{1E-C5}%
\abshead{\absauthor{PAVEL SLAVCHEV} \& \absauthor{RUSSELL STEELE}\absaffil{McGill University}, \absauthor{IAN SHRIER}\absaffil{Lady Davis Institute, Jewish General Hospital and McGill University}, \absauthor{ASHLEY NAIMI}\absaffil{University of Pittsburgh}, \absauthor{NATHALIE AUGER}\absaffil{Institut National de la Santé Public du Québec and University of Montreal Research Centre}}
        {The Analysis of Live Births as a Competing Event to Stillbirths\newline
        L'analyse des naissances vivantes comme événement concurrent aux accouchements de mort-nés}

\absSideBySide{Understanding the causal risk factors for stillbirths should help develop more effective prevention programs. The Institut National de la Santé Public du Québec (INSPQ) birth registry data includes all births over the last 25 years in the province of Quebec. Classical time-to-event analyses (e.g. using Cox-proportional hazards models) for the INSQP registry data on stillborn infants are complicated by two problems: clustering of births within mothers and live births that censor the time to stillbirth. In this presentation, we show how modelling live births as competing events to stillbirths can address both of these problems.
}{Comprendre les facteurs de risque causaux des accouchements d'un mort-né devrait contribuer à élaborer des programmes de prévention efficaces. Les données du registre des naissances de l'Institut National de la Santé Public du Québec (INSPQ) comprennent toutes les naissances au cours des 25 dernières années dans la province du Québec. Les analyses classiques de temps d'événement (p. ex., les modèles de régression des risques proportionnels de Cox) des données sur les accouchements d'un mort-né du registre de l'INSPQ sont compliquées par deux problèmes : le regroupement des naissances chez les mères et les naissances vivantes qui censurent le temps jusqu'à l'accouchement d'un mort-né. Dans cet exposé, nous démontrons comment la modélisation des naissances vivantes comme événement concurrent aux accouchements d'un mort-né peut régler ces deux problèmes.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 2A-A: Gold Medal Address\\Allocution du r\'ecipiendaire de la M\'edaille d'or}
\begin{center}\large\bfseries Room/Salle: Welch - David Howes\end{center}
\label{abs-sid:gma}
\begin{center}{\large\bfseries Abstract/R\'esum\'e}
\end{center}
%% Talk gma-rl
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 8:45]\\}
{\Author{Richard}{Lockhart}{lockhart}{2A-A}%
\abshead{\absauthor{RICHARD LOCKHART}\absaffil{Simon Fraser University}}
        {Big Data, High Dimensions, Goodness-of-fit: Something of This Nature\newline
        Données volumineuses, grandes dimensions et adéquation : quelque chose de cette nature}

\absSideBySide{Motivated by Zonker's Uncle Duke, I intend to try to trace a thread through big data, high dimensional inference, goodness-of-fit testing, transformations, invariance, contiguity and the role of theory. The plan is to give a high level overview of my attitude to the role of theory in high dimensional and big data problems by wandering through some of the areas of research in which I have been involved.
}{Motivé par l'oncle Duke de Zonker, je tenterai de tracer un fil conducteur qui relie les données volumineuses, l'inférence en grandes dimensions, les tests d'adéquation, les transformations, l'invariance, la contigüité et le rôle de la théorie. L'objectif est de proposer un aperçu de mon attitude envers le rôle de la théorie dans les problèmes de grandes dimensions et de données volumineuses en parcourant certains domaines de recherche dans lesquels j'ai travaillé.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 2B-I1: Advanced Statistical and Machine Learning Techniques for High Dimensional Data Analysis\\Techniques statistiques et apprentissage machine avanc\'es pour l'analyse de donn\'ees en haute dimension}
\begin{center}{\large Organizer and Chair / Responsable et président:  Abbas Khalili (McGill University)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Welch - David Howes\end{center}
\label{abs-sid:asm2}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk asm2-rt
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 10:20]\\}
{\Author{Ryan}{Tibshirani}{tibshirani}{2B-I1}\Author{Richard}{Lockhart}{lockhart}{2B-I1}\Author{Jonathan}{Taylor}{taylor}{2B-I1}\Author{Robert}{Tibshirani}{tibshirani}{2B-I1}\Author{Will}{Fithian}{fithian}{2B-I1}\Author{Jing}{Lei}{lei}{2B-I1}\Author{Larry}{Wasserman}{wasserman}{2B-I1}%
\abshead{\absauthor{RYAN TIBSHIRANI}\absaffil{Carnegie Mellon University}, \absauthor{RICHARD LOCKHART}\absaffil{Simon Fraser University}, \absauthor{JONATHAN TAYLOR} \& \absauthor{ROBERT TIBSHIRANI}\absaffil{Stanford University}, \absauthor{WILL FITHIAN}\absaffil{University of California, Berkeley}, \absauthor{JING LEI} \& \absauthor{LARRY WASSERMAN}\absaffil{Carnegie Mellon University}}
        {Recent Advances in Selective Inference\newline
        Avancées récentes en inférence sélective}

\absSideBySide{I will talk about new sets of tools for inference after model selection. The highlight will be doing inference along steps of the forward stepwise, least angle regression, and lasso paths, though the framework for inference is applicable well beyond these cases, and other relevant problems will be briefly described as well. I will also discuss some asymptotic results about the robustness of the proposed tests to non-Gaussian noise distributions, and if time permits, a completely different framework based on conformal inference, that is essentially distribution-free. This all represents joint work with many different authors.
}{Je présenterai de nouveaux types d'outils pour l'inférence après sélection de modèle. Je traiterai notamment de l'inférence dans le cadre de la régression multiple ascendante, de la régression de least angle et LASSO, même si le cadre de l'inférence est applicable à bien d'autres cas, et je décrirai brièvement d'autres problèmes pertinents. Je discuterai également certains résultats asymptotiques relatifs à la robustesse des tests proposés pour les distributions de bruit non gaussien et, si le temps le permet, d'un cadre complètement différent basé sur l'inférence conforme, qui est essentiellement sans distribution. Il s'agit de travaux communs avec de nombreux auteurs différents.
}}
%% Talk asm2-anv
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 10:50]\\}
{\Author{Anand N.}{Vidyshankar}{vidyshankar}{2B-I1}%
\abshead{\absauthor{ANAND N. VIDYSHANKAR}\absaffil{George Mason University}}
        {Efficiency Considerations in Post Model Selection Inference\newline
        Considérations d'efficacité dans l'inférence post-sélection de modèle}

\absSideBySide{It is common practice in high-dimensional data analysis that a model selection is first performed and then inference is carried out using the selected model presuming that the chosen model is the true model; that is, without accounting for model selection uncertainty. Recently, methods such as \emph{clean and screen} are being used to account for model selection uncertainty. However, efficiency properties of the resulting statistical procedures are largely unknown. In this presentation, we provide a systematic account of efficiency properties of post-selection estimators. In the process we address some foundational questions concerning the role of moderate deviation theory in the study of statistical efficiency.
}{Il est courant en analyse de données de grande dimension de commencer par la sélection de modèle puis de passer à l'inférence sur la base du modèle sélectionné en présumant que celui-ci est le vrai modèle, c'est-à-dire sans tenir compte de l'incertitude de sélection. Récemment, des méthodes dites de nettoyage et filtrage ont été développées pour tenir compte de cette incertitude. Cependant, l'efficacité des procédures statistiques résultantes demeure largement inconnue. Dans cette présentation, nous proposons une étude systématique des propriétés d'efficacité des estimateurs post-sélection. Nous répondrons notamment à des questions fondamentales concernant le rôle de la théorie de la déviation modérée dans l'étude de l'efficacité statistique.
}}
%% Talk asm2-ag
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 11:20]\\}
{\Author{Yi}{Yang}{yang}{2B-I1}\Author{Hui}{Zou}{zou}{2B-I1}\Author{Wei}{Qian}{qian}{2B-I1}%
\abshead{\absauthor{YI YANG}\absaffil{McGill University}, \absauthor{HUI ZOU}\absaffil{University of Minnesota}, \absauthor{WEI QIAN}\absaffil{Rochester Institute of Technology}}
        {Variable Selection Deviation for High-dimensional Classification Methods\newline
        Déviation de la sélection des variables pour les méthodes de classification à grande dimension}

\absSideBySide{Despite wide applications of the regularization methods in many areas,
sometimes we may question the reliability of the feature selection
results, mainly for two reasons: (i) the outcome of selection depends
heavily on the choice the tuning parameter, which could be very
unstable if given small amount of data; (ii) although consistency in
variable selection has been established for various regularization
methods, it often depends on the assumption that the p-dimensional
variable are sparse with many components being zero, which usually is
unlike to hold for observational data. Thus we hope to develop the
variable selection deviation (VSD) measures in the high-dimensional
classification setting to evaluate the reliability of a set of selected
variables.
}{Malgré les vastes applications des méthodes de régularisation dans de nombreux domaines, nous pouvons parfois remettre en question la fiabilité des résultats de la sélection caractéristique, principalement pour deux raisons : (i) le résultat de la sélection dépend fortement du choix du paramètre de réglage qui pourrait être très instable en présence d'une petite quantité de données; (ii) bien que la convergence dans la sélection des variables ait été établie pour diverses méthodes de régularisation, elle dépend souvent de l'hypothèse que les variables à p dimensions sont rares avec de nombreuses composantes établies à zéro, ce qui ne tient généralement pas pour les données d'observation. Ainsi, nous espérons développer des mesures de la déviation de la sélection des variables (DSV) dans le contexte de la classification à grande dimension pour évaluer la fiabilité d'un ensemble choisi de variables.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 2B-I2: Advancements to State-Space Models for Fisheries Science\\Progr\`es r\'ecents dans les mod\`eles \`a espace d'\'etats pour les sciences halieutiques}
\begin{center}{\large Chair/Président: Rick Routledge (Simon Fraser University)\protect\\[5pt]
Organizer/Responsable: Joanna Mills-Flemming (Dalhousie University)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 242\end{center}
\label{abs-sid:asm}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk asm-wa
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 10:20]\\}
{\Author{William}{Aeberhard}{aeberhard}{2B-I2}\Author{Joanna}{Mills Flemming}{millsflemming}{2B-I2}\Author{Eva}{Cantoni}{cantoni}{2B-I2}\Author{Chris}{Field}{field}{2B-I2}\Author{Ximing}{Xu}{xu}{2B-I2}%
\abshead{\absauthor{WILLIAM AEBERHARD} \& \absauthor{JOANNA MILLS FLEMMING}\absaffil{Dalhousie University}, \absauthor{EVA CANTONI}\absaffil{University of Geneva}, \absauthor{CHRIS FIELD}\absaffil{Dalhousie University}, \absauthor{XIMING XU}\absaffil{Nankai University}}
        {Robust and Consistent Estimation of Fixed Parameters in General State-Space Models\newline
        Estimation robuste et convergente des paramètres fixes de modèles espace-états}

\absSideBySide{Fixed parameters in State-Space Models (SSMs) are traditionally estimated by maximum likelihood and typically include regression, auto-regression and scale parameters. The sensitivity of these estimates to deviations from the assumed model is problematic, all the more so since distributional assumptions about latent variables cannot be verified. Standard robust estimation techniques for generalized linear and time series models cannot be directly extended to SSMs, due mainly to the high-dimensional integrals that generally need to be approximated. We propose a robust and consistent estimating method by downweighting observations on the joint log-likelihood scale and by approximating the marginal log-likelihood by Laplace's method. Encouraging simulation results are presented with an application to a fish stock assessment.
}{Les paramètres fixes des modèles espace-états sont traditionnellement estimés par le maximum de vraisemblance et incluent typiquement des coefficients de régression, d'auto-régression et d'échelle. La sensibilité de ces estimations aux déviations par rapport au modèle postulé est problématique, d'autant plus que les postulats au niveau latent sont généralement invérifiables. Les estimateurs robustes pour modèles linéaires généralisés et séries temporelles ne peuvent être directement appliqués à cause d'intégrales de haute dimension à approximer. Nous proposons une méthode d'estimation robuste et convergente où des poids sont ajoutés à la vraisemblance jointe et la vraisemblance marginale est approximée par la méthode de Laplace. En sus de résultats de simulations encourageants, une évaluation de stock de poissons sera présentée.
}}
%% Talk asm-dc
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 10:50]\\}
{\Author{David}{Campbell}{campbell}{2B-I2}%
\abshead{\absauthor{DAVID CAMPBELL}\absaffil{Simon Fraser University}}
        {Nested Laplace Approximation Diagnostics and Fixes\newline
        Diagnostic et correction de l'approximation de Laplace emboîtée}

\absSideBySide{Laplace Approximations are used to marginalize out nuisance random effect variables from state space models. Under linear conditions with Gaussian perturbations this approximation is exact. In more complex models the approximation can be far from reasonable. Typically one must resort to importance sampling or MCMC as a marginalization tool to assess appropriateness of the approximation. In this work a quick and dirty estimate of the Kullback-Leibler (KL) divergence between the Laplace approximation and the target density to marginalize out parameters is presented. In the event of a large KL divergence we propose a probabilistic numerical integrator to marginalize over nuisance parameters. The proposed approach provides an intermediate step in the trade-off between computational complexity and level of approximation.
}{Les approximations de Laplace servent à marginaliser les variables de bruit à effet aléatoire des modèles espace-état. Dans des conditions linéaires avec perturbations gaussiennes, cette approximation est exacte. Dans les modèles plus complexes, l'approximation peut au contraire être très imprécise. Il faut généralement recourir à l'échantillonnage par importance ou au MCMC comme outil de marginalisation pour évaluer la qualité de l'approximation. Nous présentons une estimation rapide de la divergence de Kullback-Leibler (KL) entre l'approximation de Laplace et la densité cible pour marginaliser les paramètres. Si la divergence de KL est élevée, nous proposons un intégrateur numérique probabiliste qui permet de marginaliser les paramètres de nuisance. Cette approche constitue une étape intermédiaire dans le compromis entre complexité computationnelle et niveau d'approximation.
}}
%% Talk asm-jmf
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 11:20]\\}
{\Author{Joanna Mills}{Flemming}{flemming}{2B-I2}%
\abshead{\absauthor{JOANNA MILLS FLEMMING}\absaffil{Dalhousie University}}
        {Addressing Statistical Challenges in Fisheries Science\newline
        Défis statistiques en sciences halieutiques}

\absSideBySide{In a recent letter from Canada's Prime Minister to its Minister of Fisheries, top priorities included working to ``protect the health of fish stocks, ..., use scientific evidence and the precautionary principle, and take into account climate change, when making decisions affecting fish stocks and ecosystem management''. Fisheries science is faced with the challenge of providing management advice based on noisy survey and commercial fishery data that may only provide indirect information about fish stock size. Here I highlight our CANSII CRT's recent progress in building robust statistical methodologies for SSMs that are suitable for both fish stock assessments and for identifying drivers of animal movement in order to predict responses to climate change.
}{Une récente lettre du Premier ministre du Canada à son ministre des Pêches indique qu'une priorité absolue doit être accordée à «~protéger la santé des stocks halieutiques, ..., s'appuyer sur les preuves scientifiques et le principe de la prudence et tenir compte des changements climatiques dans la prise de décisions ayant des répercussions sur les stocks halieutiques et la gestion des écosystèmes.~» Les sciences halieutiques ont pour défi de donner des conseils en matière de gestion sur la base de données d'enquêtes bruitées, ou en provenance des pêches commerciales, qui n'offrent qu'indirectement une information sur la taille des stocks. Je discuterai des récents progrès de notre projet de recherche en collaboration de l'INCASS~: nous avons développé des méthodologies statistiques rigoureuses pour les modèles espace-état (MEE) adaptées à la fois à l'évaluation des stocks halieutiques et à l'identification des moteurs de mouvements d'animaux, nous permettant ainsi de mieux prédire les réactions au changement climatique.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 2B-I3: Contributions of Professor Ian B. MacNeill to Statistical Theory, Methodology, Teaching and Practice\\Contributions du professeur Ian B. MacNeill \`a la th\'eorie statistique, M\'ethodologie, enseignement et la pratique}
\begin{center}{\large Chair/Président: Gary Umphrey (University of Guelph)\protect\\[5pt]
Organizer/Responsable: Krishna Jandhyala (Washington State University)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Academic South 217\end{center}
\label{abs-sid:cpi}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk cpi-ae
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 10:20]\\}
{\Author{Abdel}{El-Shaarawi}{elshaarawi}{2B-I3}\Author{Sylvia R.}{Esterby}{esterby}{2B-I3}%
\abshead{\absauthor{ABDEL EL-SHAARAWI}\absaffil{The American University in Cairo}, \absauthor{SYLVIA R. ESTERBY}\absaffil{University of British Columbia Okanagan}}
        {Ian B. MacNeill and Environmetrics\newline
        Ian B. MacNeill et l'environnemétrie}

\absSideBySide{Ian MacNeill made significant contributions to statistics for over 40 years at both the national and international levels. He made notable contributions to many 
areas of research, particularly to time series methods and change point analysis with applications to the environmental and health sciences. Ian also established 
the Department of Statistics at the University of Western Ontario on July 1, 1980.  In this talk we will focus our discussion on his role in establishing The 
International Environmetrics Society (TIES) and its journal Environmetrics.
}{Ian MacNeill a fait d'importantes contributions aux statistiques pendant plus de 40 ans au niveau à la fois national et international. Il contribua de façon 
remarquable à plusieurs domaines de recherche, particulièrement les méthodes de séries chronologiques et l'analyse des points de changement avec applications aux 
sciences environnementales et de la santé. Ian a aussi créé le département de statistique à l'université Western Ontario le 1er juillet 1980. Dans cet exposé, je 
me concentrerai sur son rôle dans la création de l'International Environmetrics Society (TIES) et de son journal Environmetrics.
}}
%% Talk cpi-en
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 10:40]\\}
{\Author{Elena}{Naumova}{naumova}{2B-I3}%
\abshead{\absauthor{ELENA NAUMOVA}\absaffil{Tufts University}}
        {Seasonality Analysis in Public Health Applications\newline
        Analyse de saisonnalité dans la santé publique}

\absSideBySide{Several aspects of seasonality analysis are essential in public health applications. This includes an ability to formally compare timing, amplitude and duration 
of seasonal peaks across locations, populations, time, and causal agents. The contribution of Professor Ian B. MacNeill to the methodology development for 
seasonality assessment in health surveillance systems and specifically, the need for preservation of information relevant to health outcome of interest at the 
finest temporal resolution, the importance of considering social calendars in modeling seasonality, and the rationales why seasonal patterns have to be taken 
into account in designing cross-sectional and prospective epidemiological studies are discussed.
}{Plusieurs aspects de l'analyse de saisonnalité sont essentiels dans des applications du domaine de la santé publique. Cela inclut la capacité de comparer 
formellement le moment, l'amplitude et la durée des pics saisonniers à travers les emplacements, les populations et les agents causaux. La contribution du professeur 
Ian B. MacNeill au développement de la méthodologie pour l'évaluation de saisonnalité dans les systèmes de surveillance de la santé et particulièrement, le besoin 
de préserver l'information pertinente aux résultats en matière de santé avec la meilleure résolution temporelle possible, l'importance de considérer les calendriers 
sociaux lors de la modélisation de saisonnalité, et les raisons pour lesquelles les cycles saisonniers doivent être pris en considération lors de la conception 
d'études épidémiologiques prospectives et transversales sont abordés.
}}
%% Talk cpi-sp
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 11:02]\\}
{\Author{Serge}{Provost}{provost}{2B-I3}%
\abshead{\absauthor{SERGE PROVOST}\absaffil{University of Western Ontario}}
        {On the q-Analogues of the Logistic and Generalized Extreme Value Distributions\newline
        Les analogues q des lois de valeurs extrêmes généralisées et logistiques}

\absSideBySide{The q-analogues of the logistic, generalized extreme value and related
distributions are being introduced, the additional parameter allowing for
greater flexibility for modeling purposes. It will be explained that the
supports of these distributions can be infinite, bounded above or below,
or even finite. Certain statistical functions such as their moments and
quantile functions shall be derived. Potential fields of application
include reliability theory, hydrology, biostatistics and survival
analysis.
}{Les analogues q des lois de valeurs extrêmes généralisées et logistiques et autres lois connexes sont présentées, le paramètre additionnel permettant une plus grande 
flexibilité à des fins de modélisation. Nous expliquerons que les supports de ces lois peuvent être infinis, avoir une borne supérieure ou inférieure, ou même être 
finis. Certaines fonctions statistiques telles que leurs fonctions de quantiles ou de moments seront dérivées. Parmi les domaines d'application possibles, il y a la 
théorie de fiabilité, l'hydrologie, la biostatistique et l'analyse de survie.
}}
%% Talk cpi-vkj
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 11:24]\\}
{\Author{Venkata Krishna}{Jandhyala}{jandhyala}{2B-I3}\Author{Shelemyahu}{Zacks}{zacks}{2B-I3}%
\abshead{\absauthor{VENKATA KRISHNA JANDHYALA}\absaffil{Washington State University}, \absauthor{SHELEMYAHU ZACKS}\absaffil{State University of New York at Binghmton}}
        {Contributions of Dr. MacNeill to Change-Point Methodology and its Environmental Applications\newline
        Les contributions du Dr MacNeill à la méthodologie des points de rupture et ses applications environnementales}

\absSideBySide{Dr. MacNeill's contributions to change-point methodology and its applications are fundamental and are well known in the statistical literature.  This talk reviews 
his main contributions to selected areas including: (i) sequences of independent random variables, (ii) linear regression models with possibly dependent errors, and 
(iii) spatial models.  The review concludes with some examples from environmental time series.
}{Les contributions du Dr. MacNeill à la méthodologie des points de rupture et ses applications sont fondamentales et bien connues dans la littérature statistique. Cet 
exposé examine ses principales contributions à certains domaines, y compris: (i) séquences de variables aléatoires indépendantes, (ii) modèles de régression linéaire 
avec de possibles erreurs dépendantes, et (iii) modèles spatiaux. L'exposé se conclue avec quelques exemples de séries chronologiques environnementales.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 2B-I4: Graduate Students in Actuarial Science\\\'Etudiants de cycles sup\'erieurs en actuariat}
\begin{center}{\large Organizer and Chair / Responsable et président:  Andrei Badescu (University of Toronto)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 246\end{center}
\label{abs-sid:gsa}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk gsa-ne
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 10:20]\\}
{\Author{Nicolas}{Essis-Breton}{essisbreton}{2B-I4}\Author{Patrice}{Gaillardetz}{gaillardetz}{2B-I4}%
\abshead{\absauthor{NICOLAS ESSIS-BRETON}\absaffil{University of Concordia}, \absauthor{PATRICE GAILLARDETZ}\absaffil{Concordia University}}
        {Multi Trial Monte Carlo\newline
        Essais multiples de Monte-Carlo}

\absSideBySide{Multi Trial Monte Carlo allows to price arbitrarily complex path-dependent American option.
The algorithm relies on the theory of approximate dynamic programming.
In this talk, we review the avalaible valuation techniques,
explain why the pricing of certain options remains a challenge,
and describe the solution that our algorithm offers.
}{L'essai multiple de Monte-Carlo permet d'établir le prix d'options américaines complexes et dépendantes du chemin parcouru. L'algorithme est fondé sur la théorie de programmation dynamique. Dans cet exposé, nous examinons les techniques d'évaluation existantes, nous expliquons pourquoi l'établissement des prix de certaines options reste un défi et nous décrivons la solution que notre algorithme apporte.
}}
%% Talk gsa-dl
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 10:35]\\}
{\Author{Dongchen}{Li}{li}{2B-I4}\Author{Xinfu}{Chen}{chen}{2B-I4}\Author{David}{Landriault}{landriault}{2B-I4}\Author{Bin}{Li}{li}{2B-I4}%
\abshead{\absauthor{DONGCHEN LI}\absaffil{University of Waterloo}, \absauthor{XINFU CHEN}\absaffil{University of Pittsburgh}, \absauthor{DAVID LANDRIAULT} \& \absauthor{BIN LI}\absaffil{University of Waterloo}}
        {On Minimizing Drawdown Risks of Lifetime Investments\newline
        Minimisation des risques de prélèvement sur les investissements à vie}

\absSideBySide{Drawdown measures the decline of portfolio value from its historic high-water mark. We study lifetime investment problems aiming at minimizing drawdown-related risk measures. Under the Black-Scholes framework, we examine two financial market models: a market with two risky assets, and a market with a risk-free asset and a risky asset. Closed-form optimal trading strategies are derived by solving the associated Hamilton-Jacobi-Bellman (HJB) equations. We show that it is optimal to minimize the portfolio variance when the fund value is at its historic high-water mark. Moreover, when the fund value drops, the proportion of wealth invested in the asset with a higher instantaneous rate of return should be increased.
}{Le prélèvement mesure le déclin de la valeur d'un portefeuille par rapport à son pic historique. Nous étudions des problèmes d'investissement à vie visant à minimiser les mesures de risque lié au prélèvement. En considérant le cadre de Black-Scholes, nous examinons deux modèles de marchés financiers~: un marché avec deux actifs à risque et un autre avec un actif sans risque et un actif à risque. Nous dérivons des stratégies optimales d'exécution des opérations de formes fermées en résolvant les équations de Hamilton-Jacobi-Bellman (HJB) associées. Nous montrons qu'il est optimal de minimiser la variance du portefeuille lorsque la valeur du fonds est à son pic historique. De plus, lorsque la valeur du fonds chute, il faut augmenter la proportion de richesse investie dans l'actif dont le taux de rendement de retour instantané est le plus élevé.
}}
%% Talk gsa-hl
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 10:50]\\}
{\Author{Haiyan}{Liu}{liu}{2B-I4}\Author{Jun}{Cai}{cai}{2B-I4}\Author{Ruodu}{Wang}{wang}{2B-I4}%
\abshead{\absauthor{HAIYAN LIU}, \absauthor{JUN CAI} \& \absauthor{RUODU WANG}\absaffil{University of Waterloo}}
        {Asymptotic Equivalence of Risk Measures under Dependence Uncertainty\newline
        Équivalence asymptotique des mesures de risque en cas d'incertitude de la dépendance}

\absSideBySide{We study the aggregate risk of a large inhomogeneous portfolio with dependence uncertainty, evaluated by a generic risk measure. We establish general asymptotic equivalence results for the classes of distortion risk measures and convex risk measures under different mild conditions. The results implicitly suggest that it is only reasonable to implement a coherent risk measure for an aggregate risk of a large portfolio with uncertainty in the dependence structure, a relevant situation for risk management practice.
}{Nous étudions le risque global d'un gros portefeuille non homogène avec incertitude concernant la dépendance, tel qu'évalué par une mesure du risque générique. Nous établissons les résultats d'équivalence asymptotique généraux pour les classes de mesures de risque de distorsion et de risque convexe sous diverses hypothèses faibles. Les résultats suggèrent implicitement qu'il n'est que raisonnable d'utiliser une mesure de risque cohérente pour le risque global d'un gros portefeuille avec incertitude dans la structure de dépendance, situation pertinente en pratique pour la gestion des risques.
}}
%% Talk gsa-jz
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 11:20]\\}
{\Author{Jingong}{Zhang}{zhang}{2B-I4}\Author{Ken Seng}{Tan}{tan}{2B-I4}\Author{Chengguo}{Weng}{weng}{2B-I4}%
\abshead{\absauthor{JINGONG ZHANG}, \absauthor{KEN SENG TAN} \& \absauthor{CHENGGUO WENG}\absaffil{University of Waterloo}}
        {Optimal Hedging with Basis Risk under Mean-Variance Criteria\newline
        Couverture optimale avec risque de base sous critères de moyenne-variance}

\absSideBySide{Optimal hedging for European options is studied under a mean-variance framework in the presence of basis risk, where the underlying asset is non-tradable and replaced by another closely related and tradable asset. The problem is formulated to determine the subgame Nash equilibrium hedging strategy. This problem differs from classic dynamic programming problems in that its objective function is not separable and therefore, the Bellman optimality principle does not apply. Optimal control process is derived by resorting to dynamic programming technique and an extended HJB equation. A closed-form optimal control process is obtained with the aid of a change-of-measure technique.
}{J'étudie la couverture optimale pour des options européennes dans un cadre de moyenne-variance en présence de risque de base, où l'actif sous-jacent n'est pas négociable et est remplacé par un autre actif étroitement lié négociable. Le problème est formulé de sorte qu'il permette de déterminer la stratégie de couverture d'équilibre de Nash par sous-jeux. Ce problème se différencie des problèmes de programmation dynamique classiques~: sa fonction objectif n'est pas séparable si bien que le principe d'optimalité de Bellman ne s'applique pas. Je dérive le processus de contrôle optimal en utilisant une technique de programmation dynamique et une équation de HJB élargie. J'obtiens un processus de contrôle optimal de forme fermée avec une technique de changement de mesure.
}}
%% Talk gsa-rz
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 11:35]\\}
{\Author{Ruixi}{Zhang}{zhang}{2B-I4}\Author{Kristina}{Sendova}{sendova}{2B-I4}%
\abshead{\absauthor{RUIXI ZHANG}\absaffil{Western University}, \absauthor{KRISTINA SENDOVA}\absaffil{The University of Western Ontario}}
        {On a Renewal Risk Process under a Dividend Barrier Strategy\newline
        Processus de risque de renouvèlement sous une stratégie de barrière de dividendes}

\absSideBySide{We consider a renewal risk process in the presence of a constant dividend barrier. Using probabilistic arguments, the probability of ruin is shown to be~$1$, if either the claim sizes exceed the barrier with positive probability or $0$ is a point of increase of the inter-claim time. For $K_{n}$-distributed inter-claim times, we offer a revised proof regarding the number of roots to the generalized Lundberg's equation. The density and moments of the time to ruin are approximated numerically for generalized Erlang($n$) inter-claim times and rational-distributed claim sizes. However, if neither condition is satisfied, the probability of ruin may be reduced to~$0$. A dividend-reinsurance strategy inspired by the last observation is discussed. Finally, generalizations to certain dependent risk processes are included.
}{Nous considérons un processus de risque de renouvèlement en présence d'une barrière de dividendes constante. En utilisant des arguments probabilistes, nous montrons que la probabilité de ruine est de~$1$, si les tailles de réclamations dépassent la barrière avec une probabilité positive ou si $0$ est un point de hausse du temps entre réclamations. Pour des temps entre réclamations de distribution $K_{n}$, nous proposons une preuve révisée concernant le nombre de racines de l'équation de Lundberg généralisée. Nous effectuons une approximation numérique de la densité et des moments du temps avant la ruine pour des temps entre réclamations d'Erlang($n$) généralisés et des tailles de réclamations à distribution rationnelle. Cependant, si aucune condition n'est remplie, la probabilité de ruine peut être réduite à~$0$. Nous discutons d'une stratégie de réassurance des dividendes inspirée par cette dernière observation. Enfin, nous incluons des généralisations à certains processus de risques dépendants.
}}
%% Talk gsa-mal
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 11:50]\\}
{\Author{Mohamed Amine}{Lkabous}{lkabous}{2B-I4}\Author{Irmina}{Czarna}{czarna}{2B-I4}\Author{Jean-François}{Renaud}{renaud}{2B-I4}%
\abshead{\absauthor{MOHAMED AMINE LKABOUS}\absaffil{Université du Québec à Montréal}, \absauthor{IRMINA CZARNA}\absaffil{University of Wrocław}, \absauthor{JEAN-FRANÇOIS RENAUD}\absaffil{Université du Québec à Montréal}}
        {Parisian ruin for a refracted Lévy process\newline
        Ruine Parisienne pour un processus de Lévy réfracté}

\absSideBySide{Parisian ruin occurs if the time spent below zero by the risk process is longer than a fixed delay. In this talk, we investigate Parisian ruin for a refracted Lévy process. We generalize the result of Loeffen, Czarna and Palmowski (2013) for the probability of Parisian ruin of a standard Lévy insurance risk process. Other fluctuation identities with Parisian delay will be presented. Finally, we will give examples to illustrate the results.
}{Dans cet exposé, on s’intéresse à la ruine parisienne pour un processus de Lévy réfracté. Nous généralisons le résultat de Loeffen, Czarna and Palmowski (2013) pour la probabilité de ruine parisienne pour les processus Lévy spectrallement négatifs.  D'autres identités avec délai parisien seront aussi présentées. Enfin, nous présenterons quelques exemples pour illustrer les résultats.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 2B-I5: Recent Developments in Designs of Statistical Experiments. In honour of the 65th birthday of Professor Douglas P. Wiens\\Progr\`es r\'ecents en planification d'exp\'eriences. En l'honneur du 65e anniversaire du professeur Douglas P. Wiens}
\begin{center}{\large Organizer and Chair / Responsable et président:  Xiaojian Xu (Brock University)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 243\end{center}
\label{abs-sid:rdd}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk rdd-hd
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 10:20]\\}
{\Author{Holger}{Dette}{dette}{2B-I5}\Author{Kirsten}{Schorning}{schorning}{2B-I5}\Author{Kathrin}{Möllenhoff}{mollenhoff}{2B-I5}\Author{Stanislav}{Volgushev}{volgushev}{2B-I5}\Author{Frank}{Bretz}{bretz}{2B-I5}%
\abshead{\absauthor{HOLGER DETTE}\absaffil{Ruhr-Universitaet Bochum}, \absauthor{KIRSTEN SCHORNING} \& \absauthor{KATHRIN MÖLLENHOFF}\absaffil{Ruhr-Universität Bochum}, \absauthor{STANISLAV VOLGUSHEV}\absaffil{Cornell University}, \absauthor{FRANK BRETZ}\absaffil{Novartis, Basel}}
        {Statistical Methodogly  for Comparing Curves\newline
        Méthodologie statistique pour la comparaison de courbes}

\absSideBySide{An important  problem in drug development is to establish the similarity between two dose response curves. We propose statistical methodogy improving the current 
state of the art in two dirctions. On the one hand optimal designs are constructed minimizing the width of the confidence band for the difference between the 
regression functions, which is currently used for a test of similarity. The use of the optimal designs proposed yields a reduction of the width of the confidence 
band by more than 50 percent. On the other hand a new test for the hypothesis of ``similarity'' is developed, and it is demonstrated that this test has substantially 
more power than the test which is based on the confidence band.
}{Un problème important dans le développement de médicaments est la détermination des similitudes entre deux courbes dose-effet. Nous proposons une méthodologie statistique 
pour améliorer l'état actuel des choses dans deux directions. D'une part, des plans optimaux sont construits pour minimiser la largeur de la bande de confiance 
pour la différence entre les fonctions de régression, qui est présentement utilisée pour un test de similarité. L'utilisation des plans optimaux proposés mène 
à une réduction de la largeur de la bande de confiance de plus de 50 pour cent. D'autre part, un nouveau test pour l'hypothèse de «similarité» est développé et il est 
démontré que ce test a considérablement plus de puissance que le test fondé sur la bande de confiance.
}}
%% Talk rdd-lk
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 10:50]\\}
{\Author{Linglong}{Kong}{kong}{2B-I5}\Author{Doug}{Wiens}{wiens}{2B-I5}%
\abshead{\absauthor{LINGLONG KONG} \& \absauthor{DOUG WIENS}\absaffil{University of Alberta}}
        {Robust Designs for Nonlinear Quantile Regression\newline
        Plans robustes pour la régression quantile non linéaire}

\absSideBySide{We give methods for the construction of designs for regression models, when the purpose of the investigation is the estimation of the conditional quantile function, 
and the estimation method is nonlinear quantile regression. The designs are robust against misspecified response functions, and against unanticipated 
heteroscedasticity. The methods are illustrated by examples.
}{Nous offrons des méthodes pour la construction de plans pour les modèles de régression lorsque la recherche vise à estimer la fonction quantile conditionnelle 
et que la méthode d'estimation est une régression quantile non linéaire. Les plans sont robustes face à des fonctions de réponses mal spécifiées et face à une 
hétéroscédasticité imprévue. Les méthodes sont illustrées avec des exemples.
}}
%% Talk rdd-jz
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 11:20]\\}
{\Author{Julie}{Zhou}{zhou}{2B-I5}%
\abshead{\absauthor{JULIE ZHOU}\absaffil{University of Victoria}}
        {Computing Optimal Designs via Convex Optimization\newline
        Calculer des plans optimaux par l'optimisation convexe}

\absSideBySide{Optimal designs of experiments have applications in many research fields.  When optimal designs are hard to construct analytically, we can use numerical algorithms 
to compute them. Various algorithms have been developed, including multiplicative, simulated annealing, coordinate exchange, semi-infinite programming, and genetic 
algorithms. In this talk, we will introduce another powerful algorithm for finding optimal designs for linear and nonlinear regression models with discrete design 
space.  This algorithm is developed to solve semidefinite programming (SDP) problems.  SDP problems form a special class of convex optimization problems,  which have 
a linear objective function subject to a linear matrix being positive semidefinite.  We will show that many optimal design problems can be transformed into SDP 
problems. Examples using the algorithm will be presented.
}{Les plans d'expériences optimaux ont des applications dans plusieurs domaines de recherche. Lorsque les plans optimaux sont difficiles à construire 
analytiquement, nous pouvons utiliser des algorithmes numériques pour les calculer. Différents algorithmes ont été développés, y compris le recuit simulé et 
multiplicatif, l'échange coordonné, la programmation semi-infinie, et les algorithmes génétiques. Dans cet exposé, nous présenterons un autre algorithme puissant 
pour trouver des plans optimaux pour les modèles de régression linéaire et non linéaire avec espace de design discret. Cet algorithme est développé pour 
résoudre les problèmes de programmation semi-définis (SDP). Les problèmes SDP constituent une classe spéciale de problèmes d'optimisation convexe qui ont 
une fonction objective soumise à une matrice linéaire positive semi-définie. Nous démontrerons que plusieurs problèmes de plans optimaux peuvent être transformés 
en problèmes SDP. Des exemples utilisant l'algorithme seront présentés.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 2B-C1: Biostatistics: Applications\\Biostatistique : applications}
\begin{center}{\large Chair/Président: Lehana Thabane (McMaster University)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 255\end{center}
\label{abs-sid:ba}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk ba-mek
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 10:20]\\}
{\Author{Mohammad Ehsanul}{Karim}{karim}{2B-C1}\Author{Robert W.}{Platt}{platt}{2B-C1}\Author{Helen}{Tremlett}{tremlett}{2B-C1}\Author{BeAMS}{Study group}{studygroup}{2B-C1}%
\abshead{\absauthor{MOHAMMAD EHSANUL KARIM} \& \absauthor{ROBERT W. PLATT}\absaffil{McGill University}, \absauthor{HELEN TREMLETT} \& \absauthor{BEAMS STUDY GROUP}\absaffil{University of British Columbia}}
        {Estimating Inverse Probability Weights using Super Learner when Weight-Model Specification is Unknown in a Marginal Structural Cox Model Context: An Application to Multiple Sclerosis\newline
        Estimation de poids de probabilité inverse à l'aide du super apprentissage lorsque la spécification des poids du modèle est inconnue dans un modèle de Cox structurel marginal Contexte : Une application à la sclérose en plaques}

\absSideBySide{This study is motivated by the investigation of the impact of beta-interferon treatment in delaying disability progression in subjects with multiple sclerosis from British Columbia (1995-2008) using a marginal structural Cox model (MSCM). MSCM estimates are known to be highly sensitive to the weight-model misspecification. Through simulation, we show that, with a misspecified MSCM weight-model, Super Learner with a rich set of candidate algorithms gives lower MSE of the treatment effect than using logistic regression model or other statistical learning approaches. We apply the findings of the simulation studies in fitting the MSCM to the multiple sclerosis cohort data.
}{Cette étude est motivée par l'étude de l'impact du traitement par l'interféron bêta visant à retarder la progression du handicap chez les sujets atteints de sclérose en plaques en Colombie-Britannique (1995-2008) en utilisant un modèle de Cox structurel marginal (MCSM). Les estimations MCSM sont reconnues pour être très sensibles aux erreurs de spécification du poids du modèle. À l'aide de simulations, nous montrons qu'avec une erreur de spécification du poids du modèle, le super apprentissage avec un riche ensemble d'algorithmes candidats donne une EQM inférieure pour l'effet du traitement qu'un modèle de régression logistique ou d'autres approches d'apprentissage statistique. Nous appliquons les résultats des études de simulation à l'ajustement du MCSM aux données de la cohorte sur la sclérose en plaques.
}}
%% Talk ba-al
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 10:35]\\}
{\Author{Alexander}{Levis}{levis}{2B-C1}\Author{Daphna}{Harel}{harel}{2B-C1}\Author{Linda}{Kwakkenbos}{kwakkenbos}{2B-C1}\Author{Brett}{Thombs}{thombs}{2B-C1}%
\abshead{\absauthor{ALEXANDER LEVIS}\absaffil{McGill University}, \absauthor{DAPHNA HAREL}\absaffil{New York University}, \absauthor{LINDA KWAKKENBOS}\absaffil{Radboud University}, \absauthor{BRETT THOMBS}\absaffil{McGill University}}
        {Using Optimal Test Assembly Methods for Shortening Patient-Reported Outcome Measures: Development and Validation of the Cochin Hand Function Scale-6\newline
        Utilisation de méthodes optimales d'ensembles de tests pour réduire le nombre de mesures de résultats rapportées par les patients : mise au point et validation de l'échelle 6 de l'indice fonctionnel de la main de Cochin}

\absSideBySide{Patient-completed questionnaires measure important aspects of health, but may be burdensome to complete. Shortening these measures while maintaining measurement equivalence could reduce burden without compromising data quality. Currently, there are no standard methods for creating shortened forms. Optimal test assembly (OTA), used widely in high-stakes educational test design, can automate the selection of a subset of items through mixed-integer programming by generating short forms that maintain maximum Fisher information across the continuum of the latent trait while satisfying user-given constraints. This study explores the applicability of OTA through an example, the Cochin Hand Function Scale, commonly used in rheumatic diseases.
}{Les questionnaires remplis par les patients mesurent des aspects importants de la santé, mais le fait de les remplir peut représenter un fardeau. Réduire le nombre de ces mesures tout en conservant l'équivalence de mesure pourrait réduire ce fardeau sans compromettre la qualité des données. Actuellement, il n'y a pas de méthodes standards permettant de raccourcir les formulaires. L'ensemble de tests optimal est largement utilisé dans la conception de tests pédagogiques importants dans le domaine de l'éducation. Il peut automatiser, par une programmation linéaire mixte, la sélection d'un sous-ensemble de questions en générant de courts formulaires qui conservent le plus d'information de Fisher dans un continuum de trait latent tout en satisfaisant les contraintes données. Cette étude explore l'applicabilité de l'ensemble de tests optimal au moyen de l'exemple de l'échelle 6 de l'indice fonctionnel de la main de Cochin, souvent utilisé dans les maladies rhumatismales.
}}
%% Talk ba-ll
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 10:50]\\}
{\Author{Lin}{Liu}{liu}{2B-C1}\Author{Baron}{John}{john}{2B-C1}\Author{Lieberman}{David}{david}{2B-C1}\Author{Jacobs}{Elizabeth}{elizabeth}{2B-C1}\Author{Cross}{Amanda}{amanda}{2B-C1}\Author{Murphy}{Gwen}{gwen}{2B-C1}\Author{Samir}{Gupta}{gupta}{2B-C1}%
\abshead{\absauthor{LIN LIU}\absaffil{University of California, San Diego}, \absauthor{BARON JOHN}\absaffil{University of North Carolina}, \absauthor{LIEBERMAN DAVID}\absaffil{Veteran Affairs Medical Center and Oregon Health and Science University}, \absauthor{JACOBS ELIZABETH}\absaffil{University of Arizona}, \absauthor{CROSS AMANDA} \& \absauthor{MURPHY GWEN}\absaffil{National Cancer Institute, NIH}, \absauthor{SAMIR GUPTA}\absaffil{University of California, San Diego}}
        {Statistical Issues in a Prognostic Model for Advanced Colorectal Neoplasia Recurrence\newline
        Problèmes statistiques dans un modèle de pronostique pour la récurrence de la néoplasie colorectale avancée}

\absSideBySide{We discuss statistical issues in a study examining whether model-based risk stratification could improve risk prediction over current US surveillance colonoscopy guidelines. Statistical challenges existed in model development, cut-point determination and validation. In a training set, bootstrap LASSO and Bayesian model averaging were used for model selection, cut-points were determined to achieve a priori gain in either sensitivity or specificity. In a validation set, sensitivity, specificity, over- and under-use rate of colonoscopy were estimated. Improvement in clinical benefit was assessed by bootstrapped confidence intervals and net reclassification improvement. This approach successfully identified an improved prognostic model and its potential benefit.
}{Nous abordons les problèmes statistiques dans une étude qui examine si la stratification du risque en fonction d'un modèle pourrait améliorer la prédiction du risque par rapport aux directives de surveillance actuelles des colonoscopies aux États-Unis. Des défis statistiques existaient dans l'élaboration du modèle, dans la détermination du seuil de risque et dans la validation. Dans un jeu de données d'apprentissage, nous avons utilisé les procédures bootstrap, LASSO et de moyenne de modèles bayésien pour la sélection du modèle et nous avons déterminé les seuils de risque  pour gagner a priori soit de la sensibilité ou de la spécificité. Dans un jeu de données de validation, nous avons estimé la sensibilité, la spécificité ainsi que le taux de sur-utilisation et de sous-utilisation de la colonoscopie. Nous avons évalué l'amélioration de l'avantage clinique au moyen d'intervalles de confiance de Bootstrap et de l'amélioration nette de la reclassification. Cette approche a permis de définir avec succès un modèle de pronostique amélioré et d'évaluer son avantage potentiel.
}}
%% Talk ba-ap
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 11:05]\\}
{\Author{Alexandre}{Piché}{piche}{2B-C1}\Author{Russell}{Steele}{steele}{2B-C1}\Author{Ian}{Shrier}{shrier}{2B-C1}\Author{Niels}{Wedderkopp}{wedderkopp}{2B-C1}%
\abshead{\absauthor{ALEXANDRE PICHÉ} \& \absauthor{RUSSELL STEELE}\absaffil{McGill University}, \absauthor{IAN SHRIER}\absaffil{Center for Clinical Epidemiology, Lady Davis Institute, Jewish General Hospital}, \absauthor{NIELS WEDDERKOPP}\absaffil{Institute of Regional Health Research, University of Southern Denmark}}
        {Nested Dirichlet Process: an Application to Censored Data\newline
        Processus emboîté de Dirichlet : application à des données censurées}

\absSideBySide{The CHAMPS-study DK project has been collecting data on the physical well being and injuries of children in Denmark. Estimating the effect of a new sport school program on injury rate is difficult due to between-grade heterogeneity, the injury recurrence, and censoring due to vacation. We used a Bayesian non-parametric method that requires weak assumptions on the distribution of the underlying data. Specifically, we adapted the Nested Dirichlet Process model to handle right-censored data in order to cluster classes, and children within class. It also efficiently estimates the time to injury distribution by sharing statistical strength across classes.
}{Le projet d'étude du Danemark sur la santé, l'activité et la performance motrice des enfants a permis de recueillir des données sur le bien-être et les blessures physiques d'enfants au Danemark. Il est difficile d'estimer l'effet d'un nouveau programme de sport à l'école sur le taux de blessures à cause de l'hétérogénéité entre les classes, de la récurrence des blessures et de la censure en raison des vacances. Nous avons utilisé une méthode bayésienne non paramétrique qui nécessite des hypothèses faibles sur la distribution des données sous-jacentes. Notamment, nous avons adapté le modèle du processus emboîté de Dirichlet pour gérer les données censurées à droite afin de regrouper les classes et les enfants dans une classe. Ce modèle permet également d'estimer efficacement la distribution du temps écoulé jusqu'à la blessure en répartissant la puissance statistique au travers des classes.
}}
%% Talk ba-rr
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 11:20]\\}
{\Author{Razvan}{Romanescu}{romanescu}{2B-C1}\Author{Rob}{Deardon}{deardon}{2B-C1}%
\abshead{\absauthor{RAZVAN ROMANESCU}\absaffil{University of Guelph}, \absauthor{ROB DEARDON}\absaffil{University of Calgary}}
        {Optimal Surveillance of Epidemics over Power Law Networks\newline
        Surveillance optimale des épidémies sur les réseaux de la loi de puissance}

\absSideBySide{Properties of statistical alarms have been well studied for simple disease surveillance models, such as normally distributed incidence rates, with a sudden or gradual shift in mean at the start of an outbreak. We investigate the behavior of various alarms on simulated epidemics over non-homogeneous networks built by the Configuration Model, with a power law degree distribution. We develop a new regression–based alarm and show that it has the best performance over this network type, for diseases with relatively short infectious periods. Finally, we illustrate its use on observed rates of influenza-like illness from the French Sentinelles network.
}{Les propriétés des alarmes statistiques ont bien été étudiées pour des modèles simples de surveillance de maladies, tels que les taux d'incidence répartis normalement, avec un changement soudain ou graduel dans la moyenne au début d'une épidémie. Nous étudions le comportement des diverses alarmes sur des épidémies simulées sur des réseaux non homogènes conçus par le modèle de configuration, au moyen d'une loi de probabilité selon le degré de puissance. Nous mettons au point une nouvelle alarme fondée sur la régression et démontrons qu'elle est la plus efficace sur ce type de réseau pour des maladies dont les périodes d'infection sont relativement courtes. Enfin, nous illustrons son utilisation sur les taux observés de maladies telles que la grippe à partir du réseau français Sentinelles.
}}
%% Talk ba-ks
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 11:35]\\}
{\Author{Krishna}{Saha}{saha}{2B-C1}%
\abshead{\absauthor{KRISHNA SAHA}\absaffil{Central CT State University}}
        {Estimating the Common Intraclass Correlation Coefficient for Clustered Binary Responses\newline
        Estimation du coefficient de corrélation intra-classe commun pour des réponses binaires groupées}

\absSideBySide{In the analysis of several treatment groups for binary responses arising from cluster randomized designs, it is often of interest in measuring the precision of the treatment effects in clinical trials. This inference problem can be addressed based on the confidence interval for a common intraclass correlation, and in many epidemiological applications it is preferable by practitioners. This article focuses on constructing the interval procedures for a common intraclass correlation of several treatment groups. We compare our proposed approaches with a number of large sample procedures through simulations, and illustrate the methodology with an example from biomedical fields.
}{Dans l'analyse de plusieurs groupes de traitement et de réponses binaires provenant de plans d'échantillonnage en grappes aléatoires, il est souvent pertinent de mesurer la précision des effets de traitement dans des essais cliniques. Ce problème d'inférence peut être résolu à partir de l'intervalle de confiance de la corrélation intra-classe commune, que les praticiens préfèrent utiliser dans de nombreuses applications épidémiologiques. Cet exposé met l'accent la construction de tels intervalles pour une corrélation intra-classe commune dans le cas de plusieurs groupes de traitement. A l'aide de simulations, nous comparons les approches que nous proposons avec certaines procédures développées pour de grands échantillons, et nous illustrons la méthodologie avec un exemple provenant du domaine biomédical.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 2B-C2: Methodological Advances: A Sm\"org\aa sbord\\Progr\`es m\'ethodologiques : un buffet abondant}
\begin{center}{\large Chair/Président: Yves Berger (University of Southampton)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 256\end{center}
\label{abs-sid:sma6}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk sma6-hc
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 10:20]\\}
{\Author{Harlan}{Campbell}{campbell}{2B-C2}%
\abshead{\absauthor{HARLAN CAMPBELL}\absaffil{University of British Columbia}}
        {A Non-parametric Rank Based Test for Semicontinuous Longitudinal Data\newline
        Un test non paramétrique basé sur les rangs pour les données longitudinales semi-continues}

\absSideBySide{Semicontinuous  longitudinal data can prove challenging. Even under the
simplest scenario, the `two-part model` (Olsen \& Schafer, 2001), which
uses correlated random-effects to account for both the semicontinuous
and longitudinal characteristics of the data, requires prohibitively
intensive numerical integration to estimate numerous parameters, these
often difficult to interpret.  Alternatively, reducing the data to binary
and fitting a logistic mixed or GEE model, while simple and
straightforward, leads to a substantial loss in power.  A non-parametric
rank-based approach, similar to that proposed by Lin, Li \& Tan (2013),
is introduced.  Simulation studies show relatively high power across a
range of scenarios.
}{Les données longitudinales semi-continues peuvent poser un défi. Même avec le scénario le plus simple, le « modèle à deux parties » (Olsen \& Schafer, 2001), qui utilise des effets aléatoires corrélés pour tenir compte à la fois des caractéristiques semi-continues et longitudinales des données, nécessite une intégration numérique intensive prohibitive pour estimer de nombreux paramètres, ceux-ci étant souvent difficiles à interpréter. Par ailleurs, réduire les données à un caractère binaire et ajuster un modèle logistique mixte ou GEE, quoique simple et direct, conduit à une perte substantielle de puissance. Une approche non paramétrique basée sur le rang, similaire à celle proposée par Lin, Li \& Tan (2013) est introduite. Des études de simulation montrent une puissance relativement élevée à travers une gamme de scénarios.
}}
%% Talk sma6-ja
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 10:35]\\}
{\Author{Julan}{Al-Yassin}{alyassin}{2B-C2}\Author{Abdulkadir}{Hussein}{hussein}{2B-C2}\Author{Tinglei}{Chen}{chen}{2B-C2}%
\abshead{\absauthor{JULAN AL-YASSIN}, \absauthor{ABDULKADIR HUSSEIN} \& \absauthor{TINGLEI CHEN}\absaffil{University of Windsor}}
        {The GLARMA Model for Zero-Inflated  Poisson Counts\newline
        Le modèle GLARMA pour les dénombrements Poisson à surreprésentation de zéros}

\absSideBySide{We investigate the generalised linear autoregressive moving average (GLARMA) model with observations assumed to follow a Poisson distribution. Since the GLARMA model 
may not be appropriate for over-dispersed count data. We propose a new approach to the GLARMA model with a connection of the Zero-Inflated Poisson function. We 
develop likelihood estimation procedures, which allow the models to be estimated.
}{Nous étudions le modèle linéaire généralisé autorégressif à moyenne mobile (GLARMA) avec des observations qui sont présumées suivre une loi de Poisson. Puisque le 
modèle GLARMA peut ne pas être approprié pour des données de dénombrement sur-dispersées, nous proposons une nouvelle approche au modèle GLARMA avec une connexion de 
la fonction de Poisson à surreprésentation de zéros. Nous développons des procédures pour l'estimation de vraisemblance ce qui permet d'estimer les modèles.
}}
%% Talk sma6-fh
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 10:50]\\}
{\Author{Fang}{He}{he}{2B-C2}%
\abshead{\absauthor{FANG HE}\absaffil{University of Western Ontario}}
        {Carbon Dioxide Concentration Estimation\newline
        Estimation de la concentration de dioxyde de carbone}

\absSideBySide{Carbon dioxide (CO2) is one of the necessary elements for plants to process photosynthesis. Changes in the CO2 concentration have an effect on crop yields. The ultimate goal of this analysis is to predict CO2 concentration based on the known information, such as previous years of flux tower data and corresponding MODIS data. In this way, sound decisions can be made concerning yields and the protection of environment in a more efficient way.
}{Le dioxyde de carbone (CO2) est l'un des élements nécessaires aux plantes pour traiter la photosynthèse. Les variations de la concentration de CO2 ont un impact sur les rendements des cultures. Le but ultime de cette analyse est de prédire la concentration de CO2 sur la base d'information connue, comme les données de tours de flux des années précédentes et les données MODIS correspondantes. De cette façon, des décisions judicieuses peuvent être prises d'une manière plus efficace au sujet des rendements et de la protection de l'environnement.
}}
%% Talk sma6-yw
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 11:05]\\}
{\Author{Yuhong}{Wei}{wei}{2B-C2}\Author{Paul D.}{McNicholas}{mcnicholas}{2B-C2}%
\abshead{\absauthor{YUHONG WEI} \& \absauthor{PAUL D. MCNICHOLAS}\absaffil{McMaster University}}
        {Robust Mixture Modelling Based on Generalized Hyperbolic Distribution and its Limiting Case Multivariate Skew-t Distribution with Incomplete Data\newline
        Modélisation robuste par mélange pour données incomplètes fondée sur une distribution hyperbolique généralisée et son cas limite en distribution t-asymétrique multivariée}

\absSideBySide{Clustering from incomplete data is a relevant topic because the real world datasets are fatter-tailed and asymmetric, and even worse, with arbitrary patterns of missing observations. Herein, we present flexible methods for mixture of the generalized hyperbolic and it's limiting case multivariate skew-t models for learning from such datasets. Under MAR mechanisms, we formulate an analytically feasible EM algorithm for carrying out parameter estimation and imputation of missing values under mixture models and missing at random mechanisms. The proposed methodologies are investigated through a simulation study with varying proportions of synthetic missing values and illustrated through a real data set.
}{La mise en grappes à partir de données incomplètes est un sujet pertinent puisque les ensembles de données réelles sont à aile plus large et asymétrique, et pire encore, avec des patrons arbitraires d'observations manquantes. Nous présentons ici des méthodes flexibles de mélange de la distribution hyperbolique généralisée et son cas limite en distribution t-asymétrique multivariée pour l'apprentissage à partir de tels ensembles de données. Sous le mécanisme de données manquantes MAR, nous formulons un algorithme EM réalisable analytiquement pour effectuer l'estimation des paramètres et l'imputation des valeurs manquantes en vertu de modèles de mélange et de mécanismes manquant au hasard. Les méthodologies proposées sont examinées grâce à une étude de simulation avec des proportions variables de valeurs manquantes synthétiques et illustrées à l'aide d'un ensemble de données réelles.
}}
%% Talk sma6-yt
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 11:20]\\}
{\Author{Yoshihiro}{Taniguchi}{taniguchi}{2B-C2}\Author{Christiane}{Lemieux}{lemieux}{2B-C2}%
\abshead{\absauthor{YOSHIHIRO TANIGUCHI} \& \absauthor{CHRISTIANE LEMIEUX}\absaffil{University of Waterloo}}
        {Importance Sampling Techniques for Archimedean Copula Models\newline
        Techniques d'échantillonnage d'importance pour les modèles de copules archimédiennes}

\absSideBySide{In risk management, the joint losses of assets in a portfolio are often modelled through a copula. An important problem in risk management is the accurate estimation of the extreme tail loss-distribution of a portfolio. As such extreme loss occurs with a very low probability, a plain Monte Carlo estimator is bound to be imprecise. Importance sampling is a popular variance reduction technique in this context. In this talk, we present an efficient importance sampling procedure for Archimedean copula models. Our sampling techniques exploit the Marshal-Olkin representation of Archimedean copulas.
}{En gestion du risque, les pertes conjointes de l'actif dans un portefeuille sont souvent modélisées à partir d'une copule. Un problème important en gestion du risque est l'estimation exacte de la distribution des pertes avec ailes extrêmes dans un portefeuille. Comme la probabilité de pertes extrêmes   est très faible, une estimation simple selon la méthode de Monte-Carlo ne pourrait être qu'imprécise. Dans ce contexte, l'échantillonnage d'importance est une technique de réduction de la variance populaire. Dans cet exposé, nous présentons une procédure efficace d'échantillonnage d'importance pour les modèles de copules archimédiennes. Nos techniques d'échantillonnage font appel à la représentation Marshall-Olkin des copules archimédiennes.
}}
%% Talk sma6-pj
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 11:35]\\}
{\Author{Patricia}{Judd}{judd}{2B-C2}\Author{Yildliz}{Yilmaz}{yilmaz}{2B-C2}%
\abshead{\absauthor{PATRICIA JUDD} \& \absauthor{YILDLIZ YILMAZ}\absaffil{Memorial University of Newfoundland}}
        {Two-Phase Response-Dependant Sampling Designs for Time-to-Event Analysis\newline
        Plans d'échantillonnage à deux phases dépendant de la réponse dans l'analyse de temps d'événement}

\absSideBySide{Budgetary constraints may prevent measuring expensive covariates in the first phase for all individuals in a cohort. Expensive covariates are only measured for the selected individuals in the second phase. Case-cohort sampling is a cost-efficient design for the second phase to analyze time-to-event data. In a case-cohort design a random sample from the cases and the cohort is selected. We explore variations of case-cohort design which give more efficient association estimates for a given sample size. We stratify cases based on the observed time-to-event values and apply basic stratified sampling. Different strata sampling proportions change the efficiency of association estimates.
}{Des contraintes budgétaires peuvent empêcher la mesure de covariables coûteuses dans la première phase pour tous les individus d'une cohorte. Les covariables coûteuses ne sont mesurées que pour les individus sélectionnés dans la seconde phase. Lors de la deuxième phase, l'échantillonnage des cas-témoins est un modèle rentable pour analyser les données de temps d'événement. Dans un modèle de cas-témoins, un échantillon aléatoire est sélectionné à partir des cas et de la cohorte. Nous explorons les variations du modèle cas-témoins qui donnent des estimations d'association plus efficaces pour une taille d'échantillon donnée. Nous stratifions les cas selon les valeurs observées de temps d'événement et appliquons un échantillonnage stratifié de base. Différentes
proportions pour l'échantillonnage à l'intérieur des strates changent l'efficacité des estimations d'association.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 2B-C3: Mixture models\\Mod\`eles de m\'elange}
\begin{center}{\large Chair/Président: Hugh Chipman (Acadia University)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 259\end{center}
\label{abs-sid:mm}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk mm-mb
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 10:20]\\}
{\Author{Martin}{Blostein}{blostein}{2B-C3}\Author{Antonio}{Punzo}{punzo}{2B-C3}\Author{Paul D.}{McNicholas}{mcnicholas}{2B-C3}%
\abshead{\absauthor{MARTIN BLOSTEIN}\absaffil{McMaster University}, \absauthor{ANTONIO PUNZO}\absaffil{University of Catania}, \absauthor{PAUL D. MCNICHOLAS}\absaffil{McMaster University}}
        {Robust High-Dimensional Modeling with the Contaminated Gaussian Distribution\newline
        Modélisation robuste de grande dimension avec une distribution gaussienne contaminée}

\absSideBySide{The contaminated Gaussian is a robust elliptic distribution that allows for automatic detection of ``bad points'', i.e. outliers and noise. The contaminated Gaussian factor analysis model is proposed as an extension of the usual latent Gaussian factor analysis model. In turn, a mixture of these contaminated Gaussian factor analyzers is introduced, allowing robust data-reduction and detection of bad points even with high-dimensional data. The number of free parameters is controlled by specifying several parsimonious models with different constraints on covariance structure. For each model, a variant of the EM algorithm is implemented for parameter estimation.
}{La gaussienne contaminée est une distribution elliptique robuste qui permet la détection automatique de « mauvais points » à savoir les valeurs aberrantes et le bruit. Le modèle d'analyse factorielle de la gaussienne contaminée est proposé comme une extension du modèle habituel d'analyse factorielle gaussien latent. En retour, cela introduit un mélange de ces analyseurs de facteur gaussien, permettant une réduction robuste des données et une détection des mauvais points, même avec des données de grande dimension. Le nombre de paramètres libres est contrôlé en spécifiant plusieurs modèles parcimonieux avec différentes contraintes sur la structure de covariance. Pour chaque modèle, une variante de l'algorithme EM est mise en œuvre pour l'estimation des paramètres.
}}
%% Talk mm-mpbg
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 10:50]\\}
{\Author{Michael Patrick Brian}{Gallaugher}{gallaugher}{2B-C3}\Author{Paul D.}{McNicholas}{mcnicholas}{2B-C3}%
\abshead{\absauthor{MICHAEL PATRICK BRIAN GALLAUGHER} \& \absauthor{PAUL D. MCNICHOLAS}\absaffil{McMaster University}}
        {Extending Fractionally Supervised Classification to Non-Gaussian Mixture Models\newline
        Étendre la classification fractionnaire supervisée aux modèles de mélanges non-gaussiens}

\absSideBySide{Fractionally-supervised classification has recently been considered in the context of the Gaussian mixture model. The approach allows for differing degrees of supervision by increasing or decreasing the respective influence of labelled and unlabelled observations when building a classifier. Using both real and simulated data, the performance of fractionally-supervised classification is considered in cases where the mixture component densities are not Gaussian.
}{La classification fractionnaire supervisée a récemment été prise en compte dans le cadre du modèle de mélanges gaussiens. L'approche permet divers degrés de supervision en augmentant ou en diminuant l'influence respective des observations étiquetées et non étiquetées lors de la construction d'un classificateur. À l'aide de données simulées et réelles, la performance de la classification fractionnaire supervisée est considérée dans les cas où les densités des composantes du mélange ne sont pas gaussiennes.
}}
%% Talk mm-yt
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 11:05]\\}
{\Author{Yang}{Tang}{tang}{2B-C3}\Author{Antonio}{Punzo}{punzo}{2B-C3}\Author{Paul D.}{McNicholas}{mcnicholas}{2B-C3}%
\abshead{\absauthor{YANG TANG}\absaffil{McMaster University}, \absauthor{ANTONIO PUNZO}\absaffil{University of Catania}, \absauthor{PAUL D. MCNICHOLAS}\absaffil{McMaster University}}
        {Model-Based Clustering of Categorical Data with Extreme Patterns\newline
        Mise en grappes fondée sur un modèle de données catégoriques avec motifs extrêmes}

\absSideBySide{We propose a mixture of latent trait models with the contaminated Gaussian distribution for the clustering of binary data. A mixture of contaminated Gaussian distributions is implemented to capture the outliers in the latent space in order to enhance the clustering performance. A variational approximation to the likelihood is exploited to derive a fast algorithm for determining the model parameters. Real and simulated data are used to demonstrate this approach.
}{Nous proposons un mélange de modèles de traits latents avec une distribution normale contaminée pour la mise en grappes de données binaires. Un mélange de distributions normales contaminées sont mises à l'œuvre pour capturer les valeurs aberrantes dans l'espace latent de façon à rehausser la performance du groupement. Une approximation variationnelle de la vraisemblance est utilisée pour en tirer un algorithme rapide pour déterminer les paramètres du modèle. Des données réelles et simulées servent à décrire cette approche.
}}
%% Talk mm-ud
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 11:35]\\}
{\Author{Utkarsh J.}{Dang}{dang}{2B-C3}\Author{Ryan}{Browne}{browne}{2B-C3}\Author{Paul D.}{McNicholas}{mcnicholas}{2B-C3}%
\abshead{\absauthor{UTKARSH J. DANG}\absaffil{McMaster University}, \absauthor{RYAN BROWNE}\absaffil{University of Waterloo}, \absauthor{PAUL D. MCNICHOLAS}\absaffil{McMaster University}}
        {Power Exponential Mixtures and Skewed Extensions\newline
        Mélanges de puissance exponentielle et extensions biaisées}

\absSideBySide{A family of parsimonious mixtures of multivariate power exponential distributions is presented. The multivariate power exponential distribution is a flexible elliptical alternative to the Gaussian and Student t-distributions, allowing for dealing with both varying tail-weight (light or heavy) and peakedness of data. For particular values of the shape parameter, special and limiting cases of this distribution include the double-exponential, Gaussian, and the uniform distributions. Furthermore, an extension of these models is presented that can also model asymmetric data. Computational and inference challenges will be discussed. Lastly, the utility of the proposed models is illustrated using both toy and benchmark data.
}{Une famille de mélanges parcimonieux de distributions multivariées de puissance exponentielles est présentée. La distribution multivariée de puissance exponentielle est une variante elliptique flexible des distributions gaussiennes et t de Student, ce qui permet de traiter à la fois des poids de queues variables (légers ou lourds) et l'aplatissement des données. Pour les valeurs particulières du paramètre de forme, des cas particuliers et limitants de cette distribution comprennent les distributions double-exponentielle, gaussienne et uniforme. En outre, une extension de ces modèles qui peut également modéliser les données asymétriques est présentée. Les défis de calcul et d'inférence seront discutés. Enfin, l'utilité des modèles proposés est illustrée à l'aide à la fois de données accessoires et de référence.
}}
%% Talk mm-as
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 11:50]\\}
{\Author{Anjali}{Silva}{silva}{2B-C3}\Author{Steven}{Rothstein}{rothstein}{2B-C3}\Author{Sanjeena}{Dang}{dang}{2B-C3}%
\abshead{\absauthor{ANJALI SILVA}, \absauthor{STEVEN ROTHSTEIN} \& \absauthor{SANJEENA DANG}\absaffil{University of Guelph}}
        {Mixture Model Selection for Cluster Analysis of RNA Sequencing Data\newline
        Sélection de modèles de mélange pour l'analyse du regroupement des données de séquençage ARN}

\absSideBySide{Model-based clustering utilizes mixture models for clustering. It is a form of unsupervised learning where the group memberships of the observations are unknown. Hence, mixture models, commonly used for clustering, are fitted for a range of possible components and model selection is applied to determine the optimal number of components. Typically, each component corresponds to a cluster. Here we perform clustering of real and simulated RNA sequencing data, which is characterized as discrete, skewed, and high dimensional. Model selection through information criteria (BIC, ICL, AIC, AIC3) and slope heuristics (Djump and DDSE) are explored and compared.
}{Les techniques de regroupement à l'aide de modèles utilisent des modèles de mélange. C'est une forme d'apprentissage non supervisé, où l'appartenance à un groupe d'observations est inconnue. Ainsi, les modèles de mélange, généralement utilisés pour le regroupement, sont ajustés pour une série de composantes du modèle possible, et une procédure de sélection de modèles est appliquée afin de déterminer le nombre optimal de composantes. En général, chaque composante correspond à un groupe. Dans cette étude, nous effectuons une analyse de regroupement pour des données de séquençage ARN réelles et simulées, caractérisées comme discrètes, asymétriques et de grandes dimensions. Nous explorons et comparons les différentes modèles sélectionnés à l'aide des critères d'information (BIC, ICL, AIC, AIC3) et de l'heuristique de la pente (Djump et DDSE).
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 2B-C4: Multivariate Data\\Donn\'ees multivari\'ees}
\begin{center}{\large Chair/Président: Lisa Lix (University of Manitoba)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 257\end{center}
\label{abs-sid:md}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk md-bf
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 10:20]\\}
{\Author{Brian}{Franczak}{franczak}{2B-C4}\Author{Paul D.}{McNicholas}{mcnicholas}{2B-C4}%
\abshead{\absauthor{BRIAN FRANCZAK} \& \absauthor{PAUL D. MCNICHOLAS}\absaffil{McMaster University}}
        {Discriminant Analysis via Parsimonious Multiple Scaled Mixtures\newline
        Analyse discriminante à l'aide de mélanges parcimonieux à multiples échelles}

\absSideBySide{Mixtures of multiple scaled distributions have garnered increased attention in the last few years. One issue with these mixtures is that the covariance structures become highly parameterized as the dimension of the data increases. Because these multiple scaled distributions are formulated using an eigen-decomposed scale matrix, we can introduce parsimony by constraining the constituent elements of this decomposition. We introduce a family of parsimonious multiple scaled mixtures where the component densities are a generalization of the multivariate-t density function. We utilize these models for discriminant analysis using real data sets, and compare their results to the state-of-the-art Gaussian alternative.
}{Les mélanges de distributions à multiples échelles ont suscité une attention accrue au cours des dernières années. Un problème avec ces mélanges est que les structures de covariance deviennent très paramétrées lorsque la dimension des données augmente. Du fait que ces distributions à multiples échelles sont formulées à partir d'une matrice d'échelle décomposée en vecteurs propres, nous pouvons introduire de la parcimonie en limitant les éléments constitutifs de cette décomposition. Nous introduisons une famille de mélanges parcimonieux à multiples échelles où les densités des composantes sont une généralisation de la fonction de densité t multivariée. Nous utilisons ces modèles pour l'analyse discriminante en utilisant de vrais jeux de données et en comparant leurs résultats à l'alternative gaussienne.
}}
%% Talk md-lh
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 10:35]\\}
{\Author{Haocheng}{Li}{li}{2B-C4}\Author{Yukun}{Zhang}{zhang}{2B-C4}\Author{Sarah}{Kozey-Keadle}{kozeykeadle}{2B-C4}\Author{Joshua}{Sampson}{sampson}{2B-C4}\Author{Charles}{Matthews}{matthews}{2B-C4}\Author{Raymond}{Carroll}{carroll}{2B-C4}%
\abshead{\absauthor{HAOCHENG LI} \& \absauthor{YUKUN ZHANG}\absaffil{University of Calgary}, \absauthor{SARAH KOZEY-KEADLE}, \absauthor{JOSHUA SAMPSON} \& \absauthor{CHARLES MATTHEWS}\absaffil{National Cancer Institute}, \absauthor{RAYMOND CARROLL}\absaffil{Texas A\&M University}}
        {Multivariate Longitudinal Data with Mixed Types of Measurements\newline
        Données longitudinales multivariées avec types de mesures mixtes}

\absSideBySide{We take a random effect modeling to longitudinal data with multiple measurements in different types. The outcomes can have continuous, binary, count and proportional variables measured at various time-points with random effects that have a hierarchical correlation structure. Via a quasilikelihood type approximation for the non-Gaussian components, we transform all types of measurements into pseudo normal variables. The transformed responses are fitted by an ECME algorithm, which is efficient for high dimensional random effects. The method is applied to physical activity data, and is evaluated empirically by a simulation study.
}{Nous prenons une modélisation de l'effet aléatoire aux données longitudinales avec plusieurs mesures de différents types. Les résultats peuvent être des variables continues, binaires, de compte et proportionnelles mesurées à différents points dans le temps avec des effets aléatoires ayant une structure de corrélation hiérarchique. À l'aide d'une approximation de type quasi-vraisemblance pour les composantes non gaussiennes, nous transformons tous les types de mesures en variables pseudo-normales. Les réponses transformées sont ajustées par un algorithme de ECME qui est efficace pour des effets aléatoires de grande dimension. La méthode est appliquée aux données d'activité physique et est évaluée empiriquement par une étude de simulation.
}}
%% Talk md-yel
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 10:50]\\}
{\Author{Yubin (Éric)}{Li}{li}{2B-C4}\Author{Sévérien}{Nkurunziza}{nkurunziza}{2B-C4}%
\abshead{\absauthor{YUBIN (ÉRIC) LI}\absaffil{University of Windsor}, \absauthor{SÉVÉRIEN NKURUNZIZA}\absaffil{University of Windsor/Université de Sherbrooke}}
        {Estimation in Multivariate Regression with Measurement Error and Change-Points\newline
        Estimation en régression multivariée avec erreur de mesure et points de rupture}

\absSideBySide{In this talk, we present an estimation problem in multivariate regression model with measurement error and unknown change-points. In particular, we consider the case where the target parameter is the matrix of the regression coefficients which is suspected to satisfy some restrictions. Under such an uncertainty, we propose shrinkage type estimators and study their asymptotic properties.  The derived asymptotic properties generalize some recent findings in literature. We also prove that the proposed shrinkage estimators dominate the unrestricted estimator.
}{Dans cette présentation, il est question du problème d'estimation dans le modèle de régression multivarié avec erreur de mesure et points de rupture inconnus. En particulier, nous considérons le cas où le paramètre d'intérêt est la matrice des coefficients de régression qui est susceptible d'être sujette à certaines restrictions. Face à cette incertitude, nous proposons les estimateurs à rétrécissement et étudions leurs propriétés asymptotiques. Les propriétés asymptotiques établies généralisent certains résultats de la littérature récente. Nous prouvons également que les estimateurs à rétrécissement établis dominent l'estimateur sans restriction.
}}
%% Talk md-ms
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 11:05]\\}
{\Author{Makosso-Kallyth}{Sun}{sun}{2B-C4}%
\abshead{\absauthor{MAKOSSO-KALLYTH SUN}\absaffil{McMaster University}}
        {Dimension Reduction of Histogram Data\newline
        Réduction de la dimension de variables de type histogramme}

\absSideBySide{We present the application of two extensions of principal component analysis to histogram data. Data where each entity and variable are histogram or empirical distribution are called histogram or distributional data. We present two approaches respectively based on the first order moments and the quantiles. We compare and we show the benefits of these methods using real data examples.
}{Nous présentons l'application de deux extensions de l'analyse en composantes principales de variables de type histogramme. Les tableaux de données distributionnelles correspondent aux tableaux de données pour lesquelles chaque individu et chaque variable sont décrits par une distribution empirique ou un histogramme. Nous présentons deux approches basées respectivement sur les moments d'ordre 1 et les quantiles. Nous comparons et montrons l'intérêt de ces méthodes à partir de données réelles.
}}
%% Talk md-iv
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 11:20]\\}
{\Author{Irene}{Vrbik}{vrbik}{2B-C4}%
\abshead{\absauthor{IRENE VRBIK}\absaffil{McGill University}}
        {Cluster Analysis of Genetic Sequence Data via the Gap Procedure\newline
        Analyse de mise en grappes de données de séquence génétique en utilisant le procédé des écarts}

\absSideBySide{Phylogenetic clustering typically involves estimating a phylogenetic tree and identifying groups of sequences having small genetic distances and high clade support.  We explore a simple distance-based clustering algorithm, called the Gap Procedure, which uses gaps in sorted pairwise distances to suggest a natural divide between group members and non-members. We show that the clusters found using the Gap Procedure agree closely with computationally expensive gold standard techniques on well separated groups of HIV sequence data. Simulation studies are presented to illustrate scenarios in which this algorithm may be employed, and more importantly, when more sophisticated methods are required.
}{La mise en grappes phylogénétique fait généralement appel à l'estimation d'un arbre phylogénétique et à l'identification de groupes de séquences de courte distance génétique et support de clade élevé. Nous explorons un algorithme de classification simple fondé sur la distance, appelé procédé des écarts, qui utilise des écarts en distances par paires ordonnées afin de suggérer une division naturelle entre les membres et non-membres d'un groupe. Nous montrons que les groupements obtenus par le procédé des écarts s'harmonisent étroitement avec ceux obtenus des techniques de référence computationnellement très coûteuses sur des groupes bien séparés de données relatives à la séquence du VIH. Des études de simulation sont présentées pour illustrer des scénarios dans lesquels cet algorithme peut être utilisé, et plus important encore, lorsque des méthodes plus complexes sont nécessaires.
}}
%% Talk md-yz
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 11:35]\\}
{\Author{Yang}{Zhao}{zhao}{2B-C4}%
\abshead{\absauthor{YANG ZHAO}\absaffil{University of Regina}}
        {Classification and Factor Analysis for Big Data Inference:  A Study of Gambling Behaviours and the Associate Factors\newline
        Classification et analyse factorielle pour l'inférence de données massives~: une étude des comportements de joueurs et des facteurs associés}

\absSideBySide{In this research we review statistical methods for big data inference.  We focus on the k-means and the generalized linear models for big data classification and factor analysis.  We are interested in studying of gambling behaviours and identifying problem gamblers.   We consider Saskatchewan casinos player club card data base.  We classify player into different risk groups based on multivariate observations and study factors that related to the gambling behaviours.
}{Cette recherche passe en revue les méthodes statistiques pour l'inférence de données massives. Nous nous attardons aux k-moyennes et aux modèles linéaires généralisés pour la classification et l'analyse factorielle des données massives. Nous nous intéressons à l'étude des comportements de joueurs et à l'identification des joueurs pour qui le jeu est problématique. Nous utilisons la base de données pour les jeux de cartes des casinos de la Saskatchewan. Nous classifions les joueurs dans divers groupes à risque en fonction d'observations multivariées et étudions les facteurs en rapport avec les comportements de joueurs.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 2B-C5: Survey Methodology 2\\M\'ethodologie d'enqu\^ete 2}
\begin{center}{\large Chair/Président: Mahmoud Torabi (University of Manitoba)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 258\end{center}
\label{abs-sid:sm2}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk sm2-hjg
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 10:20]\\}
{\Author{Hyukjun (Jay)}{Gweon}{gweon}{2B-C5}\Author{Matthias}{Schonlau}{schonlau}{2B-C5}\Author{Lars}{Kaczmirek}{kaczmirek}{2B-C5}\Author{Michael}{Blohm}{blohm}{2B-C5}%
\abshead{\absauthor{HYUKJUN (JAY) GWEON} \& \absauthor{MATTHIAS SCHONLAU}\absaffil{University of Waterloo}, \absauthor{LARS KACZMIREK} \& \absauthor{MICHAEL BLOHM}\absaffil{GESIS}}
        {New Approaches to Automated Occupation Coding Using Statistical Learning\newline
        Nouvelles approches pour le codage automatisé de la profession à l'aide de l'apprentissage statistique}

\absSideBySide{Occupation coding refers to coding a respondent's text answer into one of hundreds occupation codes. Automated coding is a challenging problem because answers usually consist of only a few words while there are hundreds of possible categories. We develop several approaches that can be useful in the automatic coding context. The approaches include a hybrid method that combines a duplicate-based approach with a statistical learning algorithm, and a scoring method that is applied in a nearest neighbor approach. We illustrate the proposed approaches with occupational coding in the German ALLBUS panel survey based on the ISCO-88 standard.
}{Le codage de la profession se réfère au codage de la réponse d'un répondant à l'une des centaines de codes de profession. Le codage automatisé est un problème difficile car les réponses consistent habituellement en quelques mots seulement alors qu'il y a des centaines de catégories possibles. Nous développons plusieurs approches qui peuvent être utiles dans le contexte du codage automatisé. Les approches incluent une méthode hybride qui combine une approche basée sur les doublons et un algorithme d'apprentissage statistique et une méthode de cotation qui est appliquée à une approche du plus proche voisin. Nous illustrons les approches proposées au codage des professions de l'enquête par panel allemand ALLBUS basée sur la norme CITP-88.
}}
%% Talk sm2-ioi
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 10:35]\\}
{\Author{Ibrahima Ousmane}{Ida}{ida}{2B-C5}\Author{Louis-Paul}{Rivest}{rivest}{2B-C5}%
\abshead{\absauthor{IBRAHIMA OUSMANE IDA} \& \absauthor{LOUIS-PAUL RIVEST}\absaffil{Laval University}}
        {Balanced Sampling by Using the Cube Method and the Rejective Algorithm\newline
        L'échantillonnage équilibré par la méthode du cube et la méthode réjective}

\absSideBySide{In recent years, balanced sampling techniques have experienced a renewed interest. They allow to reproduce the structure of the population in samples in order to improve the efficiency of survey estimates. New procedures have been proposed. These include the cube method, an exact method presented by Deville and Tillé (2004), and an approximate method, the Fuller (2009) rejective algorithm.  After a brief presentation of these methods as part of an angler survey, we compare using Monte Carlo simulations, the survey designs produced by these two sampling algorithms.
}{Au cours de ces dernières années, les techniques d'échantillonnage équilibré ont connu un regain d'intérêt. En effet, elles permettent de reproduire la structure de la population dans les échantillons afin d'améliorer l'efficacité des estimations. Récemment, des nouvelles procédures ont été proposées.  Il s'agit notamment de la méthode du cube,  une méthode exacte présentée par Deville et Tillé (2004), et une méthode approximative, l'algorithme réjectif de Fuller (2009). Après une brève présentation de ces deux méthodes dans le cadre d'un inventaire de pêcheurs, nous comparons à l'aide de simulations Monte-Carlo, les plans de sondage produits par ces deux méthodes d'échantillonnage.
}}
%% Talk sm2-il
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 10:50]\\}
{\Author{Isabelle}{Lefebvre}{lefebvre}{2B-C5}\Author{David}{Haziza}{haziza}{2B-C5}\Author{Jean-François}{Beaumont}{beaumont}{2B-C5}%
\abshead{\absauthor{ISABELLE LEFEBVRE} \& \absauthor{DAVID HAZIZA}\absaffil{Université de Montréal}, \absauthor{JEAN-FRANÇOIS BEAUMONT}\absaffil{Statistique Canada}}
        {Simplified Variance Estimation for Complex Designs\newline
        Estimation simplifiée de la variance pour des plans complexes}

\absSideBySide{In a complex design framework, standard variance estimation methods entail substantial challenges. As we know, conventional variance estimators involve second order inclusion probabilities, which can be difficult to compute for some sampling designs. Based on Ohlsson's sequential Poisson sampling method (1998), we suggest a simplified estimator for which we only need first order inclusion probabilities. The idea is to approximate a survey strategy (which consists of a sampling design and an estimator) by an equivalent strategy for which Poisson sampling is used. We will discuss proportional to size sampling and two-stage sampling. Results of a simulation study will be presented.
}{En présence de plans de sondage complexes, les méthodes classiques d'estimation de la variance présentent certains défis. Celles-ci requièrent les probabilités d'inclusion d'ordre 2 qui peuvent être complexes à obtenir. En s'inspirant d'une approche développée par Ohlsson (1998) dans un contexte d'échantillonnage de Poisson séquentiel, nous proposons un estimateur ne requérant que les probabilités d'inclusion d'ordre 1. L'idée est d'approximer la stratégie de l'enquête (consistant d'un plan de sondage et d'un estimateur) par une stratégie équivalente qui considère le plan de Poisson. Nous discuterons d'échantillonnage proportionnel à la taille et à deux degrés. Une étude par simulation sera présentée.
}}
%% Talk sm2-tv
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 11:05]\\}
{\Author{Thuva}{Vanniyasingam}{vanniyasingam}{2B-C5}\Author{Chuck E}{Cunningham}{cunningham}{2B-C5}\Author{Gary}{Foster}{foster}{2B-C5}\Author{Lehana}{Thabane}{thabane}{2B-C5}%
\abshead{\absauthor{THUVA VANNIYASINGAM}, \absauthor{CHUCK E CUNNINGHAM}, \absauthor{GARY FOSTER} \& \absauthor{LEHANA THABANE}\absaffil{McMaster University}}
        {Determining the Impact of Different Design Features on Relative Design Efficiency in Discrete Choice Experiments\newline
        Déterminer l'effet de différents aspects d'un plan sur l'efficacité relative du plan dans les expériences avec choix discrets}

\absSideBySide{Discrete choice experiments (DCEs) are used to quantify preferences of patients and health care providers. Guidance is needed to avoid creating designs with low statistical efficiency, resulting in biased preference surveys.  We simulated 3204 DCE designs to assess how varying DCE design characteristics including the number of attributes (2-20), attribute-levels (2-5), alternatives (2-5), and choice tasks (2-20) affect relative design efficiency.  Across all designs, more optimal designs were achieved with fewer attributes, fewer attribute levels, and more alternatives per choice task. These results are widely applicable for creating designs to elicit individual preferences on health services, programs, and products.
}{Les expériences avec choix discrets (ECD) sont utilisées pour quantifier les préférences des patients et des fournisseurs de soins de santé. Des lignes directrices sont nécessaires pour éviter de créer des plans avec faible efficacité statistique, aboutissant à des sondages biaisés sur les préférences. Nous avons simulé 3204 plans d'ECD pour évaluer comment leurs diverses caractéristiques, y compris le nombre d'attributs (2-20), le niveau des attributs (2-5), les alternatives (2-5) et les tâches à choix multiples (2-20) influent sur l'efficacité relative d'un plan. Les plans les plus optimaux étaient ceux qui comportaient moins d'attributs et de niveaux d'attributs et plus d'alternatives par tâche à choix multiples. Ces résultats peuvent largement s'appliquer à la création de plans qui permettent d'obtenir les préférences individuelles en matière de services, programmes et produits de santé.
}}
%% Talk sm2-wl
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 11:20]\\}
{\Author{Wei}{Lin}{lin}{2B-C5}\Author{Nancy}{Reid}{reid}{2B-C5}\Author{Karla}{Fox}{fox}{2B-C5}%
\abshead{\absauthor{WEI LIN} \& \absauthor{NANCY REID}\absaffil{University of Toronto}, \absauthor{KARLA FOX}\absaffil{Statistics Canada}}
        {Analysis of an Embedded Experiment in a Survey\newline
        Analyse d'une expérience intégrée dans le cadre d'une enquête}

\absSideBySide{We derive the Horvitz-Thompson estimator of the average treatment effect and its variance for a general design. In the presence of auxiliary information, a new model-assisted estimator for the average treatment effect is developed and the variance of the estimator is derived. We show that the new estimator is approximately design-unbiased when a general model is employed. Moreover, it doesn't require auxiliary variable information at the population level and is relatively easy to implement and compute. Simulations carried out indicate that the new estimator gains in efficiency and its relative bias is negligible.
}{Nous dérivons l'estimateur de Horvitz-Thompson de l'effet moyen du traitement et de sa variance pour un plan d'expérience général. Nous mettons au point un nouvel estimateur assisté par modèle de l'effet moyen du traitement en présence d'information auxiliaire, et nous dérivons la variance de l'estimateur. Nous démontrons que le nouvel estimateur est approximativement sans biais par rapport au plan d'expérience lorsqu'un modèle général est employé. De plus, il ne nécessite pas l'information de variables auxiliaires à l'échelle de la population et est relativement facile à mettre en œuvre et à calculer. Des simulations indiquent que le nouvel estimateur est plus efficace et que son biais relatif est négligeable.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 2C-P: Poster Session\\S\'eance d'affichage}
\par (Posters displayed 12:00-17:30. Presenters in attendance 13:30-15:30)\par (Les affiches seront exposées de 12 h à 17 h 30. Les auteurs seront présents de 13 h 30 à 15 h 30.)\vspace{3pt}

\begin{center}\large\bfseries Room/Salle: Thistle Corridor\end{center}
\label{abs-sid:ps}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk ps-ha
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 12:00]\\}
{\Author{Huda}{Al-Wahsh}{alwahsh}{2C-P}\Author{Abdulkadir}{Hussein}{hussein}{2C-P}\Author{Weiwei}{Liu}{liu}{2C-P}%
\abshead{\absauthor{HUDA AL-WAHSH}, \absauthor{ABDULKADIR HUSSEIN} \& \absauthor{WEIWEI LIU}\absaffil{University of Windsor}}
        {Scan Statistics in Cox's Model with Frailty\newline
        Statistiques de balayage dans le modèle de Cox avec fragilité}

\absSideBySide{We propose a new method to detect clusters for survival data with frailty. It is based on Moran's I test for the Martingale residuals of Cox's model with frailty. Compared to the familiar scan statistic approach, the method maintains a reasonable type I error, has higher power and a reasonable sensitivity in detecting disease clusters under Cox's model with regional frailties.\\
\\
In addition, the new approach is computationally easier to implement as it utilizes the asymptotic distribution Moran's I as opposed to the spatial scan statistics which resort to bootstrap sampling.
}{Nous proposons une nouvelle méthode pour détecter les grappes en matière de données de survie avec fragilité, méthode fondée sur le test I de Moran pour les résidus de martingale du modèle de Cox avec fragilité. Comparativement aux statistiques de balayage connues, cette méthode conserve une marge d'erreur de type I raisonnable, a une plus grande puissance et possède une sensibilité raisonnable pour la détection des grappes de maladies en vertu du modèle de Cox avec fragilités régionales.\\
\\
De plus, cette nouvelle approche est plus facile à mettre en œuvre sur le plan informatique, étant donné qu'elle fait appel à la distribution asymptotique I de Moran contrairement aux statistiques de balayage spatiales qui ont recours à l'échantillonnage bootstrap.
}}
%% Talk ps-ja
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 12:00]\\}
{\Author{Justin}{Angevaare}{angevaare}{2C-P}\Author{Zeny}{Feng}{feng}{2C-P}\Author{Rob}{Deardon}{deardon}{2C-P}%
\abshead{\absauthor{JUSTIN ANGEVAARE} \& \absauthor{ZENY FENG}\absaffil{University of Guelph}, \absauthor{ROB DEARDON}\absaffil{University of Calgary}}
        {Phylodynamic Individual Level Models: Strategies for Simulation and Inference\newline
        Modèles phylodynamiques individuels~: des stratégies de simulation et d'inférence}

\absSideBySide{Phylodynamics is an emergent field that explores the joint dynamics of disease spread and evolution. When epidemic and evolutionary processes occur on similar time scales, phylodynamic models can be used to improve our understanding of disease dynamics. A phylodynamic extension to the individual level models of infectious disease transmission of Deardon et al. (2010) is developed. Computational methods for stochastic simulation and for Bayesian inference are described, and simulation study results are presented. There are opportunities to better inform infectious disease control strategies through the use of these models, when, as is increasingly common, pathogen genetic sequence data are available.
}{La phylodynamique est un champ d'études émergent qui explore la dynamique conjointe de la dissémination et de l'évolution d'une maladie. Lorsque l'épidémie et le processus évolutif de la maladie se produisent sur des échelles de temps similaires, les modèles phylodynamiques peuvent servir pour mieux comprendre la dynamique de la maladie. Nous généralisons la phylodynamique aux modèles individuels de la transmission de maladies infectieuses de Deardon et coll. (2010). Des méthodes computationnelles pour la simulation stochastique et une inférence bayésienne sont décrites et les résultats d'études de simulation sont présentés. L'utilisation de ces modèles offre des possibilités d'établir de meilleures stratégies de contrôle des maladies infectieuses lorsque des données sur le séquençage génétique des agents pathogènes sont disponibles, ce qui est de plus en plus le cas.
}}
%% Talk ps-fa
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 12:00]\\}
{\Author{Faisal}{Atakora}{atakora}{2C-P}\Author{Po}{Yang}{yang}{2C-P}%
\abshead{\absauthor{FAISAL ATAKORA} \& \absauthor{PO YANG}\absaffil{University of Manitoba}}
        {Construction of Optimal Foldover Designs with the General Minimum Lower-Order Confounding\newline
        Élaboration de plans de repliement optimal avec le critère de l'amalgame général minimal d'ordre inférieur}

\absSideBySide{Fractional factorial designs are widely used in industry and agriculture. Over the years much research work  has been done to study these designs. Foldover fractional factorial designs can de-alias important factors (especially those of interest) so that their effects can be studied without ambiguities. 
We consider optimal foldover designs using general minimum lower-order confounding criterion. A catalogue of 16- and 32-run optimal foldover designs is constructed  and tabulated for practical use. A comparison is made between the general minimum lower-order confounding optimal foldover designs and other optimal foldover designs.
}{Les plans factoriels partiels sont largement utilisés dans l'industrie et l'agriculture. Au fil des ans, bon nombre de travaux de recherche y ont été consacrés. Ce type de plans peut éliminer le repliement de facteurs importants (notamment d'un intérêt particulier) pour en étudier les effets sans aucune ambiguïté.  
Nous envisageons les plans de repliement optimal selon le critère de l'amalgame général minimal d'ordre inférieur. À des fins d'usage pratique, un catalogue de plans de repliement optimal de 16 et 32 suites est conçu et tabulé. Une comparaison est établie entre les plans de repliement optimal selon le critère de l'amalgame général minimal d'ordre inférieur à d'autres plans de repliement optimal.
}}
%% Talk ps-ab
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 12:00]\\}
{\Author{Andrea}{Benedetti}{benedetti}{2C-P}\Author{Muhammad}{Mullah}{mullah}{2C-P}\Author{Russell}{Steele}{steele}{2C-P}\Author{James}{Hanley}{hanley}{2C-P}%
\abshead{\absauthor{ANDREA BENEDETTI}, \absauthor{MUHAMMAD MULLAH}, \absauthor{RUSSELL STEELE} \& \absauthor{JAMES HANLEY}\absaffil{McGill University}}
        {Evaluation of Estimation Techniques for Smoothing via Generalized Linear Mixed Models\newline
        Évaluation des techniques d'estimation pour le lissage à l'aide de modèles mixtes linéaires généralisés}

\absSideBySide{Semi-parametric (generalized linear) mixed models (SPMMs) provide a flexible approach to model non-linear associations for binary responses. To achieve a smooth curve, it uses regression splines, and shrinks the coefficients at knot points towards zero. Several approaches are used to estimate the SPMMs including penalized quasi-likelihood (PQL), Laplacian approximation (LA), Gauss-Hermite quadrature (GHQ), and Bayesian Markov Chain Monte Carlo (MCMC).  Through simulations, we evaluate the performance of these techniques for estimating the SPMM where smoothing is the main purpose. We then apply these methods to obtain smooth curves in a real life dataset.
}{Les modèles mixtes semi-paramétriques (linéaires généralisés) (MMSP) offrent une approche souple à la modélisation d'associations non linéaires pour les réponses binaires. Afin d'obtenir une courbe lisse, ces modèles font appel à des splines de régression et rétrécissent les coefficients à des nœuds vers zéro. Plusieurs approches sont utilisées pour estimer les MMSP, y compris la quasi-vraisemblance pénalisée (QVP), l'approximation de Laplace (AL), la quadrature Gauss-Hermite (QGH) et la méthode bayésienne de Monte-Carlo par chaînes de Markov (MCMC). À l'aide de simulations, nous évaluons le rendement de ces techniques pour l'estimation des MMSP lorsque le but premier est le lissage. Nous appliquons ensuite ces méthodes pour obtenir des courbes lisses dans un ensemble de données réelles.
}}
%% Talk ps-ry
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 12:00]\\}
{\Author{Shelley B.}{Bull}{bull}{2C-P}\Author{Jihyung}{Shin}{shin}{2C-P}\Author{Ruiyang}{Yi}{yi}{2C-P}%
\abshead{\absauthor{SHELLEY B. BULL}\absaffil{University of Toronto}, \absauthor{JIHYUNG SHIN} \& \absauthor{RUIYANG YI}\absaffil{Lunenfeld-Tanenbaum Research Institute of Mount Sinai Hospital}}
        {Bootstrap Resampling in Genetic Association Analysis of Low-Frequency Variants\newline
        Rééchantillonnage bootstrap pour une analyse d'association génétique avec variantes peu fréquentes}

\absSideBySide{The winner's curse refers to upward bias in the magnitude of effect estimates obtained in genome-wide association studies. Performance of bootstrap resampling to reduce bias has been demonstrated for common variants (MAF>5\%), but not for low-frequency variants. In standard logistic regression of a binary outcome, effect estimation can fail for low allele counts, particularly in bootstrap samples. The objective of our study is to assess the value of Firth's penalized logistic regression (PML) in bias reduction of effect estimates for variants with MAF=1-5\%. In simulation studies we observed slightly larger PML shrinkage effects for lower compared to higher MAF variants.
}{La malédiction du vainqueur renvoie à un biais positif de la magnitude des estimations d'effet obtenues dans des études d'association pangénomique. La performance du rééchantillonnage bootstrap pour réduire le biais a été démontrée pour des variantes communes (fréquence des allèles mineurs > 5 \%), mais non pour des variantes peu fréquentes. Dans la régression logistique standard d'une réponse binaire, l'estimation d'effet peut échouer lorsque le nombre d'allèles est peu élevé, en particulier avec des échantillons bootstrap. Le but de notre étude est d'évaluer la valeur de la régression logistique pénalisée (RLP) de Firth pour réduire le biais de l'estimation d'effet pour des variantes avec FAM =1-5 \%. Des études de simulation ont permis d'observer des effets de rétrécissement RLP plus grands pour des variantes avec une FAM moins élevée plutôt que plus élevée.
}}
%% Talk ps-bc
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 12:00]\\}
{\Author{Bo}{Chen}{chen}{2C-P}\Author{Andrea}{Benedetti}{benedetti}{2C-P}%
\abshead{\absauthor{BO CHEN} \& \absauthor{ANDREA BENEDETTI}\absaffil{McGill University}}
        {Quantifying Heterogeneity in Individual Participant Data Meta-Analysis with Binary Outcomes\newline
        Quantification de l'hétérogénéité dans la méta-analyse de données relatives au participant avec réponses binaires}

\absSideBySide{We consider how best to quantify heterogeneity in the context of individual participant data meta-analyses of binary data. Both two- and one-stage approaches are evaluated via simulation study. In the two-stage approach, we use the $I^2$ and $R^2$ statistics proposed by Higgins et al. In the one-stage approach, we adapt a simulation based intraclass correlation coefficient (ICC) proposed by Goldstein et al. to estimate the $I^2$. Preliminary results show that in the presence of effect modification, the estimated $I^2$ from the one-stage model has better performance than that from the two-stage model.
}{Nous réfléchissons au meilleur moyen de quantifier l'hétérogénéité dans le contexte d'une méta-analyse de données binaires relatives au participant. Des études de simulation permettent d'évaluer des approches en une et deux étapes. Avec l'approche en deux étapes, nous utilisons les statistiques 
$I^2$ et $R^2$ proposées par Higgins et coll. Avec l'approche en une étape, nous adaptons une simulation fondée sur le coefficient de corrélation intra-classe (CCI) proposé par Goldstein et coll. pour estimer la statistique $I^2$. Des résultats préliminaires montrent qu'en présence d'une modification d'effet, la performance de la statistique $I^2$ estimée dans le modèle en une étape est meilleure que dans le modèle en deux étapes.
}}
%% Talk ps-je
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 12:00]\\}
{\Author{Jonathan}{Earl}{earl}{2C-P}\Author{Paul D.}{McNicholas}{mcnicholas}{2C-P}%
\abshead{\absauthor{JONATHAN EARL} \& \absauthor{PAUL D. MCNICHOLAS}\absaffil{McMaster University}}
        {Ranking and Tanking: GLMs for Football\newline
        Classement et sous-performance délibérée  : une approche MLG au football}

\absSideBySide{The problem of ranking American college football teams using a statistical model is considered. A GLM approach is taken so that, for some team $i$ with schedule $S_i$, the rating of team $i$ is derived using $S_i$ as predictor variables and a modified rule of succession as the response variable. The basic assumptions and benefits of the approach are discussed along with an explanation of how the ranking concepts can be abstracted.
}{Nous abordons le problème du classement des équipes collégiales américaines de football à l'aide d'un modèle statistique. Nous faisons appel à un modèle linéaire généralisé (MLG) de façon à classer une certaine équipe $i$ avec un calendrier $S_i$, en utilisant $S_i$ comme variable prédictive et une règle de succession modifiée comme variable réponse. Nous traitons des postulats de base et avantages de l'approche et expliquons les moyens d'en extraire des concepts de classement.
}}
%% Talk ps-yl
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 12:00]\\}
{\Author{Yuanhao}{Lai}{lai}{2C-P}\Author{McLeod}{A. Ian}{aian}{2C-P}%
\abshead{\absauthor{YUANHAO LAI} \& \absauthor{MCLEOD A. IAN}\absaffil{Western University}}
        {Computing Numerical Distribution Functions for Periodicity Tests in Microarray Time Series\newline
        Calcul numérique de fonctions de répartition pour des tests de périodicité dans des séries chronologiques en biopuce}

\absSideBySide{Identifying the periodicity in microarray time series data has become
increasingly important in microarray technology. We propose an innovative
likelihood ratio test based on fitting a four parameter harmonic regression
and show that it is competitive compared with the other proposed tests
of periodicity. A response surface regression approach is implemented
to compute the P-values of the test statistic for any periodicity
and sample size of the data. In addition, the computation is dramatically
speeded up by utilizing the parallel computation through the Shared
Hierarchical Academic Research Computer Network.
}{Il importe de plus en plus en technologie de biopuce d'identifier la périodicité dans les séries chronologiques en biopuce. Nous proposons un test novateur de rapport de vraisemblance basé sur la modélisation d'une régression harmonique à quatre paramètres et en montrons la compétitivité en le comparant à d'autres tests de périodicité. Une approche régressive de surface de réponse est mise en œuvre pour calculer les valeurs de P de la statistique de test pour toute périodicité et taille d'échantillonnage des données. De plus, le calcul est fortement accéléré en utilisant le calcul parallèle par l'entremise de SHARCNET (Shared Hierarchical Academic Research Computer Network).
}}
%% Talk ps-sm
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 12:00]\\}
{\Author{Sean}{McGrath}{mcgrath}{2C-P}\Author{Xiaofei}{Zhao}{zhao}{2C-P}\Author{Andrea}{Benedetti}{benedetti}{2C-P}%
\abshead{\absauthor{SEAN MCGRATH}\absaffil{McGill University}, \absauthor{XIAOFEI ZHAO}\absaffil{RI MUHC}, \absauthor{ANDREA BENEDETTI}\absaffil{McGill University}}
        {Meta Analysis of Medians\newline
        Méta-analyse des valeurs médianes}

\absSideBySide{Our work focusses on meta-analyzing medians.  Such data typically arises from meta-analyses that evaluate time-based outcomes (e.g. diagnostic delay, length of hospitalization), and is especially challenging given the variety of outcome data reported by the source studies (e.g. medians and percentiles, means and SDs, etc.). Via simulation, we compared various approaches to meta-analyzing medians including transforming medians and percentiles to means and SDs, and using linear quantile mixed models.  We present results from a simulation study, demonstrate our approaches on several real life data sets and provide guidelines for data analysts.
}{Nous nous intéressons à la méta-analyse des valeurs médianes. De telles données émanent généralement de méta-analyses qui évaluent des réponses d'ordre temporel (par ex.~: délai de diagnostic, durée d'hospitalisation) et elles sont particulièrement intéressantes, compte tenu de la variété des réponses rapportées dans les études sources (par ex.~: valeurs médianes et percentiles,  moyennes et écarts-types, etc.). Nous avons comparé par simulation différentes approches de méta-analyse des valeurs médianes, y compris la transformation de médianes et percentiles en moyennes et écarts-types et l'utilisation de modèles linéaires mixtes de quantiles. En plus de présenter les résultats d'une étude de simulation, nous faisons état de nos approches à l'aide de plusieurs ensembles de données réelles et fournissons des lignes directrices aux analystes de données.
}}
%% Talk ps-pn
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 12:00]\\}
{\Author{Paul}{Nguyen}{nguyen}{2C-P}\Author{Lennon}{Li}{li}{2C-P}\Author{Patrick}{Brown}{brown}{2C-P}\Author{Steven}{Johnson}{johnson}{2C-P}\Author{John}{McLaughlin}{mclaughlin}{2C-P}%
\abshead{\absauthor{PAUL NGUYEN} \& \absauthor{LENNON LI}\absaffil{Public Health Ontario}, \absauthor{PATRICK BROWN}\absaffil{Cancer Care Ontario}, \absauthor{STEVEN JOHNSON} \& \absauthor{JOHN MCLAUGHLIN}\absaffil{Public Health Ontario}}
        {Spatial Cluster Detection for Ontario Public Health Research\newline
        Détection de grappes spatiales aux fins d'une recherche de Santé publique Ontario}

\absSideBySide{Cluster detection is important for assessment and surveillance of disease patterns in public health; however, location data are reported with postal codes, which are dense in urban and sparse in rural areas. Misclassification errors are unavoidable when geocoding data to census regions. To overcome these difficulties, kernel smoothing local-EM algorithm is applied to data aggregated to forward sortation areas (geographical units based on the first 3 characters of postal codes). This method estimates the risk on a tessellation where cumulative intersections of multiple maps are distinct regions. Local-EM algorithm is demonstrated on simulated influenza and gonorrhea cases in Southern Ontario.
}{La détection de grappes est importante pour l'évaluation et la surveillance du tableau réel de la morbidité en santé publique; cependant, les données de localisation sont fournies par codes postaux, des valeurs denses en milieu urbain et éparses en zones rurales. Le géocodage des données par zones de recensement rend inévitables les erreurs de mauvaise classification. Afin de surmonter ces difficultés, un algorithme EM local de lissage par noyau est appliqué aux données agrégées pour arriver à des zones de triage (unités géographiques basées sur les trois premiers caractères du code postal). Cette méthode permet d'estimer le risque pour un pavage sur lequel les intersections cumulatives de cartes multiples sont des régions distinctes. L'algorithme EM local est mis en œuvre dans des cas simulés d'influenza et de gonorrhée dans le Sud ontarien.
}}
%% Talk ps-np
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 12:00]\\}
{\Author{Nidhi}{Patel}{patel}{2C-P}\Author{Utkarsh J.}{Dang}{dang}{2C-P}\Author{Paul D.}{McNicholas}{mcnicholas}{2C-P}%
\abshead{\absauthor{NIDHI PATEL}, \absauthor{UTKARSH J. DANG} \& \absauthor{PAUL D. MCNICHOLAS}\absaffil{McMaster University}}
        {Longitudinal Clustering via Mixtures of Multivariate Power Exponential Distributions\newline
        Mise en grappes longitudinale par mélanges de lois de puissance exponentielles  multivariées}

\absSideBySide{A mixture model approach for clustering longitudinal data is introduced. The approach, which is based on mixtures of multivariate power exponential distributions, allows for varying tail-weight and peakedness in data. In the longitudinal setting, this corresponds to more or less concentration around the most central time course in a component. The models utilize a modified Cholesky decomposition of the component scale matrices and the associated MLEs are derived via a generalized EM algorithm.
}{Un modèle de mélange pour une mise en grappes de données longitudinales est présenté. Cette approche fondée sur des mélanges de lois de puissance exponentielles multivariées permet de varier le relèvement des ailes et la pointicité des données. Dans un contexte longitudinal, cela correspond à une plus ou moins forte concentration autour de la chronologie la plus centrée dans une composante. Les modèles font appel à une décomposition de Cholesky des matrices d'échelle des composantes et les estimateurs du maximum de vraisemblance associés sont dérivés à l'aide d'un algorithme EM généralisé.
}}
%% Talk ps-rp
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 12:00]\\}
{\Author{Reuben}{Pereira}{pereira}{2C-P}\Author{Patrick}{Brown}{brown}{2C-P}\Author{Paul}{Nguyen}{nguyen}{2C-P}%
\abshead{\absauthor{REUBEN PEREIRA}\absaffil{University of Toronto}, \absauthor{PATRICK BROWN}\absaffil{Cancer Care Ontario}, \absauthor{PAUL NGUYEN}\absaffil{Public Health Ontario}}
        {Comparing Spatial Models for Mapping Larynx Cancer: A Simulation Study\newline
        Comparaison de modèles spatiaux pour cartographier le cancer du larynx~: une étude de simulation}

\absSideBySide{When dealing with aggregated spatial information, standard spatial methods can be inappropriate and result in misleading inference. The goal of this study was to assess the level of aggregation at which aggregated and location specific models yield comparable results, and the context under which this may be achieved. For this analysis the aggregated and location specific models were fitted to simulated larynx cancer events at the 2010 census block and county levels in Kentucky and California. The calculated Mean Integrated Squared Errors and ROC curves were used to investigate at which aggregation level the spatial models were comparable.
}{Lorsqu'il faut traiter des données spatiales agrégées, les méthodes spatiales standards peuvent être inappropriées et aboutir à une inférence trompeuse. Le but de cette étude était d'évaluer à la fois le niveau d'agrégation où les modèles agrégés et ceux de position précise donnent des résultats comparables et le contexte dans lequel cela se produit. Aux fins de cette analyse, les modèles agrégés et ceux de position précise ont été ajustés sur des cas de cancer du larynx simulés dans les îlots de recensement et comtés du Kentucky et de la Californie pour l'année 2010. Les erreurs quadratiques moyennes intégrées et courbes ROC calculées ont été utilisées pour savoir à quel niveau d'agrégation les modèles spatiaux sont comparables.
}}
%% Talk ps-gp
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 12:00]\\}
{\Author{Gregory}{Pond}{pond}{2C-P}\Author{Cecilia A.}{Cotton}{cotton}{2C-P}\Author{Richard J.}{Cook}{cook}{2C-P}%
\abshead{\absauthor{GREGORY POND}\absaffil{McMaster University}, \absauthor{CECILIA A. COTTON} \& \absauthor{RICHARD J. COOK}\absaffil{University of Waterloo}}
        {A Biostatistics Training Initiative for Cancer Research\newline
        Projet de formation en biostatistique pour la recherche sur le cancer}

\absSideBySide{The Biostatistics Training Initiative (BTI) is an important program sponsored by the Ontario Institute for Cancer Research. Its goals are to increase the capacity and capability for cancer research in the province through advanced, experiential training in biostatistics. To date 23 Master's students from the University of Waterloo have been placed in eight month internships with mentors at cancer centres in Ontario. In 2016, through additional support from CANSSI, a new and expanded BTI provides support for doctoral and postdoctoral trainees in the province, and features a monthly seminar series on biostatistical methods for, and applications to, cancer research.
}{Le projet de formation en biostatistique pour la recherche sur le cancer (acronyme anglais BTI) est un important programme parrainé par l'Institut ontarien de recherche sur le cancer. L'objectif est d'accroître la capacité de recherche sur le cancer dans cette province par une formation expérientielle avancée en biostatistique. À ce jour, 23 étudiants à la maîtrise de l'Université de Waterloo ont eu droit à un stage de huit mois avec un mentor dans des centres de cancérologie ontariens. En 2016, grâce à un soutien additionnel de l'Institut canadien des sciences statistiques, un nouveau projet BTI élargi est ouvert à des étudiants au doctorat et en formation postdoctorale dans la province. Il comporte une série de séminaires mensuels sur les méthodes biostatistiques et leurs applications en matière de recherche sur le cancer.
}}
%% Talk ps-ats
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 12:00]\\}
{\Author{Anna Theresa}{Santiago}{santiago}{2C-P}\Author{Malcolm}{Binns}{binns}{2C-P}\Author{Wendy}{Lou}{lou}{2C-P}%
\abshead{\absauthor{ANNA THERESA SANTIAGO}, \absauthor{MALCOLM BINNS} \& \absauthor{WENDY LOU}\absaffil{University of Toronto}}
        {Application of Dimension Reduction to Acquired Brain Injury Assessment\newline
        Application de la réduction dimensionnelle à l'évaluation de la lésion cérébrale acquise}

\absSideBySide{The high-dimensional and inter-correlated nature of variables used in the evaluation of human brain function often calls for exploratory and confirmatory factor analyses that allow for the detailed exploration of structural relationships in neuroscience data.  Motivated by a study involving patients with acquired brain injury (ABI), an overview of current statistical approaches, including principal component analysis, will be presented. The pros and cons of selected dimension reduction techniques will be discussed through real examples with joint consideration of neuropsychological, psychosocial, and functional data from these ABI patients.
}{La nature à dimensions élevées et à intercorrélations des variables utilisées dans l'évaluation de la fonction cérébrale humaine exige souvent des analyses factorielles exploratoires et confirmatoires qui permettent d'examiner en profondeur les relations structurelles des données en neurosciences. Motivée par une étude portant sur des patients atteints d'une lésion cérébrale acquise (LCA), on présentera un aperçu des approches statistiques courantes, y compris l'analyse en composantes principales. Les avantages et inconvénients de certaines techniques de réduction dimensionnelle seront discutés à l'aide d'exemples réels prenant en compte des données relatives aux patients avec une LCA à la fois d'ordre neuropsychologique, psychosocial et fonctionnel.
}}
%% Talk ps-ss
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 12:00]\\}
{\Author{Samira}{Soleymani}{soleymani}{2C-P}\Author{Ian}{McLeod}{mcleod}{2C-P}%
\abshead{\absauthor{SAMIRA SOLEYMANI} \& \absauthor{IAN MCLEOD}\absaffil{Western University}}
        {Simulating with Box-Cox transformations\newline
        Étude de simulation avec transformations de Box-Cox}

\absSideBySide{A new more efficient algorithm is developed for simulating from truncated multivariate normal distributions. A new family of statistical distributions generated by Box-Cox transformations is introduced and the simulation method is data generation from these distributions. Our results have applications to space-time Kriging.
}{Un nouvel algorithme plus efficace est mis au point à des fins de simulation à partir de distributions normales multivariées. Une nouvelle famille de distributions statistiques générée par les transformations de Box-Cox est présentée et la méthode en simulation est une production de données à partir de ces distributions. Nos résultats comportent des applications en krigeage de l'espace-temps.
}}
%% Talk ps-ms
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 12:00]\\}
{\Author{Mohsen}{Soltanifar}{soltanifar}{2C-P}\Author{Armend}{Lokku}{lokku}{2C-P}\Author{Shahriar}{Shams}{shams}{2C-P}\Author{Paul}{Corey}{corey}{2C-P}%
\abshead{\absauthor{MOHSEN SOLTANIFAR}, \absauthor{ARMEND LOKKU}, \absauthor{SHAHRIAR SHAMS} \& \absauthor{PAUL COREY}\absaffil{University of Toronto}}
        {Factors Associated to Brain Reaction Time: A Higher Ordered Complete Random Block Design Study\newline
        Facteurs associés au temps de réaction du cerveau~: étude d'un plan en blocs aléatoires complet d'ordre supérieur}

\absSideBySide{Brain reactionary abilities in relation to individual and contextual factors have been of increasing interest among psychologists in past decades. The dataset for this study was collected from a completely randomized block design cross-over trial using American game developer Gary Darby's computer game on 48 university graduate students with oral and visual distractors to measure their association with reaction times. Results showed (i) a significant impairing association of both oral and visual distractor with reaction times, (ii) their significant effect modification on each other's association with outcome, and; (iii) significant association of arithmetic skills and driving experience with reaction times.
}{L'intérêt des psychologues pour les capacités réactionnelles du cerveau liées aux facteurs individuels et contextuels s'est accru au cours des dernières décennies. L'ensemble de données utilisé pour cette étude a été collecté à partir de l'essai intercroisé en plan blocs aléatoires complet faisant appel au jeu informatique du développeur américain Gary Darby. L'essai a été mené auprès de 48 étudiants universitaires de cycle supérieur et comportait des distracteurs verbaux et visuels pour en mesurer l'association à leur temps de réaction. Les résultats indiquaient (i) une association significativement déficiente du distracteur à la fois verbal et visuel avec le temps de réaction; (ii) une modification d'effet significative de l'association de l'un et l'autre à la réponse; et (iii) une association significative des aptitudes arithmétiques et expérience de conduite avec le temps de réaction.
}}
%% Talk ps-as
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 12:00]\\}
{\Author{Aaron}{Springford}{springford}{2C-P}\Author{David}{Thomson}{thomson}{2C-P}%
\abshead{\absauthor{AARON SPRINGFORD} \& \absauthor{DAVID THOMSON}\absaffil{Queen's University}}
        {Predicting Long-Term Missing Ground-Level Ozone Using Precursor, Solar and Climate Data\newline
        Prévoir l'absence à long terme de l'ozone au sol à l'aide de données de précurseurs, solaires et climatiques}

\absSideBySide{Recent years have seen a general decline in ambient ground-level ozone concentrations in developed nations. However, Health Canada researchers are investigating what causes variation in ozone-related mortality risk in larger Canadian cities. As part of this investigation, we are examining the ground-level ozone records carefully, and generating predicted ozone concentrations when none are available. We present two case studies -- Halifax and Toronto -- in which local ozone concentrations appear to be primarily driven by different mechanisms, and develop methods of prediction for each. Predictions are based on data that are often measured at different locations and different sampling rates.
}{Depuis quelques années, on a observé un déclin général des concentrations ambiantes de l'ozone au sol. Cependant, les chercheurs de Santé Canada s'emploient à étudier les causes d'une variation du risque de mortalité lié à l'ozone dans de grandes villes canadiennes. Un volet de cette étude vise à examiner attentivement les rapports concernant l'ozone au sol, et à déterminer les concentrations d'ozone lorsqu'aucune prévision n'est disponible. Nous présentons deux études de cas – Halifax et Toronto – là où les concentrations locales d'ozone semblent être principalement causées par divers mécanismes et nous élaborons des méthodes de prévision dans chaque cas. Ces prévisions sont basées sur des données souvent mesurées en fonction de différents endroits et taux d'échantillonnage.
}}
%% Talk ps-iw
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 12:00]\\}
{\Author{Ian}{Waudby-Smith}{waudbysmith}{2C-P}\Author{Zhihui (Amy)}{Liu}{liu}{2C-P}\Author{Olli}{Saarela}{saarela}{2C-P}%
\abshead{\absauthor{IAN WAUDBY-SMITH}\absaffil{Cancer Care Ontario; University of Waterloo}, \absauthor{ZHIHUI (AMY) LIU}\absaffil{Cancer Care Ontario}, \absauthor{OLLI SAARELA}\absaffil{University of Toronto}}
        {Multi-state Models for Chronic Kidney Disease Prevalence Projections in Ontario\newline
        Modèles multi-états pour des prévisions sur la prévalence de la néphropathie chronique en Ontario}

\absSideBySide{We consider multi-state models aimed at projecting the prevalence of chronic kidney disease and the cost of its treatment in Ontario. Compared to using time-series methods to predict aggregate patient counts, both short and long term projections can benefit from an individual-level modeling approach by capturing the entire disease progression and treatment history and incorporating patient-level characteristics. We model the transitions from healthy state to pre-dialysis, through various dialysis modalities such as home and in-facility dialysis, to attrition. We then simulate each patient's future trajectories, which can be translated to measures such as patient years and treatment costs.
}{Nous faisons appel à des modèles multi-états visant à fournir des prévisions sur la prévalence de la néphropathie chronique et du coût de son traitement en Ontario. En les comparant aux méthodes de séries chronologiques pour prédire les nombres agrégés de patients, il peut être avantageux pour arriver à des prévisions à court et à long terme d'utiliser une modélisation individuelle en capturant la progression complète de la maladie et l'historique de traitement, tout en incorporant des caractéristiques propres aux patients. Nous modélisons le passage d'un bon état de santé au stade prédialytique à partir de diverses modalités de dialyse, à domicile ou en milieu médical, puis à l'attrition. Nous simulons ensuite les trajectoires ultérieures de chaque patient qui peuvent ensuite être transformées en mesures, comme le nombre de patients-années et les coûts de traitement.
}}
%% Talk ps-dzdx
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 12:00]\\}
{\Author{Da Zhong (Dexen)}{Xi}{xi}{2C-P}\Author{Steve}{Taylor}{taylor}{2C-P}%
\abshead{\absauthor{DA ZHONG (DEXEN) XI}\absaffil{Western University}, \absauthor{STEVE TAYLOR}\absaffil{Pacific Forestry Centre}}
        {Joint Modeling of Duration and Burn Area of Lightning Caused Forest Fires\newline
        Modélisation conjointe de la durée et de l'étendue des feux de forêt causés par les éclairs}

\absSideBySide{This talk discusses the development and the application of statistical methods to jointly model the duration and burn area of severe lightning-caused wildfires in BC. In the joint analysis of the survival processes corresponding to fire duration and the burn area, outcomes are linked through a shared latent random risk term. For such correlated outcomes, incorporating risks that reflect shared latent factors affecting outcomes may be useful for gaining precision for the estimation of parameters and, hence, the identification of risk factors. The use of copulas models to connect the two outcomes will also be considered.
}{Cet exposé fait état de la mise au point et de l'application de méthodes statistiques pour modéliser conjointement la durée et l'étendue des feux de forêt hors contrôle causés par des éclairs en Colombie-Britannique. Dans cette analyse conjointe des processus de survie correspondant à la durée et à l'étendue de l'incendie, les réponses sont liées par terme de risque aléatoire latents partagé. Pour de telles réponses corrélées, il peut être utile d'incorporer des risques reflétant des facteurs latents partagés influant sur les réponses, à des fins de précision accrue de l'estimation des paramètres et conséquemment, de l'identification des facteurs de risque. Le recours à des modèles de copules pour lier les deux réponses est aussi considéré.
}}
%% Talk ps-xxz
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 12:00]\\}
{\Author{Xu Xuan}{Zhang}{zhang}{2C-P}\Author{Paul D.}{McNicholas}{mcnicholas}{2C-P}%
\abshead{\absauthor{XU XUAN ZHANG} \& \absauthor{PAUL D. MCNICHOLAS}\absaffil{McMaster University}}
        {Non-Gaussian Mixture Model Averaging for Clustering\newline
        Approche par modèle moyen de mélange non gaussien pour la répartition en grappes}

\absSideBySide{Model averaging has recently been applied for Gaussian mixture model-based clustering, where either models or a posteriori probabilities are averaged. A similar approach is taken using non-Gaussian mixtures, where averaging models demands consideration of concentration and/or skewness parameters. In particular, merging will be applied to mixtures of skew t-distributions and the performance of the two averaging approaches – averaging a posteriori probabilities and model averaging --- will be compared using both real and simulated data.
}{L'approche par modèle moyen a récemment été appliquée à la répartition en grappes à l'aide de modèles de mélange gaussiens lorsque la moyenne soit des modèles, soit des probabilités a posteriori est calculée. Nous adoptons une approche similaire pour des modèles de mélange non gaussiens lorsque le calcul du modèle moyen exige de prendre en compte des paramètres de concentration ou d'asymétrie. En particulier, une fusion sera appliquée à des mélanges de lois t asymétriques (skew-t) et la performance des deux méthodes de calcul de moyenne – calcul de moyenne de probabilités a posteriori et aussi de modèles – sera comparée à l'aide de données réelles et simulées.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 2D-I1: Developing Curriculum Guidelines for Canadian Statistics Undergraduate Programs\\Comment mettre au point des directives pour les programmes de premier cycle en statistique au Canada}
\begin{center}{\large Organizer and Chair / Responsable et présidente:  Bethany White (University of Western Ontario)}
\end{center}
\par \vspace{3pt}

\begin{center}\large\bfseries Room/Salle: Academic South 217\end{center}
\label{abs-sid:dcg}
\begin{center}{\large\bfseries Description}\end{center}
\bigskip
\absSideBySide{A working group formed at the 2015 "Advancing Innovation and Scholarship in Statistics Education in Canada" workshop is developing a set of
recommendations along the lines of the ASA-endorsed US recommendations
(\url{http://www.amstat.org/education/curriculumguidelines.cfm}). The goal is to
create guidelines that Canadian departments can use to inform curriculum
decisions. To initiate discussion, we will provide an overview of the
development process and share the draft guidelines. Then, attendees and
working group members will revise and refine them together. Copies of the
draft guidelines will be circulated during the session. Advance copies are
available from Bethany White (bwhite@stats.uwo.ca).\\
Working Group:
Karen Buro (MacEwan), Sotirios Damouras (Toronto-Scarborough), Alison
Gibbs (Toronto), John Petkau (British Columbia), John Sheriff
(Lethbridge), Jim Stallard (Calgary), and Bethany White (Western).
}{Un groupe de travail établi lors de l'atelier «~Comment faire progresser l'innovation et l'érudition en éducation statistique au Canada~» de 2015 est en train de mettre au point une série de recommandations inspirées des recommandations américaines approuvées par l'ASA (\url{http://www.amstat.org/education/curriculumguidelines.cfm}). L'objectif est de créer des directives que pourront utiliser les départements canadiens pour informer leurs décisions relatives aux programmes d'enseignement. Pour lancer la discussion, nous présenterons un survol du processus de développement et une première ébauche des directives. Ensuite, les participants et les membres du groupe de travail les réviseront et les affineront ensemble. Des copies des ébauches de directives seront distribuées lors de la session. Vous pouvez vous en procurer à l'avance auprès de Bethany White (bwhite@stats.uwo.ca).\\
Groupe de travail :
Karen Buro (MacEwan), Sotirios Damouras (Toronto-Scarborough), Alison
Gibbs (Toronto), John Petkau (British Columbia), John Sheriff
(Lethbridge), Jim Stallard (Calgary), et Bethany White (Western).}\bigskip
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 2D-I2: Genomic Data Analysis, Modelling and Testing\\Analyse, mod\'elisation et tests pour des donn\'ees de s\'equen\c cage g\'enomique}
\begin{center}{\large Organizer and Chair / Responsable et président:  Brad McNeney (Simon Fraser University)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Welch - David Howes\end{center}
\label{abs-sid:gda}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk gda-ak
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 13:30]\\}
{\Author{Ali}{Karimnezhad}{karimnezhad}{2D-I2}\Author{David R.}{Bickel}{bickel}{2D-I2}%
\abshead{\absauthor{ALI KARIMNEZHAD} \& \absauthor{DAVID R. BICKEL}\absaffil{University of Ottawa}}
        {Robust Bayesian Multiple Hypothesis Testing with Application to Genetic Association Data\newline
        Test d'hypothèses multiples bayésien robuste avec applications aux données d'association génétiques}

\absSideBySide{An important objective in simultaneous hypothesis testing terminology is to control false discoveries. In most of studies in the literature, local false discovery 
rates (LFDRs) are estimated based on a combined analysis in which all features presented together are combined to be analyzed together. However, sometimes the 
nature of data suggests a separate analysis in which only features with the same property in common are analyzed together.\\
\\
Conducting both separate and combined analyses might lead to two different decisions which results in an uncertainty in making a unique decision. Overcoming with 
this kind of uncertainty, we introduce novel approaches including robust Bayes and information-theoretic ones. We analyze a genetic association data set to 
illustrate practical utility of the proposed methods.
}{Un objectif important de la terminologie des tests d'hypothèses simultanés est le contrôle des fausses découvertes. Dans la plupart des études, 
le taux de fausses découvertes locales (LFDR) est estimé à partir d'une analyse combinée dans laquelle toutes les caractéristiques présentées sont combinées pour 
être analysées conjointement. Cependant, la nature des données suggère parfois une analyse séparée où seules les caractéristiques avec une même propriété en commun 
sont analysées conjointement.\\
\\
Procéder aux analyses séparées et combinées peut conduire à deux décisions différentes ce qui amène une incertitude dans la prise d'une décision unique. Pour 
surmonter ce type d'incertitude, nous présentons des approches novatrices dont des approches robustes bayésiennes et théoriques de l'information. Nous analysons 
un ensemble de données d'association génétiques pour illustrer l'utilité pratique des méthodes proposées.
}}
%% Talk gda-ds
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 14:00]\\}
{\Author{David A.}{Stephens}{stephens}{2D-I2}\Author{Farhad}{Shokoohi}{shokoohi}{2D-I2}\Author{Aurélie}{Labbe}{labbe}{2D-I2}%
\abshead{\absauthor{DAVID A. STEPHENS}, \absauthor{FARHAD SHOKOOHI} \& \absauthor{AURÉLIE LABBE}\absaffil{McGill University}}
        {Hidden Markov Models for Identifying Differentially Methylated Regions\newline
        Modèles de Markov cachés pour l'identification des régions différentiellement méthylées}

\absSideBySide{We develop specialized hidden Markov models (HMMs) to address the issue of detecting differentially methylated regions.  These regions are potentially important in 
explaining phenotypic differences.  HMM tools have only rarely been used in the analysis of methylation data, and our new model attempts to address issues of data 
corruption, and also the different sources of variability that methylation profiles typically exhibit.  The computational challenge is, however, large and different 
strategies based on EM, direct optimization and MCMC are examined.  A test data set involving the BLK gene region, different cell types, is studied.
}{Nous développons des modèles de Markov cachés spécialisés (HMMs) pour traiter du problème de la détection des régions différentiellement méthylées. Ces régions sont 
potentiellement importantes pour expliquer les différences phénotypiques. Les outils HMM ont rarement été utilisés pour l'analyse de données de méthylation, et notre 
nouveau modèle tente d'aborder les questions de corruption de données et les différentes sources de variabilité que les profils de méthylation possèdent habituellement. Par 
contre, ce défi computationnel est vaste et différentes stratégies fondées sur le EM, l'optimisation directe et MCMC sont étudiées. Un ensemble de données test 
impliquant la région génétique BLK, différents types de cellules, est étudié.
}}
%% Talk gda-es
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 14:30]\\}
{\Author{Elena}{Szefer}{szefer}{2D-I2}\Author{Jinko}{Graham}{graham}{2D-I2}\Author{Faisal}{Beg}{beg}{2D-I2}\Author{Donghuan}{Lu}{lu}{2D-I2}\Author{Farouk}{Nathoo}{nathoo}{2D-I2}%
\abshead{\absauthor{ELENA SZEFER}\absaffil{Simon Fraser University \& The EMMES Corporation}, \absauthor{JINKO GRAHAM}, \absauthor{FAISAL BEG} \& \absauthor{DONGHUAN LU}\absaffil{Simon Fraser University}, \absauthor{FAROUK NATHOO}\absaffil{University of Victoria}}
        {A Multivariate Imaging Genomics Application in Alzheimer's Disease\newline
        Une application d'imagerie génomique multivariée dans les cas de la maladie d'Alzheimer}

\absSideBySide{Both genetic variants and brain region abnormalities are recognized to play a role in cognitive decline. We explore the relationship between genome-wide variation 
and region-specific rates of decline in brain structure using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI). The high-dimensionality of the 
genomic data lends itself to regularized methods, and we use regularized multivariate methods to estimate sparse relationships between the tens of thousands of 
SNPs and dozens of brain imaging phenotypes.  Because variable selection is challenging in this high dimensional setting, resampling techniques are incorporated 
to estimate the importance of each SNP in our sample.  We also pursue validation of our results using data from the second phase of the ADNI study.
}{Les variantes génétiques et les anomalies de certaines régions du cerveau sont toutes deux reconnues comme jouant un rôle dans le déclin cognitif. Nous explorons la 
relation entre la variation pangénomique et les taux de décroissance dans la structure cérébrale en utilisant des données de l'initiative en neuroimagerie de la 
maladie d'Alzheimer (ADNI). La haute dimensionnalité des données génomiques convient aux méthodes régularisées et nous utilisons des méthodes régularisées 
multivariées pour estimer les rares relations entre les dizaines de milliers de SNPs et les douzaines de phénotypes d'imageries cérébrales. Parce que la sélection de 
variables est difficile dans ce cadre à grandes dimensions, des techniques de rééchantillonnage sont incorporées pour estimer l'importance de chaque SNP dans notre 
échantillon. Nous poursuivons aussi la validation de nos résultats en utilisant des données de la deuxième phase de l'étude ADNI.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 2D-I3: Long Memory Time Series\\S\'eries chronologiques \`a m\'emoire longue}
\begin{center}{\large Organizer and Chair / Responsable et président:  Bovas Abraham (University of Waterloo)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 242\end{center}
\label{abs-sid:lmt}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk lmt-cmh
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 13:30]\\}
{\Author{Clifford M.}{Hurvich}{hurvich}{2D-I3}\Author{Rohit}{Deo}{deo}{2D-I3}\Author{Jun}{Liu}{liu}{2D-I3}%
\abshead{\absauthor{CLIFFORD M. HURVICH}, \absauthor{ROHIT DEO} \& \absauthor{JUN LIU}\absaffil{New York University}}
        {The Slow Convergence of OLS Estimators of Alpha, Beta, and Portfolio Weights Under Long-Memory Stochastic Volatility\newline
        La lente convergence des estimateurs OLS d'alpha, de beta et des poids de portfolio selon la volatilité stochastique à mémoire longue}

\absSideBySide{We consider inference for the market model coefficients based on
simple linear regression under a long memory stochastic volatility
generating mechanism for the returns. We obtain limit theorems for
the ordinary least squares (OLS) estimators of $\alpha$ and $\beta$
in this framework. These theorems imply that the convergence rate of
the OLS estimators is typically slower than $\sqrt{T}$ if both the
regressor and the predictor have long memory in volatility, where $T$
is the sample size. The traditional standard errors of the
OLS-estimated intercept ($\hat \alpha$) and slope ($\hat\beta$),
which disregard long memory in volatility, are typically too
optimistic, and therefore the traditional $t$-statistic for testing,
say, $\alpha = 0$ or $\beta = 1$, will diverge under the null
hypothesis.
}{Nous examinons l'inférence pour les coefficients des modèles de marché fondée sur la régression linéaire simple selon un mécanisme générateur de volatilité stochastique 
à mémoire longue pour les rendements. Nous obtenons des théorèmes limites pour les estimateurs moindres carrés réguliers (OLS) de  $\alpha$ et $\beta$ dans ce cadre. 
Ces théorèmes impliquent que la vitesse de convergence des estimateurs OLS soit typiquement plus lente que $\sqrt{T}$ si le régresseur et le prédicteur possèdent tous 
deux une volatilité à mémoire longue, où $T$ est la taille de l'échantillon. L'erreur type traditionnelle de l'ordonnée ($\hat \alpha$) et de la pente estimées 
des OLS, qui ne tiennent pas compte de la longue mémoire de la volatilité, sont typiquement trop optimistes et par conséquent, la statistique-$t$ traditionnelle pour 
tester, par exemple, $\alpha = 0$ ou $\beta = 1$, va diverger sous l'hypothèse nulle.
}}
%% Talk lmt-vr
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 14:00]\\}
{\Author{Valderio}{Reisen}{reisen}{2D-I3}\Author{Celine}{Levy-Leduc}{levyleduc}{2D-I3}\Author{Fabio}{Fajardo}{fajardo}{2D-I3}\Author{Murad}{Taqqu}{taqqu}{2D-I3}%
\abshead{\absauthor{VALDERIO REISEN}\absaffil{University of Espirito Santo}, \absauthor{CELINE LEVY-LEDUC}\absaffil{AgroParisTech}, \absauthor{FABIO FAJARDO}\absaffil{UFES}, \absauthor{MURAD TAQQU}\absaffil{Boston University}}
        {Long-memory Processes. Robust Spectral Estimators Based on Frequency Domain\newline
        Processus à mémoire longue. Estimateurs spectraux robustes fondés sur un domaine de fréquence}

\absSideBySide{This paper discusses the outlier effects on the estimation of a spectral estimator for long memory process under additive outliers and proposes robust spectral 
estimators. Some asymptotic properties of the proposed robust methods are derived and Monte Carlo simulations investigate their empirical properties.  Pollution 
series, such as, PM (Particulate matter), SO2 (Sulfur dioxide), are the applied examples investigated here to show the usefulness of the proposed  robust methods 
in real applications.  These pollutants present, in general, observations with high levels of pollutant concentrations which may produce sample densities with 
heavy tails  and these high levels of concentrations can be identified as outliers which can destroy the statistical properties of sample functions such as the 
standard mean,  covariance and the periodogram.
}{Cet article discute des effets des valeurs aberrantes sur l'estimation d'un estimateur spectral pour un processus à mémoire longue selon des valeurs aberrantes 
additives et propose des estimateurs robustes spectraux. Quelques propriétés asymptotiques des méthodes robustes proposées sont dérivées et des simulations 
Monte-Carlo examinent leurs propriétés empiriques. Des séries de pollution, telles que PM (particules fines), S02 (dioxyde de soufre), sont les exemples appliqués 
examinés pour démontrer l'utilité des méthodes robustes proposées dans des applications réelles. Ces polluants présentent, en général, des observations avec de hauts 
niveaux de concentration en polluants qui peuvent produire des densités d'échantillon avec des ailes lourdes et ces hauts niveaux de concentration peuvent être 
identifiés comme étant des valeurs aberrantes, ce qui peut détruire les propriétés statistiques des fonctions d'échantillon telles que la moyenne standard, la covariance 
et le périodogramme.
}}
%% Talk lmt-at
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 14:30]\\}
{\Author{Aera}{Thavaneswaren}{thavaneswaren}{2D-I3}%
\abshead{\absauthor{AERA THAVANESWAREN}\absaffil{University of Manitoba}}
        {Nonlinear and Long Memory Count Time Series Models and Inference Using Estimating Functions\newline
        Modèles de séries chronologiques de dénombrement non linéaires et à mémoire longue et inférence utilisant des fonctions d'estimation}

\absSideBySide{This paper introduces a class of generalized count models and shows that 
the Autoregressive Conditional Poisson (ACP) models and parameter driven
count time series models
discussed in the literature are special cases. 
It is also shown that count time series
models such as quadratic stochastic count time series  models
and long memory count time series models  are special cases.
The moments of these count time series are derived. 
The combined martingale estimating functions approach, which  provides a convenient framework for
deriving optimal  inference for nonlinear time series models (Thavaneswaran et al. (2015)), is described for the proposed generalized count time series models. 
Filtering and prediction for
doubly stochastic long memory count time series models are also discussed in some detail.
}{Cet article présente une classe de modèles de dénombrement généralisés et démontre que les modèles autorégressifs conditionnels de Poisson (ACP) et les modèles 
de séries chronologiques de dénombrement axés sur les paramètres présentés dans la littérature sont des cas spéciaux. Il est aussi démontré que les modèles de séries 
chronologiques tels que les modèles chronologiques de dénombrement stochastiques quadratiques et les modèles chronologiques de dénombrement à mémoire longue sont des 
cas spéciaux. Les moments de ces séries chronologiques sont dérivés. L'approche combinée martingale des fonctions d'estimation, qui fournit un cadre pratique pour 
dériver l'inférence optimale pour les modèles chronologiques non linéaires (Thavaneswaran et al. (2015)), est décrite pour les séries chronologiques de dénombrement 
généralisées proposées. Le filtrage et la prédiction pour les modèles de séries chronologiques doublement stochastiques à mémoire longue sont aussi discutés en détail.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 2D-I4: Methods in Survival Analysis Motivated by challenges in clinical data\\M\'ethodes d'analyse de survie motiv\'ees par les probl\`emes de donn\'ees cliniques}
\begin{center}{\large Organizer and Chair / Responsable et présidente:  Lisa Le (UHN Research)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 247\end{center}
\label{abs-sid:msa}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk msa-ab
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 13:30]\\}
{\Author{Audrey}{Boruvka}{boruvka}{2D-I4}\Author{Yidan}{Shi}{shi}{2D-I4}\Author{Leilei}{Zeng}{zeng}{2D-I4}\Author{Richard J.}{Cook}{cook}{2D-I4}%
\abshead{\absauthor{AUDREY BORUVKA}\absaffil{University of Michigan}, \absauthor{YIDAN SHI}, \absauthor{LEILEI ZENG} \& \absauthor{RICHARD J. COOK}\absaffil{University of Waterloo}}
        {Analysis of Clinical Trial Endpoints under Dual Censoring\newline
        Analyse de résultats d'essais cliniques sous double censure}

\absSideBySide{Progression-free survival (PFS), given by the earliest of disease progression and death, is a common endpoint in trials for accelerated approval. The adequacy of PFS as a surrogate for overall survival has long been under debate. Issues in the analysis PFS has, until recently, received relatively little attention. The literature offers a variety of approaches to account for the ``dual'' censoring schemes typical of progression-related endpoints. In this talk we show that the assumptions needed for unbiased estimation have substantive implications for trial design and use of nonparametric methods. To address this issue we devise some recommendations for sizing trials, the timing of assessments for progression, and endpoint evaluation.
}{La survie sans progression ou PFS (telle que définie par la première des éventualités suivantes~: progression de la maladie ou décès) est une issue commune dans les essais pour autorisation accélérée. La pertinence de la PFS comme substitut de la survie globale fait depuis longtemps débat. Les problèmes concernant l'analyse de la PFS n'ont reçu que peu d'attention jusqu'à récemment. La littérature propose une variété d'approches pour tenir compte des mécanismes de censure «~double~» typiques des issues liées à la progression. Dans cette présentation, nous montrons que les hypothèses nécessaires à une estimation non biaisée ont des conséquences importantes pour les protocoles d'essais et l'utilisation de méthodes non paramétriques. Pour répondre à ce problème, nous formulons des recommandations quant à la taille des essais, au moment de l'évaluation de la progression et à l'évaluation des issues.
}}
%% Talk msa-mp
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 14:00]\\}
{\Author{Melania}{Pintilie}{pintilie}{2D-I4}\Author{Harmke}{Groot}{groot}{2D-I4}\Author{Michael}{Schaapveld}{schaapveld}{2D-I4}%
\abshead{\absauthor{MELANIA PINTILIE}\absaffil{Princess Margaret Cancer Centre}, \absauthor{HARMKE GROOT} \& \absauthor{MICHAEL SCHAAPVELD}\absaffil{The Netherlands Cancer Institute}}
        {Power for a Case-cohort Study in the Presence of Competing Risks\newline
        Puissance d'une étude de cas-cohorte en la présence de risques concurrents}

\absSideBySide{When the number of events is small relative to the size of a cohort, an efficient design is the case-cohort. In this way, all the events and a fraction of the records without events are included in the study, minimising the resources necessary to obtain the complete data for each record. In the presence of competing risks the power of the case-cohort design decreases. The calculation of power for a case-cohort design in the presence of competing risks will be presented. The results of simulations will also be shown. This work was inspired by a case-cohort study seeking to investigate the effect of testicular cancer treatment on urogenital cancer risk in the presence of mortality as a competing risk.
}{Lorsque le nombre d'événements est petit par rapport à la taille de la cohorte, un plan d'étude efficace est l'étude cas-cohorte. Celle-ci permet d'inclure dans l'étude tous les événements ainsi qu'une fraction des enregistrements sans événement, minimisant ainsi les ressources nécessaires à l'obtention des données complètes pour chaque enregistrement. Cependant, en la présence de risques concurrents, la puissance d'un tel plan d'essai diminue. Nous calculons la puissance d'un plan cas-cohorte en la présence de risques concurrents et présentons les résultats de simulations. Ces travaux ont été inspirés par une étude cas-cohorte visant à étudier l'effet du traitement du cancer testiculaire sur le risque de cancer urogénital, avec la mortalité comme risque concurrent.
}}
%% Talk msa-ep
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 14:30]\\}
{\Author{Eleanor}{Pullenayegum}{pullenayegum}{2D-I4}%
\abshead{\absauthor{ELEANOR PULLENAYEGUM}\absaffil{The Hospital for Sick Children}}
        {Using Recurrent Event Models to Handle Longitudinal Data Subject to Irregular Follow-Up\newline
        Analyse de données longitudinales à suivi irrégulier par modèles d'événements récurrents}

\absSideBySide{Recurrent event models are helpful in analysing longitudinal data subject to irregular follow-up. Irregular follow up can happen when working with clinic-based cohorts where all visits happen as part of usual care. If sicker patients are seen more often, this risks overestimating the burden of disease. One solution is to weight observations by the inverse of the visit intensity, which requires a recurrent event model for the visit process. Traditionally, this has been done using a Cox proportional hazards model. This talk will highlight examples where frailty models and accelerated failure time models are of interest.
}{Les modèles d'événements récurrents peuvent être utiles pour l'analyse de données longitudinales à suivi irrégulier. Le suivi peut en effet être irrégulier pour les cohortes en clinique où les visites se font dans le cadre des soins habituels. Puisque les patients plus malades sont examinés plus souvent, la charge de morbidité peut en être surestimée. Une solution est de pondérer les observations par l'inverse de l'intensité des visites, ce qui exige la création d'un modèle d'événements récurrent pour le processus de visite. Traditionnellement, on utilise pour cela un modèle des risques proportionnels de Cox. Nous présentons ici des exemples où les modèles de fragilité et du temps de défaillance accéléré peuvent être plus intéressants.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 2D-I5: On Some Applications of James-Stein Shrinkage Estimation\\\`A propos de quelques applications des estimateurs de r\'etr\'ecissement \`a la James Stein}
\begin{center}{\large Organizer and Chair / Responsable et présidente:  Melody Ghahramani (University of Winnipeg)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 246\end{center}
\label{abs-sid:osa}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk osa-ea
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 13:30]\\}
{\Author{Syed Ejaz}{Ahmed}{ahmed}{2D-I5}%
\abshead{\absauthor{SYED EJAZ AHMED}\absaffil{Brock University}}
        {Shrinkage Strategy: From the Margin to the Centre of Universe\newline
        Stratégie de rétrécissement~: de la bordure au centre de l'univers}

\absSideBySide{Stein revealed the most stunning result that the classical maximum likelihood estimator is inadmissible to the suggested shrinkage estimator. Since its inception, the Stein-type shrinkage estimation strategies have received considerable attention from researchers. Modern regularization estimation strategies extend Stein's procedures powerfully. A lot of research is going on in high dimensional data, where the number of variables is greater than the observations. Interestingly, in 1995, in the newsletter of the Royal Statistical Society, Efron predicted that shrinkage and empirical Bayes methodology would be a major area of statistical research for the early 21st century. Shrinkage strategy continues to be useful tools for efficient estimation. I will give some perspectives and historical developments of shrinkage strategy and its applications in big data analytics.
}{Stein a révélé le résultat le plus étonnant~: l'estimateur du maximum de vraisemblance classique est inapplicable à l'estimateur de rétrécissement suggéré. Depuis toujours, les stratégies d'estimation de rétrécissement de type Stein ont bénéficié d'une grande attention de la part des chercheurs. La plupart des stratégies d'estimation par régularisation étendent puissamment les procédures de Stein. De nombreux travaux de recherche sont en cours dans le domaine des données de grande dimension, où le nombre de variables est supérieur à celui des observations. Fait intéressant, en 1995, dans le bulletin de nouvelles de la Royal Statistical Society, Efron a prédit que le rétrécissement et les méthodes bayésiennes empiriques constitueraient un domaine de recherche important au début du 21e siècle. La stratégie de rétrécissement continue d'offrir des outils utiles pour une estimation efficace. Je donnerai un survol et une mise à jour sur la stratégie de rétrécissement et ses applications en analyse des données volumineuses.
}}
%% Talk osa-sh
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 14:00]\\}
{\Author{Shakhawat}{Hossain}{hossain}{2D-I5}\Author{Trevor}{Thomson}{thomson}{2D-I5}%
\abshead{\absauthor{SHAKHAWAT HOSSAIN} \& \absauthor{TREVOR THOMSON}\absaffil{University of Winnipeg}}
        {Shrinkage Estimation for Generalized Linear Mixed Models\newline
        Estimation de rétrécissement pour les modèles linéaires mixtes généralisés}

\absSideBySide{We proposed the pretest and shrinkage estimation methods in the generalized linear mixed models when some of the regression parameters are restricted to a linear subspace. We develop the properties of the pretest and shrinkage estimators including asymptotic distributional biases and risks. We show that these estimators have a significantly higher relative efficiency than the classical estimator. We also consider the LASSO, and numerically compare its relative performance with the proposed estimators. A Monte Carlo simulation study is conducted to evaluate the performance of these estimators with respect to the classical estimators. The study shows that the proposed estimation methods are comparable to the LASSO. A real data example is applied to illustrate the practical usefulness of the proposed estimation methods.
}{Nous proposons des méthodes d'estimation de prétest et de rétrécissement pour les modèles mixtes linéaires généralisés où certains des paramètres de régression sont limités à un sous-espace linéaire. Nous développons les propriétés des estimateurs de prétest et de rétrécissement, y compris les biais de distribution et les risques asymptotiques. Nous montrons que l'efficacité relative de ces estimateurs est beaucoup plus élevée que celle de l'estimateur classique. Nous examinons aussi le LASSO et comparons numériquement sa performance relative à celle des estimateurs proposés. Nous effectuons une étude par simulation de Monte-Carlo pour évaluer la performance de ces estimateurs par rapport aux estimateurs classiques. L'étude montre que les méthodes d'estimation proposées sont comparables au LASSO. Nous proposons un exemple de données réelles pour illustrer l'utilité pratique des méthodes d'estimation proposées.
}}
%% Talk osa-by
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 14:30]\\}
{\Author{Bahadır}{Yüzbaşi}{yuzbasi}{2D-I5}\Author{Syed Ejaz}{Ahmed}{ahmed}{2D-I5}%
\abshead{\absauthor{BAHADIR YÜZBAŞI}\absaffil{Inonu University}, \absauthor{SYED EJAZ AHMED}\absaffil{Brock University}}
        {Pretest, Shrinkage, Positive Shrinkage Ridge Estimators in Linear Regression Models\newline
        Estimateurs de prétest, de rétrécissement et de rétrécissement positif pseudo-orthogonal dans les modèles de régression linéaire}

\absSideBySide{We suggest pretest, shrinkage and positive part shrinkage estimators based on ridge estimation in the context of a linear regression model. We compare their performance with some penalty estimators namely lasso, adaptive lasso and SCAD. Monte Carlo studies are conducted to compare the relative performance of the estimators and a real data example is given to illustrate the usefulness of the suggested methods. Further, we investigate the asymptotic properties of suggested estimators.
}{Nous suggérons des estimateurs de prétest, de rétrécissement et de rétrécissement de partie positive basés sur l'estimation pseudo-orthogonale (ridge) dans le contexte d'un modèle de régression linéaire. Nous en comparons la performance avec d'autres estimateurs par pénalité comme le lasso, le lasso adaptif et le SCAD. Nous effectuons des études de Monte-Carlo pour comparer la performance relative des estimateurs et donnons un exemple de données réelles pour illustrer l'utilité des méthodes suggérées. Enfin, nous étudions les propriétés asymptotiques des estimateurs suggérés.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 2D-I6: Risk Measures in Actuarial Science\\Mesures du risque en actuariat}
\begin{center}{\large Organizer and Chair / Responsable et président:  Jun Cai (University of Waterloo)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 256\end{center}
\label{abs-sid:rma}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk rma-ef
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 13:30]\\}
{\Author{Edward}{Furman}{furman}{2D-I6}\Author{Ricardas}{Zitikis}{zitikis}{2D-I6}%
\abshead{\absauthor{EDWARD FURMAN}\absaffil{York University}, \absauthor{RICARDAS ZITIKIS}\absaffil{Western University}}
        {A New Risk Measure for Heavy Tailed Risks\newline
        Nouvelle mesure de risque pour les risques à queues lourdes}

\absSideBySide{I will introduce a new tail-based risk measure, the Tail Gini (TG) risk measure, and discuss its properties and links to Solvency II. The TG risk measure aims to catch the variability along the (right) tail of the risk's distribution, but unlike the Tail Standard Deviation risk measure ([Furman, E. and Landsman, Z. (2006). Tail variance premium with applications for Elliptical portfolio of risks. ASTIN Bulletin, 36(2), 433 - 462]), the TG risk measure only requires the finiteness of the first moment. I will suggest an economic capital allocation rule induced by the TG and show explicit expressions in the context of risk portfolios with jointly elliptical risk components. This is a joint work with Ricardas Zitikis of Western University.
}{J'introduis une nouvelle mesure du risque à queue, dite Tail Gini (TG), et discute de ses propriétés et de ses liens avec le régime Solvabilité II. La mesure du risque TG vise à capter la variabilité sur la queue (droite) de la distribution du risque, mais différemment de la mesure du risque d'écart-standard à queue (Tail Standard Deviation) ([Furman, E. et Landsman, Z. (2006). Tail variance premium with applications for Elliptical portfolio of risks. ASTIN Bulletin, 36(2), 433 - 462]), la mesure de risque TG n'exige qu'une finitude du premier moment. Je suggère une règle de répartition du capital économique induite par la mesure TG et montre des expressions explicites dans le contexte des portefeuilles de risques avec des éléments de risque conjointement elliptiques. Il s'agit d'un projet conjoint avec Ricardas Zitikis de la Western University.
}}
%% Talk rma-mm
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 14:00]\\}
{\Author{Mélina}{Mailhot}{mailhot}{2D-I6}\Author{Mhamed}{Mesfioui}{mesfioui}{2D-I6}\Author{Manuel}{Morales}{morales}{2D-I6}\Author{Nicholas}{Beck}{beck}{2D-I6}%
\abshead{\absauthor{MÉLINA MAILHOT}\absaffil{Concordia University}, \absauthor{MHAMED MESFIOUI}\absaffil{Université du Québec à Trois-Rivières}, \absauthor{MANUEL MORALES}\absaffil{Université de Montréal}, \absauthor{NICHOLAS BECK}\absaffil{Concordia University}}
        {Multivariate TVaR Risk Decomposition Techniques\newline
        Allocation de capital basée sur la TVaR multivariée}

\absSideBySide{In this talk, different methods of calculating the contribution of each risk within a portfolio with dependent business lines that are not aggregated will be presented. From an enterprise wide risk management point of view, it has become important to calculate the contribution of each risk
and provide conservative provisions. The multivariate Value-at-Risk and Tail-Value-at-Risk will be presented, and we focus on three different methods to calculate optimal finite sets for the contribution of each risk within the sums of random vectors to the overall portfolio, which could particularly apply in actuarial science. Approximations methods and empirical estimators will also be presented.
}{Différentes méthodes afin de calculer la contribution de chaque risque d'un portefeuille ayant des lignes d'affaire dépendantes seront présentées. Il est maintenant extrêmement important pour une compagnie d'assurance ou institution financière d'allouer du capital d'une manière conservatrice, afin de protéger adéquatement les assurés ou investisseurs. Les VaR et TVaR multivariées seront présentées et trois différentes techniques afin d'obtenir un vecteur de même dimension que le nombre de lignes d'affaire du portefeuille en question. Des méthodes d'approximation et estimateurs empiriques seront présentés.
}}
%% Talk rma-rw
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 14:30]\\}
{\Author{Ruodu}{Wang}{wang}{2D-I6}%
\abshead{\absauthor{RUODU WANG}\absaffil{University of Waterloo}}
        {Risk Aversion in Monetary Risk Measures\newline
        Aversion au risque dans les mesures du risque monétaire}

\absSideBySide{We incorporate the notion of risk aversion favoring prudent decisions from financial institutions into the concept of monetary risk measures. The class of monetary risk measures representing this risk aversion is referred to as consistent risk measures. We characterize the class of consistent risk measures by establishing an Expected Shortfall-based representation. The results obtained suggest that for the determination of regulatory capital, every regulator in favor of risk-averse financial decisions is essentially using a combination of Expected Shortfalls up to some adjustments. This reveals important advantages of the Expected Shortfall for prudent regulation as compared to the Value-at-Risk, a topic very much under discussion in Basel III.
}{Nous intégrons la notion d'aversion au risque qui encourage les institutions financières à prendre des décisions prudentes dans le concept des mesures du risque monétaire. Nous appelons cette classe de mesures du risque monétaire qui représente cette aversion au risque, mesures de risque cohérentes. Nous caractérisons cette classe de mesures de risque cohérentes en établissant une représentation basée sur le déficit prévu. Les résultats obtenus suggèrent que pour la détermination du capital règlementaire, tous les régulateurs en faveur de décisions financières minimisant les risques utilisent essentiellement une combinaison de déficits prévus avec certains ajustements. Cela révèle les importants avantages que présente le déficit prévu pour une règlementation prudente par rapport à la valeur exposée au risque, sujet très discuté dans le cadre de Bâle III.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 2D-C1: Actuarial Science and Finance 2\\Actuariat et finance 2}
\begin{center}{\large Chair/Président: Cary Tsai (Simon Fraser University)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 255\end{center}
\label{abs-sid:asf2}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk asf2-pa
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 13:30]\\}
{\Author{Peter}{Adamic}{adamic}{2D-C1}\Author{Jenna}{Guse}{guse}{2D-C1}%
\abshead{\absauthor{PETER ADAMIC}\absaffil{Laurentian University}, \absauthor{JENNA GUSE}\absaffil{University of Waterloo}}
        {LOESS Density Estimation for Multivariate Survival Data Subject to Censoring and Masking\newline
        Estimation de densité LOESS pour données de survie multivariées soumises à de la censure et à du masquage}

\absSideBySide{Actuaries often encounter censored and masked survival data when constructing
multiple decrement tables. In this presentation, we propose estimators for the cause-specific failure time density using LOESS smoothing techniques that are employed 
in the presence of left-censored data, while still allowing for right-censored and exact observations in addition to masked causes of failure. The smoothing mechanism 
is incorporated as part of an Expectation-Maximization (EM) algorithm. The proposed models are applied to a bivariate African trypanosomiasis data set.
}{Les actuaires rencontrent souvent des données de survie censurées et masquées lors de la construction de tables de décréments multiples. Dans cette présentation, nous 
proposons des estimateurs pour la densité des temps de défaillance par cause en utilisant les techniques de lissage LOESS qui sont utilisées en présence de données 
censurées à gauche tout en autorisant les observations exactes et censurées à droite en plus des causes masquées de défaillance. Le mécanisme de lissage est 
incorporé dans le cadre d'un algorithme de maximisation de l'espérance (EM). Les modèles proposés sont appliqués à un ensemble de données à deux variables sur la  
trypanosomiase africaine.
}}
%% Talk asf2-tb
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 13:45]\\}
{\Author{Taehan}{Bae}{bae}{2D-C1}\Author{Alex}{Kreinin}{kreinin}{2D-C1}%
\abshead{\absauthor{TAEHAN BAE}\absaffil{University of Regina}, \absauthor{ALEX KREININ}\absaffil{IBM}}
        {A Backward Construction and Simulation of Correlated Poisson Processes\newline
        Une construction en arrière et simulation de processus Poisson corrélés}

\absSideBySide{In this talk, I will discuss a generalisation of the backward simulation method by Duch et al. (2014) to build and simulate bivariate Poisson processes with specific 
time correlation structures. The proposed backward construction uses the Marshall-Olkin bivariate binomial distribution for the conditional law and some well-known 
families of bivariate copulas for joint success probability in lieu of the typical conditional independence assumption. With an appropriate choice of copula function 
and parameters, the resulting bivariate Poisson processes can incorporate various time correlation structures which are commonly observed in real data.
}{Dans cet exposé, je discuterai d'une généralisation de la méthode de simulation en arrière par Duch et al. (2014) pour construire et simuler des processus bidimensionnels 
Poisson avec des structures de corrélation chronologique spécifiques. La construction en arrière proposée utilise la loi binomiale bidimensionnelle Marshall-Olkin pour 
la loi conditionnelle et quelques familles connues de copules bidimensionnelles pour la probabilité à succès conjointe à la place de l'hypothèse d'indépendance 
conditionnelle typique. Avec le choix d'une fonction de copule et de paramètres appropriés, les processus bidimensionnels Poisson résultants peuvent incorporer différentes 
structures de corrélation chronologique qui sont communément observées dans des données réelles.
}}
%% Talk asf2-yf
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 14:00]\\}
{\Author{Yizhou}{Fang}{fang}{2D-C1}\Author{Martin}{Lysy}{lysy}{2D-C1}%
\abshead{\absauthor{YIZHOU FANG} \& \absauthor{MARTIN LYSY}\absaffil{University of Waterloo}}
        {Stochastic Volatility Modeling With Non-linear Filtering\newline
        Modélisation de la volatilité stochastique avec filtrage non-linéaire}

\absSideBySide{Since Heston model, various stochastic volatility modeling methods become very popular in the financial field and multiple assets joint modeling also attracts some attentions. However, popular models incur a heavy computational burden even in the single asset case. We try to incorporate filtering methods to models with latent variables, in order to achieve fast estimation of the joint likelihood. This approach will potentially make multiple assets stochastic volatility joint modeling possible. Primary inference results will be compared with Heston model as a benchmark.
}{Depuis le modèle Heston, diverses méthodes de modélisation de la volatilité stochastique deviennent très populaires dans le domaine financier et la modélisation conjointe de multiples actifs attire également l'attention. Cependant, les modèles populaires subissent un lourd fardeau informatique même dans le cas d'actifs uniques. Nous essayons d'intégrer des méthodes de filtrage aux modèles à variables latentes afin d'obtenir une estimation rapide de la vraisemblance conjointe. Cette approche pourrait potentiellement rendre possible la modélisation conjointe de la volatilité stochastique de plusieurs actifs. À titre de référence, les résultats d'inférence primaire seront comparés au modèle Heston.
}}
%% Talk asf2-rmk
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 14:15]\\}
{\Author{Rahim M}{Khorasany}{khorasany}{2D-C1}\Author{R. Mark}{Reesor}{reesor}{2D-C1}%
\abshead{\absauthor{RAHIM M KHORASANY}\absaffil{Western University}, \absauthor{R. MARK REESOR}\absaffil{SHARCNET Research Chair in Financial Mathematics  Department of Statistical and Actuarial Sciences  Richard Ivey School of Busin}}
        {Monte Carlo Valuation of Multiple Exercise Options: A Structured Regression Approach\newline
        Évaluation de Monte-Carlo de multiples levées d'options : une approche de régression structurée}

\absSideBySide{Recent work by Letourneau and Stentoft (2014) shows American option price estimator bias is reduced 
by imposing additional structure on the regressions used in Monte Carlo pricing algorithms. We extend 
their methodology to the Monte Carlo valuation of multiple exercise options by requiring additional 
structure on the regressions used to estimate continuation values.  The resulting price estimators have 
reduced bias, particularly for small sample sizes, and results hold across a variety of option types, 
maturities, moneyness.
}{Les travaux récents de Letourneau et Stentoft (2014) montrent que le biais de l'estimateur américain du prix de l'option est réduit en imposant une structure supplémentaire sur les régressions utilisées dans les algorithmes de prix de Monte-Carlo. Nous étendons leur méthodologie à l'évaluation de Monte Carlo de multiples levées d'options en exigeant plus de structure sur les régressions utilisées pour estimer les valeurs de continuation. Les estimateurs de prix résultant ont un biais réduit, en particulier pour les petites tailles d'échantillon, et les résultats tiennent à travers une variété de types d'options,
d'échéances et de liquidités.
}}
%% Talk asf2-em
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 14:30]\\}
{\Author{Etienne}{Marceau}{marceau}{2D-C1}\Author{Hélène}{Cossette}{cossette}{2D-C1}%
\abshead{\absauthor{ETIENNE MARCEAU} \& \absauthor{HÉLÈNE COSSETTE}\absaffil{Université Laval}}
        {A Note on Extreme Negative Dependence\newline
        Une note sur la dépendance négative extrême}

\absSideBySide{In this talk, we examine the aggregation of pair of countermonotonic random variables. Notably, we derive closed-form expressions for the cumulative distribution function or other risk measures for sum of two continuous positive countermonotonic random variables in specific cases. A general approach is also proposed to derive the VaR of the sum of two continuous positive countermonotonic random variables. Results related to extreme negative dependence for vector of more than two random variables are also presented.
}{Dans cet exposé, on examinera l'agrégation de deux variables aléatoires antimonotones. Notamment, on développera les expressions fermées de la fonction de répartition et de mesures de risque pour la somme de variables aléatoires continues positives antimonotones. Une approche générale sera aussi abordée. Des résultats reliés à la dépendance négative extrême pour des vecteurs de plus de 2 variables aléatoires seront aussi présentés.
}}
%% Talk asf2-nn
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 14:45]\\}
{\Author{Natalia}{Nolde}{nolde}{2D-C1}\Author{Johanna}{Ziegel}{ziegel}{2D-C1}%
\abshead{\absauthor{NATALIA NOLDE}\absaffil{University of British Columbia}, \absauthor{JOHANNA ZIEGEL}\absaffil{University of Bern}}
        {Elicitability and Backtesting\newline
        Élicitabilité et contrôle ex-post}

\absSideBySide{Conditional forecasts of risk measures are of importance in internal risk management and regulatory capital calculations. Backtesting is a common way to assess performance of a risk measurement procedure. While traditional backtests are concerned with assessing some optimality property of a set of risk measure estimates, they are not appropriate to compare different procedures. We investigate the proposal of comparative backtests, which are better suited for method comparisons, but necessitate an elicitable risk measure. The discussion focuses on three risk measures, value-at-risk, expected shortfall and expectiles, and is supported by simulation studies and a data analysis of financial time series.
}{Les prévisions conditionnelles des mesures de risque sont importantes dans la gestion du risque interne et dans les calculs de capital réglementaire. Le contrôle ex-post est une méthode commune pour évaluer l'efficacité d'une procédure de mesure du risque. Bien que les contrôles ex-post classiques servent à évaluer certaines propriétés d'optimalité d'un ensemble d'estimateurs de mesure de risque, ils ne sont pas appropriés pour comparer les différentes procédures. Nous étudions une proposition de contrôles ex-post comparatifs, mieux adaptés aux comparaisons de méthodes, mais qui nécessitent une mesure de risque. L'exposé met l'accent sur trois mesures de risques, à savoir la valeur à risque, la perte de valeur espérée et les expectiles. Il est appuyé par des études de simulation et une analyse de données de séries chronologiques financières.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 2D-C2: Biostatistics: Treatment Comparisons\\Biostatistique : comparaisons de traitements}
\begin{center}{\large Chair/Président: Gyanendra Pokharel (University of Calgary)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 254\end{center}
\label{abs-sid:btc}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk btc-shc
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 13:30]\\}
{\Author{Siu Hung}{Cheung}{cheung}{2D-C2}\Author{Wenfu}{Xu}{xu}{2D-C2}\Author{Feifang}{Hu}{hu}{2D-C2}%
\abshead{\absauthor{SIU HUNG CHEUNG}\absaffil{The Chinese University of Hong Kong}, \absauthor{WENFU XU}\absaffil{Renmin University}, \absauthor{FEIFANG HU}\absaffil{George Washington University}}
        {Adaptive Treatment Allocation for Non-inferiority Trials with Multiple Experimental Treatments\newline
        Allocation de traitement adaptatif pour les essais de non-infériorité avec traitements expérimentaux multiples}

\absSideBySide{Statistical procedures are available for treatment comparisons in non-inferiority clinical trials that have multiple experimental treatments. An ethical concern for non-inferiority trials is that some patients would be treated by the less effective treatments, and this problem is more serious when multiple experimental treatments are included in a balanced design, where sample sizes are the same for all experimental treatments. With the aim to send fewer patients to inferior treatments, we propose an adaptive treatment allocation scheme which is also superior to a balanced design in terms of test power.
}{Les procédures statistiques sont disponibles pour les comparaisons de traitement dans les essais cliniques de non-infériorité qui ont des traitements expérimentaux multiples. Un souci éthique dans les essais de non-infériorité fait en sorte que certains patients seraient traités avec des traitements moins efficaces et ce problème est plus sérieux lorsque plusieurs traitements expérimentaux sont inclus dans le plan équilibré où les tailles d'échantillon sont les mêmes pour tous les traitements expérimentaux. Dans le but d'envoyer moins de patients à des traitements inférieurs, nous proposons un schéma d'allocation de traitement adaptatif qui est également supérieur à un modèle équilibré en termes de puissance du test.
}}
%% Talk btc-jh
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 13:45]\\}
{\Author{James}{Hanley}{hanley}{2D-C2}\Author{Zhihui (Amy)}{Liu}{liu}{2D-C2}\Author{Olli}{Saarela}{saarela}{2D-C2}%
\abshead{\absauthor{JAMES HANLEY}\absaffil{McGill University}, \absauthor{ZHIHUI (AMY) LIU}\absaffil{Cancer Care Ontario}, \absauthor{OLLI SAARELA}\absaffil{University of Toronto}}
        {Fitting a Model of the Mortality Reductions Produced by One/Several  Rounds of Cancer Screening: Time and Sample Size Considerations\newline
        Ajuster un modèle de réduction de la mortalité produit pour un ou plusieurs cycles de dépistage du cancer : considérations de temps et de taille d'échantillon}

\absSideBySide{Previously (J Med Scr 2013, Int Stat Inst Review 2014) we described: how the earlier treatments prompted by repeated screenings halt/alter the time courses of otherwise-fatal cancers; why screening-induced  reductions in time-specific mortality rates exhibit a bathtub-shaped rate ratio curve;  and how a 3-parameter model characterizing the  impact of a single round of screening can be fitted to the observed year-specific mortality data. We now address the time and sample size requirements (numbers and time locations of deaths) to fit this rate ratio curve with a given degree of precision. This approach also applies to other non-proportional-hazards settings.
}{Auparavant (J Med Scr 2013, Revue Int Stat Inst 2014), nous avons décrit : comment les traitements antérieurs provoqués par des dépistages répétés stoppent / modifient les cours du temps de cancers autrement fatals; pourquoi les réductions induites par dépistage des taux de mortalité à des temps spécifiques présentent une courbe du ratio des taux en forme de baignoire; et comment un modèle à 3 paramètres caractérisant l'impact d'un seul cycle de dépistage peut être ajusté aux données de mortalité observées spécifiques à chaque année. Nous abordons maintenant les exigences de temps et de taille d'échantillon (nombres et emplacements des temps des décès) pour adapter cette courbe du rapport des taux avec un degré de précision donné. Cette approche est également valable pour d'autres contextes de risques non-proportionnels.
}}
%% Talk btc-tck
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 14:00]\\}
{\Author{Taddele Cherinet}{Kibret}{kibret}{2D-C2}\Author{Tian}{Feng}{feng}{2D-C2}\Author{Regina S.}{Kampo}{kampo}{2D-C2}\Author{Jemila S.}{Hamid}{hamid}{2D-C2}\Author{Joseph}{Beyene}{beyene}{2D-C2}%
\abshead{\absauthor{TADDELE CHERINET KIBRET}, \absauthor{TIAN FENG} \& \absauthor{REGINA S. KAMPO}\absaffil{McMaster University}, \absauthor{JEMILA S. HAMID}\absaffil{St. Michael's Hospital, Toronto}, \absauthor{JOSEPH BEYENE}\absaffil{McMaster University}}
        {Bayesian Network Meta-analysis (NMA) using MCMC and  Integrated Nested Laplace Approximation (INLA): A Comparative Study\newline
        Méta-analyse bayésienne par réseau à l'aide de chaînes de Markov et approximation de Laplace emboîtée intégrée (ALEI) : une étude comparative}

\absSideBySide{Network meta-analysis (NMA) is an emerging methodology and has become very popular because it allows synthesis of evidence across multiple treatments. The method combines direct and indirect evidence. A MCMC-based Bayesian approach is widely used to fit NMA models, but MCMC is generally computationally intensive. A computationally efficient alternative approach to MCMC is the so-called Integrated Nested Laplace Approximation (INLA). Using comprehensive simulations, we compared the performance of INLA and MCMC with respect to key NMA results including relative treatment effect estimates, rank probabilities for the treatments, and goodness-of-fit measures. The two approaches are also illustrated using real data sets.
}{La méta-analyse par réseau (MAR) est une méthode émergente qui est devenue très populaire car elle permet la synthèse de preuves à travers plusieurs  traitements. La méthode combine une preuve directe et indirecte. Une approche bayésienne basée sur les chaînes de Markov est largement utilisée pour ajuster les modèles MAR, mais les chaînes de Markov sont généralement computationnelles intensives. Une autre approche computationnelle intensive aux chaînes de Markov est la soi-disant approximation de Laplace emboîtée intégrée (ALEI). À l'aide de simulations complètes, nous avons comparé la performance de l'ALEI et des chaînes de Markov par rapport aux résultats clés de la MAR, y compris des estimations de l'effet relatif du traitement, les probabilités de rang pour les traitements et les mesures de validité de l'ajustement. Les deux approches sont également illustrées à l'aide de jeux de données réels.
}}
%% Talk btc-sfl
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 14:15]\\}
{\Author{Shun Fu}{Lee}{lee}{2D-C2}\Author{Allan}{Donner}{donner}{2D-C2}\Author{Neil}{Klar}{klar}{2D-C2}%
\abshead{\absauthor{SHUN FU LEE}\absaffil{Population Health Research Institute}, \absauthor{ALLAN DONNER} \& \absauthor{NEIL KLAR}\absaffil{University of Western Ontario}}
        {Adjusted $I^2$ Statistic and its Confidence Intervals\newline
        Statistique $I^2$ corrigée et ses intervalles de confiance}

\absSideBySide{The $I^2$ statistic has been adopted by many researchers for quantifying heterogeneity across trials for its intuitive interpretations allowing comparisons across meta-analyses. In case of the meta-analyses of cluster randomized trials, it is important to adjust the $I^2$ statistic for clustering and modify existing approaches for constructing confidence intervals including MOVER, Q distribution, test-based and nonparametric bootstrap. A simulation study is used to evaluate the performance of the confidence intervals in terms of coverage, tail errors and interval width. Data from to a meta-analysis of four cluster randomization trials is used for illustration.
}{De nombreux chercheurs ont adopté la statistique $I^2$ pour quantifier l'hétérogénéité entre les essais, en raison de ses interprétations intuitives qui permettent de comparer les méta-analyses. Dans le cas de méta-analyses d'essais randomisés par grappes, il est important de corriger la statistique $I^2$ pour la structure de grappes et de modifier les approches existantes pour créer des intervalles de confiance, y compris le modèle MOBILE, la distribution q, et le test de type bootstrap non paramétrique. Nous utilisons une étude de simulation pour évaluer l'efficacité des intervalles de confiance en termes de taux de couverture, d'erreur de queues et de taille de l'intervalle. Nous illustrons nos travaux à l'aide de données d'une méta-analyse de quatre essais de randomisation par grappes.
}}
%% Talk btc-tl
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 14:30]\\}
{\Author{Tong-Yu}{Lu}{lu}{2D-C2}\Author{Wai-Yin}{Poon}{poon}{2D-C2}\Author{Siu Hung}{Cheung}{cheung}{2D-C2}\Author{Ping}{Yang}{yang}{2D-C2}%
\abshead{\absauthor{TONG-YU LU}\absaffil{China Jiliang University}, \absauthor{WAI-YIN POON}, \absauthor{SIU HUNG CHEUNG} \& \absauthor{PING YANG}\absaffil{The Chinese University of Hong Kong}}
        {Covariates-Adjusted Comparison of Treatments with Ordinal Responses\newline
        Comparaison de traitements par covariables ajustées, avec des réponses ordinales}

\absSideBySide{Treatment comparison is frequently performed in clinical studies. When the responses are measured with an ordinal scale, the traditional Wilcoxon-Mann-Whitney test is a popular tool. However, it is less effective than the latent variable methods which have recently been proposed. For treatment comparisons, the inclusion of covariates can increase estimation precision and test power. However, there is a lack of effective methods for the covariates-adjusted comparison of treatments with ordinal responses. Our research gives a new identification procedure for ordinal regression models. These latent ordinal regression models can be conveniently used to conduct covariates-adjusted comparison of treatments with ordinal responses.
}{Les études cliniques visent souvent à comparer des traitements. Lorsque les réponses sont mesurées au moyen d'une échelle ordinale, on utilise souvent le test traditionnel de Wilcoxon-Mann-Whitney. Cependant, ce test est moins efficace que les méthodes de variables latentes qui ont été récemment proposées. Pour les comparaisons de traitements, l'inclusion de covariables peut augmenter la précision de l'estimation et la puissance du test. Toutefois, il y a un manque de méthodes efficaces pour comparer les traitements par covariables ajustées dans le cadre de réponses ordinales. Notre recherche implémente une nouvelle procédure d'identification pour les modèles de régression ordinale. Ces modèles latents de régression ordinale peuvent facilement être utilisés pour comparer les traitements par covariables ajustées.
}}
%% Talk btc-bn
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 14:45]\\}
{\Author{Binod}{Neupane}{neupane}{2D-C2}\Author{Joseph}{Beyene}{beyene}{2D-C2}%
\abshead{\absauthor{BINOD NEUPANE} \& \absauthor{JOSEPH BEYENE}\absaffil{McMaster University}}
        {Performance of Bayesian Network Meta-analysis for Sparse Data\newline
        Efficacité de la méta-analyse en réseau bayésienne pour des données rares}

\absSideBySide{Bayesian network meta-analysis (NMA) is increasingly used to combine evidence across multiple treatments. The method has been used in a range of scenarios including sparse data settings (e.g., rare adverse outcomes, several small trials, etc.). However, comparative performance of fixed-effects (FE) and random-effects (RE) models, and impact of discarding or including trials with zero-event in all arms are not well understood. Using extensive simulations, we showed that overall RE performs better unless event rates are very rare, average trial sizes are small or heterogeneity is low. Discarding zero total event trials did not have marked impact on the results.
}{La méta-analyse en réseau bayésienne est de plus en plus utilisée pour combiner les données scientifiques  de nombreux traitements. Cette méthode a été utilisée dans divers scénarios, y compris dans des ensembles de données rares (ex., dans l'analyse de résultats indésirables rares, dans plusieurs essais cliniques de taille modeste, etc.). Cependant, l'efficacité comparative des modèles à effets fixes et à effets aléatoires, ainsi que les répercussions de l'exclusion de ou de l'inclusion d'essais sans événement dans toutes les branches ne sont pas bien compris. Grâce à plusieurs simulations, nous avons démontré, qu'en général, les modèles à effets aléatoires fonctionnent mieux, à moins que les taux d'événement soient très rares, que la taille moyenne des essais soit modeste ou que l'hétérogénéité soit faible. L'exclusion des essais ayant un nombre d'évènements total de zéro n'a pas de répercussions importantes sur les résultats.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 2D-C3: Statistical Computing and Visualization\\Calcul statistique et visualisation}
\begin{center}{\large Chair/Président: Martin Lysy (University of Waterloo)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 257\end{center}
\label{abs-sid:scv}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk scv-sb
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 13:30]\\}
{\Author{Simon}{Bonner}{bonner}{2D-C3}\Author{Matthew}{Schofield}{schofield}{2D-C3}\Author{Steven}{Price}{price}{2D-C3}%
\abshead{\absauthor{SIMON BONNER}\absaffil{University of Western Ontario}, \absauthor{MATTHEW SCHOFIELD}\absaffil{University of Otago}, \absauthor{STEVEN PRICE}\absaffil{University of Kentucky}}
        {Mark-recapture, Misidentification, and Markov Bases\newline
        Capture-recapture, identification erronée et bases de Markov}

\absSideBySide{Link et al. (2010) introduced the latent multinomial as a general tool for modelling mark-recapture data in which individuals are partially, or incorrectly, identified. Key to this approach is an MCMC algorithm that samples from the set of possible true data sets that could have generated the observed data. We link this model and the field of algebraic statistics, showing that the original algorithm can produce reducible Markov chains that fail to cover the entire sample space and providing a solution using (dynamic) Markov bases. As an example, we present data from a mark-recapture study of queen snakes in Kentucky.
}{Link et al. (2010) ont présenté le multinomial latent comme un outil général pour la modélisation des données de capture-recapture dans lequel les individus sont partiellement ou mal identifiés. La clé de cette approche est un algorithme MCMC qui échantillonne à partir de l'ensemble des vrais jeux de données possibles qui auraient pu générer les données observées. Nous relions ce modèle au domaine des statistiques algébriques, montrant que l'algorithme original peut produire des chaînes de Markov réductibles ne parvenant pas à couvrir tout l'espace de l'échantillon et fournissant une solution à l'aide de bases de Markov (dynamiques). A titre d'exemple, nous présentons des données d'une étude de capture-recapture de couleuvres royales au Kentucky.
}}
%% Talk scv-pg
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 13:45]\\}
{\Author{Philippe}{Gagnon}{gagnon}{2D-C3}\Author{Mylène}{Bédard}{bedard}{2D-C3}\Author{Alain}{Desgagné}{desgagne}{2D-C3}%
\abshead{\absauthor{PHILIPPE GAGNON} \& \absauthor{MYLÈNE BÉDARD}\absaffil{Université de Montréal}, \absauthor{ALAIN DESGAGNÉ}\absaffil{Université du Québec à Montréal}}
        {Weak Convergence and Automation of the Reversible Jump Algorithm\newline
        Convergence faible et automatisation de l'algorithme à sauts réversibles}

\absSideBySide{The reversible jump algorithm is a useful MCMC method that allows switches between subspaces of differing dimensionality and therefore model determination. This method is now increasingly used in key areas of human activity (e.g., biology). Its main drawback is the difficulty to implement it practically and efficiently. In this paper, we focus on a simple sampling context to obtain theoretical results that lead to optimisation of the algorithm. The main contribution of this paper is the weak convergence of the stochastic process engendered by the algorithm, which is used to design optimally the functions needed to do the implementation.
}{L'algorithme à sauts réversibles est une méthode MCMC qui permet de passer d'un sous-espace ayant une certaine dimension à un autre n'ayant pas nécessairement la même dimension, et alors, de sélectionner un modèle. Cette méthode est utilisée dans plusieurs sphères de l'activité humaine (par exemple, la biologie). Il est cependant difficile d'implémenter cette méthode de manière à obtenir un algorithme efficace. Dans cet article, on étudie un contexte d'échantillonnage relativement simple afin d'obtenir des résultats théoriques qui mènent à l'optimisation de l'algorithme. La contribution principale de cet article est la convergence faible du processus stochastique engendré par l'algorithme.
}}
%% Talk scv-yl
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 14:00]\\}
{\Author{Yun}{Ling}{ling}{2D-C3}\Author{Martin}{Lysy}{lysy}{2D-C3}%
\abshead{\absauthor{YUN LING} \& \absauthor{MARTIN LYSY}\absaffil{University of Waterloo}}
        {Superfast Gaussian Likelihood Solver and Application\newline
        Solveur très rapide du maximum de vraisemblance gaussien et application}

\absSideBySide{Gaussian distributions with Toeplitz variance matrices commonly occur. Toeplitz variance matrices admit 'fast' likelihood evaluations by the famous Durbin-Levinson algorithm, scaling as $\mathcal O(N^2)$ instead of the usual $\mathcal O(N^3)$. We adapt a 'superfast' algorithm of Ammar and Gragg (1988) to calculate the likelihood in $\mathcal O(N \log^2 N)$. We also use it to derive formulas for the score function and Hessian matrix, thereby obaining superfast versions of common statistical inference algorithms. Our current implementation in the ${R}$ package ${SuperGaussian}$ beats Durbin-Levinson for $N > 512$. We present an application to drift estimation of sub-diffusive pathogen trajectories in pulmonary mucus.
}{Les lois gaussiennes ayant comme matrice de variance une matrice de Toeplitz sont communes. Les matrices de variance de Toeplitz permettent des évaluations « rapides » du maximum de vraisemblance au moyen de l'algorithme bien connu de Durbin-Levinson d'ordre $\mathcal O(N^2)$ au lieu de $\mathcal O(N^3)$. Nous adaptons l'algorithme « très rapide » de Ammar et Gragg (1988) afin de calculer le maximum de vraisemblance d'ordre $\mathcal O(N \log^2 N)$. Nous l'utilisons également afin d'obtenir des formules pour la fonction de caractérisation et la matrice de Hessian, ce qui nous permet d'obtenir des versions très rapides d'algorithmes d'inférence statistiques communs. Notre mise en œuvre actuelle dans le paquet ${R}$ ${SuperGaussian}$ surpasse l'algorithme de Durbin-Levinson lorsque $N > 512$. Nous présentons une application de l'estimation du changement des trajectoires d'un pathogène à sous-diffusion dans la muqueuse pulmonaire.
}}
%% Talk scv-wo
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 14:15]\\}
{\Author{Wayne}{Oldford}{oldford}{2D-C3}\Author{Adrian}{Waddell}{waddell}{2D-C3}%
\abshead{\absauthor{WAYNE OLDFORD}\absaffil{University of Waterloo}, \absauthor{ADRIAN WADDELL}\absaffil{Roche, Basel Switzerland}}
        {Loon – the Interactive R Visualization Toolkit for Exploration\newline
        Loon : une boîte à outils interactive pour la visualisation avec R à des fins exploratoires}

\absSideBySide{The power and extendibility of loon will be demonstrated with new tools for exploring data interactively. These include graph-based exploration of low dimensional spaces, interactive parallel and radial axes displays and glyphs, and interactive maps for geographic spatial data. Loon complements and supports statistical exploration: of data, of models, even of methods. The principal example will be visual clustering and exploration of high dimensional data. An exceedingly brief demonstration of the ease with which loon may be adapted to serve almost any visual exploration will also be given.
}{Nous démontrerons la puissance et l'extensibilité de Loon avec de nouveaux outils d'exploration interactive de données. Ces outils permettent l'exploration graphique d'espaces à basse dimension, l'affichage d'axes parallèles et radiaux  et de glyphes interactifs, ainsi que de cartes interactives pour les données géographiques spatiales. Loon complémente et permet l'exploration statistique de données, de modèles et même de méthodes. Notre principal exemple sera le regroupement et l'exploration visuelle de données de grande dimension. Nous démontrerons très brièvement la facilité avec laquelle Loon peut être adapté pour servir à presque toutes les explorations visuelles.
}}
%% Talk scv-yz
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 14:30]\\}
{\Author{Yukun}{Zhang}{zhang}{2D-C3}\Author{Haocheng}{Li}{li}{2D-C3}\Author{Sarah}{Kozey-Keadle}{kozeykeadle}{2D-C3}\Author{Charles}{Matthews}{matthews}{2D-C3}\Author{Raymond}{Carroll}{carroll}{2D-C3}%
\abshead{\absauthor{YUKUN ZHANG}\absaffil{Yukun Zhang}, \absauthor{HAOCHENG LI}\absaffil{University of Calgary}, \absauthor{SARAH KOZEY-KEADLE} \& \absauthor{CHARLES MATTHEWS}\absaffil{National Institutes of Health/National Cancer Institute}, \absauthor{RAYMOND CARROLL}\absaffil{Texas A\&M University}}
        {PAactivPAL: An R Package for Statistical Analysis on Physical Activity Data\newline
        « PAactivPAL » : un paquet R pour l'analyse statistique de données sur l'activité physique}

\absSideBySide{Researches for the links between physical activity and cancer risk have been revolutionized by the availability of accelerometers. ActivPAL$^{TM}$ is a popular wearable device in application which classifies the person's activity and estimates daily energy expenditure. The device could produce over 10,000 observations per person per day on a second-by-second basis which makes the raw activPAL$^{TM}$ data be complicated to analyze. We develop an R package PAactivPAL to solve this problem. The functions embedded summarize variables recorded in physical activity. The package also includes functions for plotting and group comparison, which facilitates users to analyze the data for different perspectives.
}{La disponibilité des accéléromètres a révolutionné les recherches sur les liens entre l'activité physique et le risque de cancer. ActivPAL$^{TM}$ est un appareil portable populaire en application  qui catégorise l'activité d'une personne et calcule sa dépense d'énergie quotidienne. L'appareil peut fournir de seconde en seconde plus de 10 000 observations par personne et par jour, ce qui complique l'analyse des données brutes activPAL$^{TM}$. Nous avons élaboré le paquet R PAactivPAL pour résoudre ce problème. Les fonctions intégrées résument les variables enregistrées de l'activité physique. Le paquet comporte aussi des fonctions pour le traçage et la comparaison de groupes, ce qui permet aux utilisateurs d'analyser facilement les données selon différentes perspectives.
}}
%% Talk scv-ab
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 14:45]\\}
{\Author{Andrej}{Blejec}{blejec}{2D-C3}%
\abshead{\absauthor{ANDREJ BLEJEC}\absaffil{National Institute of Biology, Ljubljana, Slovenia}}
        {Presentation of Statistical Concepts with Animated Graphics and Simulations in R\newline
        Présentation des concepts statistiques à l'aide de graphiques animés et de simulations en R}

\absSideBySide{Understanding of statistical concepts is important for proper use of statistics. The idea of using simulations and dynamic graphics to foster understanding of statistical concepts is not new. In recent years, R became the \textit{lingua franca} for statistical data analysis. While R graphical devices are not meant for display of animated graphics, my aim is to use base R graphics for display of animated sequences. This is enabled with a R package \textit{animator}, which supports smooth transitions of graphical elements and simplifies preparation of animated displays. I will present package features and animations that might be useful for statistics teaching.
}{La compréhension des concepts statistiques est importante pour une utilisation adéquate des statistiques. L'idée d'utiliser des simulations et des graphiques dynamiques pour favoriser la compréhension des concepts statistiques n'est pas nouvelle. Au cours des dernières années, R est devenu le \textit{lingua franca} de l'analyse de données statistiques. Alors que les dispositifs graphiques de R ne sont pas destinés à l'affichage de graphiques animés, mon but est d'utiliser des graphiques de base de R pour l'affichage de séquences animées. Cette option est activée avec un paquet de R \textit{animator}, qui prend en charge des transitions en douceur d'éléments graphiques et simplifie la préparation des affichages animés. Je présenterai les caractéristiques du paquet et des animations qui pourraient être utiles pour l'enseignement statistique.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 2D-C4: Statistical Genetics\\G\'en\'etique statistique}
\begin{center}{\large Chair/Président: David Campbell (Simon Fraser University)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 259\end{center}
\label{abs-sid:sg}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk sg-bc
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 13:30]\\}
{\Author{Bo}{Chen}{chen}{2D-C4}\Author{Radu V.}{Craiu}{craiu}{2D-C4}\Author{Lei}{Sun}{sun}{2D-C4}%
\abshead{\absauthor{BO CHEN}, \absauthor{RADU V. CRAIU} \& \absauthor{LEI SUN}\absaffil{University of Toronto}}
        {Bayesian Model Averaging Method for X Chromosome Inactivation Problem\newline
        Méthode de la moyenne bayésienne des modèles pour l'inactivation du chromosome X}

\absSideBySide{In genetic association studies it is not known whether the X-chromosome is inactivated or not. In the absence of evidence in favour of one specific model, we consider a Bayesian model averaging framework that provides a principled way to account for model uncertainty.  Given an established association, we then use data-based evidence for model selection via Bayes factors. We examine the inferential properties of the proposed methods via simulations and applications and compare with frequentist solutions.
}{Dans les études d'association génétique, on ne sait pas si le chromosome X est inactif ou pas.  En absence d'évidence en faveur d'un modèle ou l'autre, nous considérons la méthode de la moyenne bayésienne des modèles qui nous offre une alternative de principe pour intégrer, dans l'estimation, l'incertitude concernant le  modèle correct. Après qu'une association génétique ait été établie, nous considérons l'évidence fournie par le facteur  Bayes  pour choisir un modèle.  Nous examinons les propriétés de l'analyse statistique que nous proposons en utilisant des simulations et des données réelles et nous les comparons  avec des solutions fréquentistes.
}}
%% Talk sg-lc
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 13:45]\\}
{\Author{Lu}{Cheng}{cheng}{2D-C4}\Author{Mu}{Zhu}{zhu}{2D-C4}%
\abshead{\absauthor{LU CHENG} \& \absauthor{MU ZHU}\absaffil{University of Waterloo}}
        {Epistasis Detection by Reducing Disease Model Space using Clustering\newline
        Détection de l'épistasie en réduisant l'espace du modèle de maladie à l'aide de la classification}

\absSideBySide{Failure of replication for single locus effects in GWAS motivates interaction effect (epistasis) detection. Determining the interaction form, or ``disease model'' is important for the essentially variable selection task. There are theoretically $2^9$ interaction forms for each pair of SNPs and assessing all is not statistically or computationally efficient. We thus define a novel `distance' metric to measure how different they are, do a clustering of them, and then select representatives to guide the epistasis detection. Our method provides competitive power when compared to two most relevant existing method, i.e., MDR and Wan et al. (2013)'s complete compositional epistasis detection method.
}{L'échec de réplication des effets du locus simple dans les GWAS motive la détection de l'effet d'interaction (épistasie). Il est important de déterminer la forme de l'interaction ou « modèle de maladie » pour la tâche de sélection des variables. Il existe des formes d'interaction théoriques $2^9$ pour chaque paire de SNP et il n'est pas statistiquement ou informatiquement efficace de toutes les évaluer. Nous définissons alors une nouvelle métrique de « distance » pour mesurer dans quelle mesure elles sont différentes, en faire une classification et ensuite sélectionner des représentants pour guider la détection de l'épistasie. Notre méthode fournit une puissance compétitive en comparaison avec les deux méthodes existantes les plus pertinentes, soit la MDR et la méthode de détection de l'épistasie par composition complète de Wan et al. (2013).
}}
%% Talk sg-ghj
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 14:00]\\}
{\Author{Gun Ho}{Jang}{jang}{2D-C4}\Author{Lincoln}{Stein}{stein}{2D-C4}\Author{John}{Bartlett}{bartlett}{2D-C4}%
\abshead{\absauthor{GUN HO JANG}, \absauthor{LINCOLN STEIN} \& \absauthor{JOHN BARTLETT}\absaffil{Ontario Institute for Cancer Research}}
        {Locally Constant Signal Segmentation using a Relative Belief Inference\newline
        Segmentation de signal localement constant utilisant une inférence de croyance relative}

\absSideBySide{Considerable amount of copy number alterations are observed in many tumor samples. As tumor develops chromosomal regions are involved in deletions and amplifications, which are often assumed to follow a locally constant signal model in many developed methods. However weak signals affected by low abnormal tumor fraction and/or subclonal heterogeneous tumor are still hard to be identified.\\
\\
We propose a segmentation method using a relative belief inference on a locally constant model. The performance of the proposed method is presented and compared with several segmentation algorithms including circular binary segmentation, allele-specific piecewise constant fitting and SCAN algorithms.
}{Des quantités considérables de modifications du nombre de copies sont observées dans de nombreux échantillons de tumeurs. Lorsque la tumeur se développe, les régions chromosomiques sont impliquées dans des suppressions et des amplifications. Cela est souvent supposé suivre un modèle de signal localement constant dans de nombreux modèles développés. Cependant, les signaux faibles affectés par une fraction de la tumeur anormalement faible et/ou une tumeur hétérogène subclonale sont encore difficiles à identifier.\\
\\
Nous proposons une méthode de segmentation qui utilise une inférence de croyance relative sur un modèle localement constant. La performance de la méthode proposée est présentée et comparée à plusieurs algorithmes de segmentation, y compris la segmentation binaire circulaire, l'ajustement par morceaux constants d'allèles spécifiques et les algorithmes SCAN.
}}
%% Talk sg-ms
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 14:15]\\}
{\Author{Mateen}{Shaikh}{shaikh}{2D-C4}\Author{Jen}{Stearns}{stearns}{2D-C4}\Author{Michael}{Surette}{surette}{2D-C4}\Author{Sonia}{Anand}{anand}{2D-C4}\Author{Russell}{de Souza}{desouza}{2D-C4}\Author{Joseph}{Beyene}{beyene}{2D-C4}%
\abshead{\absauthor{MATEEN SHAIKH}, \absauthor{JEN STEARNS}, \absauthor{MICHAEL SURETTE}, \absauthor{SONIA ANAND}, \absauthor{RUSSELL DE SOUZA} \& \absauthor{JOSEPH BEYENE}\absaffil{McMaster University}}
        {The Beta Negative Binomial Distribution for the Analysis of Microbiome Data\newline
        La distribution bêta-binomiale négative pour l'analyse des données sur le microbiome}

\absSideBySide{Microbiome data is a high-dimensional count data that has gained considerable attention in recent years, both biologically and statistically. The data is typically noisy and exhibits complex structure. Appropriately modelling these data is important for a variety of applications as human microbiomes in particular, influences and is influenced by the health of the host. In this study, we compare the performance of the beta negative binomial distribution to model microbiome data in comparison with other discrete distributions. We assessed performance of alternative distributions using both simulated data as well as real data generated by studies investigating developmental origins of diseases.
}{Les données sur le microbiome sont des données de dénombrement de grandes dimensions qui ont suscité beaucoup d'intérêt biologique et statistique au cours des dernières années. Les données sont généralement bruyantes et présentent une structure complexe. Modéliser ces données de façon appropriée est important pour de nombreuses applications puisque  le microbiome humain est influencé par – et influence - la santé de l'hôte. Dans cette étude, nous comparons l'efficacité de la distribution bêta-binomiale négative pour modéliser les données sur le microbiome, par rapport à d'autres distributions discrètes. Nous évaluons l'efficacité des distributions alternatives au moyen de données simulées ainsi que de données réelles provenant des études sur les origines développementales de maladies.
}}
%% Talk sg-js
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 14:30]\\}
{\Author{Jihyung}{Shin}{shin}{2D-C4}\Author{Shelley B.}{Bull}{bull}{2D-C4}%
\abshead{\absauthor{JIHYUNG SHIN}\absaffil{Lunenfeld-Tanenbaum Research Institute, Mount Sinai Hospital, University of Toronto}, \absauthor{SHELLEY B. BULL}\absaffil{Lunenfeld-Tanenbaum Research Institute, Mount Sinai Hospital, Dalla Lana School of Public Health, University of Toronto}}
        {On the Analysis of Low-Frequency Genetic Variants in the Presence of Quantitative Covariates Using Prospective and Retrospective Penalized Logistic Regression Models\newline
        À propos de l'analyse de variants génétiques rares en présence de covariables quantitatives utilisant des modèles prospectifs et rétrospectifs par régression logistique pénalisée}

\absSideBySide{In genetic association studies of binary outcomes, standard logistic regression analysis of low-frequency variants can violate large sample assumptions, yielding invalid conclusions about genetic effects. Firth's penalized logistic regression (PLR) is an alternative. 
We evaluate empirical estimation bias, LR test size and power of PLR under two analytic approaches: (i) prospective model with disease outcome as response variable and genotype as predictor; (ii) retrospective model with an additively constrained genotype as response variable and disease outcome as predictor. We recommend prospective PLR for analysis in the presence of a quantitative covariate and suggest that the retrospective model be applied cautiously.
}{Dans les études d'association génétique de phenotypes binaires, les modèle de régression logistique standards qui incluent des variants rares peuvent ne pas être conforme aux hypothèses de grand échantillon, ce qui rend les conclusions sur les effets génétiques invalides. La régression logistique pénalisée de Firth constitue une solution de rechange. 
Nous évaluons le biais d'estimation empirique, la taille du test du ratio de vraisemblance et la puissance de la régression logistique pénalisée dans le cadre de deux approches analytiques : i) un modèle prospectif présentant une maladie comme variable-réponse et un génotype comme prédicteur; ii) un modèle rétrospectif présentant un génotype additivement contraint comme variable-réponse et une maladie comme prédicteur. Nous recommandons l'analyse prospective lorsque la covariable est quantitative, et nous suggérons que le modèle rétrospectif soit appliqué avec prudence.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 2D-C5: Statistical Inference and Applications 1\\Inf\'erence statistique et applications 1}
\begin{center}{\large Chair/Président: Jeffrey Rosenthal (University of Toronto)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 258\end{center}
\label{abs-sid:sia2}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk sia2-mb
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 13:30]\\}
{\Author{Mikelis}{Bickis}{bickis}{2D-C5}%
\abshead{\absauthor{MIKELIS BICKIS}\absaffil{University of Saskatchewan}}
        {Statistical Relativity\newline
        La relativité en statistique}

\absSideBySide{Einstein's theory of relativity is based on the understanding that certain intrinsic properties should remain invariant under changes to the frame of reference.  A statistical model is also described relative to a frame of reference. Meaningful inferences ought not to depend on this often arbitrary frame of reference. Nonetheless, commonly used statistical techniques such as principal components, model selection, and design optimality are affected by reparametrization. While this failing is implicitly acknowledged by practitioners it raises the question about whether such methods can be described in a way that gives meaning to their application
}{La théorie de la relativité d'Einstein est basée sur l'idée que certaines caractéristiques intrinsèques doivent rester invariantes lorsque le système de référence est modifié. Un modèle statistique est également décrit par rapport à un système de référence. Les inférences raisonnables ne doivent pas dépendre d'un système de référence souvent arbitraire.  Néanmoins, des techniques statistiques d'usage courant, comme les composantes principales, la sélection des modèles, et l'optimalité des plans expérimentaux sont affectés par la paramétrisation.  Quoique les praticiens reconnaissent ce défaut, il pose la question à savoir si on peut décrire de telles méthodes d'une façon qui donne un sens à leur application.
}}
%% Talk sia2-eg
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 13:45]\\}
{\Author{Edit}{Gombay}{gombay}{2D-C5}%
\abshead{\absauthor{EDIT GOMBAY}\absaffil{University of Alberta}}
        {Change Detection Algorithms\newline
        Algorithmes de détection du changement}

\absSideBySide{``A contributed session in Honor of Professor Ian MacNeill.''\\
\\
In this talk we give a  brief review of the different type of change detection algorithms.
Various models are considered and the common features and the common underlying theory are explained. Change detection is a field of statistics that is fast growing in importance. Its applications include Economics and Finance, climate change studies, clinical studies, just to name a few major areas.
}{« Une séance libre en l'honneur du professeur Ian MacNeill. »\\
\\
Dans cet exposé, nous donnons un bref aperçu des différents types d'algorithmes de détection du changement.
Différents modèles sont considérés et les caractéristiques et la théorie sous-jacente communes sont expliquées. La détection du changement est un champ de la statistique dont l'importance s'accroît rapidement. Ses applications comprennent l'économie et la finance, les études sur les changements climatiques et les études cliniques pour ne citer que plusieurs grands domaines.
}}
%% Talk sia2-km
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 14:00]\\}
{\Author{Kevin}{Mongeon}{mongeon}{2D-C5}%
\abshead{\absauthor{KEVIN MONGEON}\absaffil{Brock University}}
        {The Process of Winning and Losing Sporting Contests\newline
        Le processus de gagner ou perdre des évènements sportifs}

\absSideBySide{Bayes Rule is used to derive an in-game probabilistic model of winning and losing sporting contests. We use Bayesian hierarchical regression models to estimate the unknown model parameters and iteratively sample the conditional posterior distributions to obtain the posterior expected win probabilities. We decompose the expected in-game win probabilities into their prior and conditional likelihood components to examine team behaviours under various game-state conditions.
}{Le théorème de Bayes est utilisé pour dériver un modèle probabiliste de gagner ou perdre des évènements sportifs lors du déroulement du match. Les paramètres inconnus du modèle sont estimés à partir de modèles de régression hiérarchique de Bayes et les probabilités de gagner escomptées sont obtenues par le biais d'un échantillonnage aléatoire des distributions conditionnelles a posteriori. Les probabilités escomptées de gagner sont décomposées en composantes a posteriori et conditionnelle pour examiner les comportements d'équipe sous diverses conditions lors du déroulement du match.
}}
%% Talk sia2-szs
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 14:15]\\}
{\Author{Shaun Zheng}{Sun}{sun}{2D-C5}%
\abshead{\absauthor{SHAUN ZHENG SUN}\absaffil{University of the Fraser Valley}}
        {Improved Tests for Independence in Ordinal Contingency Tables\newline
        Tests améliorés pour l'indépendance des tableaux de contingence ordinaux}

\absSideBySide{In a contingency table, the standard test for independence among variables is the Pearson's Chi-squared. When the variable(s) are ordinal, such as those in Likert scale survey data, the Chi-squared statistic does not take into account the natural orderings.\\
\\
In this talk, an ordinal sensitive test is proposed and large sample theory is given.  Monte Carlo studies show that the new test is more powerful than some classical tests, including the Chi-squared and the likelihood ratio tests. An example from an education survey will be given. The test can be extended to higher dimensional tables.
}{Dans un tableau de contingence, le test standard d'indépendance entre les variables est le khi-deux de Pearson. Lorsque la(les) variable(s) sont ordinales, telles que celles des données d'enquête selon une échelle de Likert, la statistique du khi-deux ne tient pas compte des ordres naturels.\\
\\
Dans cet exposé, un test ordinal sensible est proposé et la théorie des grands échantillons est donnée. Des études de Monte-Carlo montrent que le nouveau test est plus puissant que certains tests classiques, y compris les tests du khi-deux et des ratios de vraisemblance. Un exemple à partir d'une enquête sur l'éducation sera présenté. Le test peut être étendu à des tableaux de dimensions supérieures.
}}
%% Talk sia2-gju
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 14:30]\\}
{\Author{Gary J.}{Umphrey}{umphrey}{2D-C5}%
\abshead{\absauthor{GARY J. UMPHREY}\absaffil{University of Guelph}}
        {Aesthetics and Measurement: The Measurement of Aesthetics and the Aesthetics of Measurement\newline
        Esthétique et mesure~: la mesure de l'esthétique et l'esthétique de la mesure}

\absSideBySide{An object (such as an artwork, a piece of music, a mineral specimen, or an ant) that is perceived as having aesthetic qualities is referred to here as an ``aesthetic object''. Investigating what properties contribute to the perception of an object as aesthetic and why is often facilitated by taking measurements. But are measurements themselves aesthetic objects?  This talk will explore the idea that measurements – particularly collections of measurements we call data sets – have aesthetic qualities. A question is posed: Does the act of analyzing data cast the statistician in dual roles of data scientist and data artist?
}{Un objet (œuvre d'art, pièce musicale, spécimen minéralogique ou fourmi) perçu comme ayant des qualités esthétiques sera appelé dans le présent contexte « objet esthétique ». L'analyse et l'explication des propriétés d'un objet qui le font percevoir comme esthétique sont souvent simplifiées en faisant appel à des mesures. Ces mesures sont-elles aussi des objets esthétiques? Cet exposé porte sur l'idée que les mesures – en particulier les séries de mesures que nous appelons ensembles de données – ont elles-mêmes des qualités esthétiques. Une question se pose~: Par la simple analyse de données, le statisticien remplit-il à la fois les rôles de scientifique et d'artiste des données?
}}
%% Talk sia2-xw
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 14:45]\\}
{\Author{Xikui}{Wang}{wang}{2D-C5}\Author{You}{Liang}{liang}{2D-C5}\Author{Lysa}{Porth}{porth}{2D-C5}%
\abshead{\absauthor{XIKUI WANG}, \absauthor{YOU LIANG} \& \absauthor{LYSA PORTH}\absaffil{University of Manitoba}}
        {Dynamic Risk Measures and Agricultural Insurance\newline
        Mesures de risque dynamiques et assurance agricole}

\absSideBySide{We discuss important ideas of dynamic risk measures and the use of Markov decision processes to characterize and derive such measures. We also discuss how such measures may be used in the study of agricultural insurance.
}{Nous discutons des idées importantes de mesures de risque dynamiques et de l'utilisation des processus de décision de Markov pour caractériser et dériver de telles mesures. Nous discutons aussi comment ces mesures peuvent être utilisées dans l'étude de l'assurance agricole.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 2E-I1: Adaption and Approximation in Markov Chain Monte Carlo\\Adaptation et approximation pour la m\'ethode de Monte-Carlo par cha\^\i nes de Markov}
\begin{center}{\large Organizer and Chair / Responsable et président:  Aaron Smith (University of Ottawa)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 242\end{center}
\label{abs-sid:aam}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk aam-mb
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 15:30]\\}
{\Author{Mylène}{Bédard}{bedard}{2E-I1}%
\abshead{\absauthor{MYLÈNE BÉDARD}\absaffil{Université de Montréal}}
        {Hierarchical Models: Local Proposal Variances for the RWM-within-Gibbs\newline
        Modèles hiérarchiques: variances instrumentales locales pour le RWM-dans-Gibbs}

\absSideBySide{We study the performance of RWM-within-Gibbs algorithms for sampling from hierarchical models. Using existing scaling analyses, we develop asymptotically optimal tunings for that sampler. This leads to locally optimal proposal variances that depend on the mixing components of the hierarchical model and that correspond to the classical asymptotically optimal acceptance rate of 0.234. Ignoring the local character of the optimal scaling leads to an optimal proposal variance that remains fixed for the duration of the algorithm; the corresponding asymptotically optimal acceptance rate is then lower than 0.234. We provide results for location and scale hierarchies, and illustrate the findings through numerical studies. We compare these local and constant approaches to RWM with diagonal covariance matrix and Adaptive Metropolis samplers.
}{Nous étudions la performance d'un algorithme RWM-dans-Gibbs pour échantillonner de modèles hiérarchiques. En nous basant sur des analyses existantes, nous obtenons des variances instrumentales locales et optimales pour cet algorithme. Celles-ci dépendent du paramètre de mélange dans le modèle hiérarchique, et mènent au taux d'acceptation optimal classique de 0.234. Il est également possible d'obtenir une variance instrumentale optimale et constante pour la durée de l'algorithme; le taux d'acceptation asymptotiquement optimal est alors inférieur à 0.234. Nous fournissons des résultats dans le cadre de modèles hiérarchiques avec paramètres de position et/ou d'échelle, et illustrons nos résultats à l'aide d'études de simulation. Nous comparons les approches locale et constante aux algorithmes RWM (avec matrice de covariance instrumentale diagonale) et Metropolis adaptatif.
}}
%% Talk aam-dj
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 16:00]\\}
{\Author{Daniel}{Jerison}{jerison}{2E-I1}%
\abshead{\absauthor{DANIEL JERISON}\absaffil{Cornell University}}
        {Honest MCMC Convergence Guarantees\newline
        Garanties honnêtes de convergence MCMC}

\absSideBySide{Is MCMC estimation as trustworthy as sampling directly from the target probability distribution (if that were feasible)? Usually not: Monte Carlo standard errors, which purport to measure the uncertainty introduced by the Markov chain, are asymptotically valid but provide no finite-time guarantees. The few nonasymptotic results are difficult to apply in practice. I will discuss new MCMC estimation theorems for Markov chains with a regenerative structure. These theorems give accuracy guarantees of the same type that sampling directly from the target distribution would provide. I will illustrate the results using a Gibbs sampler for a Bayesian hierarchical model.
}{L'estimation MCMC est-elle aussi fiable que le serait un échantillonnage direct de la distribution de probabilité cible (si cela était possible)? Généralement pas: les écarts-types Monte-Carlo, qui prétendent mesurer l'incertitude introduite par la chaîne de Markov, sont asymptotiquement valides mais n'offrent aucune garantie en temps fini. Les quelques résultats non asymptotiques sont difficiles à appliquer en pratique. Je discuterai de nouveaux théorèmes d'estimation MCMC pour les chaînes de Markov avec une structure régénératrice. Ces théorèmes offrent des garanties d'exactitude du même type qu'offrirait un échantillonnage direct de la distribution cible. J'illustrerai les résultats à l'aide d'un échantillonneur de Gibbs pour un modèle hiérarchique bayésien.
}}
%% Talk aam-mv
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 16:30]\\}
{\Author{Matti}{Vihola}{vihola}{2E-I1}%
\abshead{\absauthor{MATTI VIHOLA}\absaffil{University of Jyväskylä}}
        {Unbiased Estimators and Multilevel Monte Carlo\newline
        Estimateurs non biaisés et Monte-Carlo multiniveaux}

\absSideBySide{Multilevel Monte Carlo (MLMC) and recently proposed debiasing schemes are closely related methods which can be applied in scenarios where exact simulation methods are difficult to implement, but biased estimators are easily available. An important example of such a scenario is the inference with continuous-time diffusion processes, where the process is difficult to simulate exactly but time-discretized approximations are available. I will present a new general class of unbiased estimators which admits earlier debiasing schemes as special cases, and new lower variance estimators which behave asymptotically like MLMC, both in terms of variance and cost, under general conditions. This suggests that bias can often be eliminated entirely with arbitrarily small extra cost.
(arXiv:1512.01022)
}{Les méthodes Monte-Carlo multiniveaux (MLMC) et les récentes techniques de réduction du biais sont des méthodes étroitement liées applicables à des scénarios dans lesquels il est difficile de mettre en œuvre une méthode de simulation exacte, mais des estimateurs biaisés sont disponibles. Un bon exemple: une inférence avec des processus de diffusion en temps continu, scénario dans lequel il est difficile de simuler le processus de manière exacte, mais il existe des approximations en temps discrétisé. Je présenterai une nouvelle classe générale d'estimateurs non biaisés qui permet certaines techniques de réduction du biais à titre de cas spéciaux, ainsi que de nouveaux estimateurs à variance réduite qui présentent un comportement asymptotique similaire au MLMC (en ce qui concerne la variance et le coût) dans des conditions générales. Cela suggère que le biais peut souvent être entièrement éliminé pour un très léger surcoût.
(arXiv:1512.01022)
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 2E-I2: Building a Research Career: From New Investigator to Present\\Comment b\^atir une carri\`ere en recherche : de nouveau chercheur \`a aujourd'hui}
\begin{center}{\large Organizer and Chair / Responsable et président:  Martin Lysy (University of Waterloo)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Welch - David Howes\end{center}
\label{abs-sid:brc}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk brc-jr
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 15:30]\\}
{\Author{James}{Ramsay}{ramsay}{2E-I2}%
\abshead{\absauthor{JAMES RAMSAY}\absaffil{McGill University}}
        {The Art of the Awkward Question (and Some Psychology, Too)\newline
        De l'art de la question délicate (et de la psychologie)}

\absSideBySide{Sciences advances by questions, not answers, and a recent biography of James Clerk Maxwell makes this case superbly.   This talk will focus on the awkward  questions that shaped my career, including an early one in the Princeton bookstore that still haunts me.\\
\\
But accolades in science are given for answers, and those who ask awkward questions can expect consequences for their relationships with their colleagues.  And perhaps even for their careers.  One way out is to avoid asking questions, but there are better ways, most of which I have yet to discover.
}{Les sciences avancent grâce aux questions plutôt qu'aux réponses, ainsi que le montre brillamment une récente biographie de James Clerk Maxwell. Cette présentation mettra l'accent sur les questions délicates qui ont influencé ma carrière, dont une – il y a bien longtemps – dans une librairie de Princeton – qui me hante encore.\\
\\
Mais, en sciences, ce sont les réponses qui sont félicitées – et ceux qui posent des questions délicates peuvent souvent s'attendre à des conséquences en ce qui concerne les relations avec leurs collègues, voire leur carrière. Une solution est d'éviter de poser des questions, mais il en existe assurément de meilleures, dont certaines que je n'ai pas encore découvertes.
}}
%% Talk brc-jr2
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 16:00]\\}
{\Author{Jeffrey}{Rosenthal}{rosenthal}{2E-I2}%
\abshead{\absauthor{JEFFREY ROSENTHAL}\absaffil{University of Toronto}}
        {Lessons From a Twisted Career Path\newline
        Leçons d'un chemin de carrière tordu}

\absSideBySide{In this talk, I will reflect upon the unique twists and turns of my own academic career path, which took me from enthusiastic pure-math undergrad, to worried Harvard PhD student, to struggling junior probability professor, to ultimately receiving the COPSS Presidents' Award for outstanding contributions to statistics. I will try to use my story to provide lessons and insights for budding statistical scholars and researchers.
}{Dans cette présentation, je méditerai sur les tours et détours de mon propre chemin de carrière universitaire, de l'étudiant enthousiaste en mathématiques pures, au doctorant inquiet de Harvard, au professeur adjoint  en probabilité en difficulté, au fier récipiendaire du prix COPSS pour mes contributions à la statistique. Je tâcherai de tirer de mon histoire des leçons et des conclusions pour les jeunes scientifiques et chercheurs en statistique d'aujourd'hui.
}}
%% Talk brc-mt
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 16:30]\\}
{\Author{Mary}{Thompson}{thompson}{2E-I2}%
\abshead{\absauthor{MARY THOMPSON}\absaffil{University of Waterloo}}
        {Early Career Research Back Then and Now\newline
        Recherches de début de carrière~: hier et aujourd'hui}

\absSideBySide{In many ways the research culture has changed radically from the early 1970s, when reading was done in the library, papers were handwritten for a typist, and manuscripts were submitted through the post. In fact in some ways it is quite a challenge to find the commonalities between then and now. I will discuss how the environment at the University of Waterloo influenced the course of my research career, following graduate studies at a US university. For a beginning researcher today it is certainly necessary to employ different strategies, but the fundamentals would be the same: follow what is fascinating, and find one's collaborative strengths.
}{À de nombreux égards, la culture de recherche a radicalement changé depuis le début des années 1970, où on lisait en bibliothèque, on rédigeait ses articles à la main avant de les faire dactylographier et on soumettait ses manuscrits par la poste. En fait, il est assez difficile de trouver des points communs entre cette époque et la nôtre. Je discuterai de la façon dont l'environnement de Waterloo a influencé la trajectoire de ma carrière en recherche, après des études de cycle supérieur aux États-Unis. Un jeune chercheur d'aujourd'hui emploiera certainement d'autres stratégies, mais les principes de base n'ont pas changé~: suivez ce qui vous fascine et découvrez vos talents de collaborateur.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 2E-I3: Design and Methodological Issues in Health Studies of Vulnerable Populations\\Probl\`emes de conception et de m\'ethodologie des \'etudes sur la sant\'e des populations vuln\'erables}
\begin{center}{\large Chair/Président: John Petkau (University of British Columbia)\protect\\[5pt]
Organizer/Responsable: Lehana Thabane (McMaster University)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 247\end{center}
\label{abs-sid:dmi}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk dmi-ll
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 15:30]\\}
{\Author{Lisa}{Lix}{lix}{2E-I3}%
\abshead{\absauthor{LISA LIX}\absaffil{Ottawa University of Manitoba}}
        {Methodological Challenges in Aboriginal Health Research\newline
        Problèmes méthodologiques en recherche en santé autochtone}

\absSideBySide{Aboriginal populations in Canada, which include First Nations, Inuit, and Metis people, often have poorer health outcomes and a greater frequency of health risk behaviors than non-Aboriginal populations. Comparative information about the health and healthcare use of Aboriginal and non-Aboriginal populations is therefore important in the design and delivery of healthcare services. Misclassification bias in ascertaining ethnicity, small sample sizes, incomplete data about on-reserve populations, and differential item functioning of self-reported health status measures are challenges that can affect the validity of comparative studies. This talk will describe the opportunities for biostatisticians to contribute to high-quality Aboriginal health research in Canada, by addressing these methodological challenges while remaining sensitive to data ownership and control principles and information privacy legislation.
}{Les populations autochtones du Canada, qui incluent les Premières Nations, les Inuit et les Métis, présentent souvent des résultats de santé moins bons et une plus grande fréquence de comportements à risque que les populations non autochtones. Il est donc important de disposer d'informations comparatives sur la santé et l'utilisation des soins de santé par ces deux catégories de populations pour concevoir et fournir des services de santé adaptés. Le biais d'erreur de classification dans la détermination de l'ethnicité, la petite taille des échantillons, les données incomplètes sur les populations des réserves et le fonctionnement différentiel des items sur l'état de santé auto-déclaré sont autant de défis susceptibles d'affecter la validité des études comparatives. Je décrirai les possibilités pour les biostatisticiens de contribuer à une recherche en santé autochtone de qualité au Canada en relevant ces défis méthodologiques tout en demeurant sensibles aux principes de propriété et de contrôle des données et à la législation sur la protection des renseignements personnels.
}}
%% Talk dmi-rp
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 16:00]\\}
{\Author{Robert W.}{Platt}{platt}{2E-I3}%
\abshead{\absauthor{ROBERT W. PLATT}\absaffil{McGill University}}
        {Randomized Trials in Obstetric Populations\newline
        Essais randomisés en milieu obstétrique}

\absSideBySide{Pregnancy is a vulnerable period, in which both mother and fetus are at risk for benefit and harm. I discuss three problems arising in studies during pregnancy: 1) Treatments can have opposite effects for the fetus and the mother, and some important outcomes are very rare. Composite outcomes are a solution, but raise other problems. 2) Timing during pregnancy of an intervention (e.g., induction of pregnancy) is a common research interest. This raises statistical and logistical challenges; the optimal statistical design can be at odds with feasible care plans. 3) Pregnancy is a highly dynamic process, in which the condition of the mother and fetus can change rapidly. Consideration must be given to timing of recruitment and randomization.
}{La grossesse est une période vulnérable pendant laquelle mère et enfant à naître courent certains risques. Je discuterai de trois problèmes qui se posent dans les études menées pendant la grossesse: 1) Les traitements peuvent avoir des effets opposés pour le fœtus et la mère et certains événements importants sont très rares. Les résultats composites constituent une solution, mais ils soulèvent d'autres problèmes. 2) Le moment d'une intervention pendant la grossesse (p. ex., déclenchement) est un domaine d'intérêt commun en recherche. Ceci soulève des problèmes statistiques et logistiques; le plan statistique optimal ne cadre pas toujours avec un plan de soins réaliste. 3) La grossesse est un processus très dynamique, dans lequel l'état de la mère et du fœtus peut évoluer rapidement. Il faut prendre en considération le moment de recrutement et de randomisation.
}}
%% Talk dmi-lt
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 16:30]\\}
{\Author{Lehana}{Thabane}{thabane}{2E-I3}%
\abshead{\absauthor{LEHANA THABANE}\absaffil{McMaster University}}
        {Planning for the Twilight Years: Why We Need More Statisticians Involved in Trials in the Elderly Living in Long-Term Care Settings\newline
        Planification du crépuscule de la vie: pourquoi il nous faut plus de statisticiens dans les essais menés auprès des personnes âgées vivant en centres de soins de longue durée}

\absSideBySide{Providing optimal healthcare of the elderly living in long-term care (LTC) settings requires deliberate planning and good evidence to support decision-making. Conducting trials in LTC to generate evidence or apply evidence in practice can present many design, implementation, methodological, ethical and analytical challenges. First, the population is quite frail which can present serious challenges; second, the environment is hard to work in because of the overcommitted staff; third, outcome assessment can be challenging; fourth, determining the appropriate unit of randomization, analysis or inference can be complicated. In this presentation, I will use experiences from some of our studies to illustrate the issues and highlight the role of biostatisticians in addressing them.
}{Pour fournir des soins de santé optimaux aux personnes âgées vivant en centres de soins de longue durée (SLD), il faut une planification délibérée et de bonnes données à l'appui de la prise de décisions. Or l'organisation d'essais sur les SLD pour générer des données et la mise en pratique de ces données peut présenter de nombreux défis de conception, de mise en application, d'éthique et d'analyse. Tout d'abord, il s'agit d'une population fragile, ce qui peut poser de graves défis; deuxièmement, l'environnement est difficile à aborder en raison de la surcharge en travail du personnel; troisièmement, l'évaluation des issues peut être difficile; quatrièmement, il peut être compliqué de déterminer l'unité de randomisation, d'analyse ou d'inférence appropriée. Dans cette présentation, j'utiliserai des exemples de nos études pour illustrer les problèmes et souligner le rôle que peuvent jouer les biostatisticiens pour y répondre.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 2E-I4: Inference and Prediction from Distributed Data\\Inf\'erence et pr\'evisions \`a partir de donn\'ees distribu\'ees}
\begin{center}{\large Organizer and Chair / Responsable et président:  Jean-Francois Plante (HEC Montreal)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 258\end{center}
\label{abs-sid:ipd}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk ipd-dc
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 15:30]\\}
{\Author{Doina}{Caragea}{caragea}{2E-I4}\Author{Li}{HongMin}{hongmin}{2E-I4}\Author{Sopova}{Oleksandra}{oleksandra}{2E-I4}\Author{Herndon}{Nic}{nic}{2E-I4}%
\abshead{\absauthor{DOINA CARAGEA}, \absauthor{LI HONGMIN}, \absauthor{SOPOVA OLEKSANDRA} \& \absauthor{HERNDON NIC}\absaffil{Kansas State University}}
        {Learning Domain Adaptation Classifiers from Multiple Distributed Sources\newline
        Classificateurs d'adaptation de domaines d'apprentissage à partir de sources multiples distribuées}

\absSideBySide{For many inference and classification problems, the amount of labeled data available is limited, making it impossible to learn accurate supervised classifiers. To 
address this challenge, we propose a domain adaptation approach that leverages labeled data from multiple distributed source domains to learn classifiers for a 
target domain. Our proposed approach, which is based on expectation maximization and the naïve Bayes classifier, identifies information that needs to be extracted 
from source domains and transferred to the target domain. This information includes relevant features and instances in each source, sufficient statistics to be 
transferred to the target, as well as importance weights for each source. We evaluate our approach in the context of disaster management and response based on 
microblogging data.
}{Pour plusieurs problèmes d'inférence et de classification, la quantité de données étiquetées disponible est limitée, ce qui rend impossible d'apprendre des 
classificateurs supervisés précis. Pour remédier à ce problème, nous proposons une approche d'adaptation de domaine qui tire profit de données étiquetées provenant de 
multiples domaines sources distribués pour apprendre les classificateurs pour un domaine cible. Notre approche, qui se base sur la maximisation de l'espérance et sur 
le classificateur naïf bayésien, identifie l'information qui doit être extraite des domaines sources et transférée au domaine cible. Cette information inclut 
les caractéristiques et les cas pertinents de chaque source, les statistiques exhaustives à être transférées à la cible, ainsi que des pondérations importantes de 
chaque source. Nous évaluons notre approche dans le contexte de la gestion et de la réponse en cas de catastrophe sur la base de données de microblogging.
}}
%% Talk ipd-go
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 16:00]\\}
{\Author{George}{Ostrouchov}{ostrouchov}{2E-I4}%
\abshead{\absauthor{GEORGE OSTROUCHOV}\absaffil{Oak Ridge National Laboratory \& University of Tennessee}}
        {Statistical Computing with R on Distributed and Multicore Platforms\newline
        Calcul statistique avec R sur des plateformes distribuées et multicœur}

\absSideBySide{The Programming with Big Data in R (pbdR) project (see pbdr.org) aims to teach and simplify parallel programming in R and to harness parallel numerical libraries relevant to statistical computing. In this talk, I will give a very brief overview of parallel computing platforms and introduce the pbdR project. I will give two examples. One, where parallelism is introduced directly in R and another, where parallel numerical linear algebra libraries are engaged.
}{Le projet de programmation avec mégadonnées dans le logiciel R (voir pbdr.org) vise à enseigner et à simplifier la programmation parallèle dans R ainsi qu'à exploiter les bibliothèques numériques pertinentes pour les calculs statistiques. Dans cet exposé, je donnerai un aperçu des plateformes de parallélisme et je présenterai le projet de programmation avec des mégadonnées dans R. Je donnerai deux exemples : l'un où le parallélisme est effectué directement dans R et l'autre où les bibliothèques numériques d'algèbre linéaire sont utilisées.
}}
%% Talk ipd-rx
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 16:30]\\}
{\Author{Ruibin}{Xi}{xi}{2E-I4}\Author{Nan}{Lin}{lin}{2E-I4}%
\abshead{\absauthor{RUIBIN XI}\absaffil{Peking University}, \absauthor{NAN LIN}\absaffil{Washington University in St. Louis}}
        {Statistical Aggregation for Big Data Analysis\newline
        Agrégation statistique pour l'analyse de données volumineuses}

\absSideBySide{Big data provides great new opportunities for many fields. Deep analysis of big data may reveal important unknown phenomena. At the same time, big data also presents new statistical and computational challenges. In this talk, I will discuss the statistical aggregation technique that we developed in the last few years to perform statistical analysis in big data setting. This technique is based on the divide-and-conquer strategy. We first divide the big data to smaller subsets, perform statistical analysis on each subsets and aggregate the analysis results of each subsets to get a solution for the entire data set. Distributed computation of the resulted estimators is straightforward. We also show that the new estimators are statistically equivalent to the original estimator.
}{Les données volumineuses offrent de nouvelles possibilités pour de nombreux domaines. Une analyse en profondeur de ces données pourrait révéler un phénomène important inconnu. De plus, les données volumineuses présentent également des défis statistiques et informatiques. Dans cet exposé, je parlerai de la technique d'agrégation que nous avons mise au point au cours des dernières années pour effectuer une analyse statistique dans un contexte de données volumineuses. Cette technique est basée sur la stratégie de diviser pour régner. Nous divisons tout d'abord les données volumineuses en sous-ensembles plus petits. Ensuite, nous effectuons une analyse statistique de chaque sous-ensemble et nous agrégeons les résultats de l'analyse pour chaque sous-ensemble afin d'obtenir une solution du jeu entier des données. Le calcul distribué des estimateurs qui en résultent est direct. Nous démontrons également que les nouveaux estimateurs sont statistiquement équivalents à l'estimateur original.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 2E-I5: Statistical Challenges in Large-Scale Inference\\D\'efis statistiques en inf\'erence \`a grande \'echelle}
\begin{center}{\large Organizer and Chair / Responsable et président:  Kun Liang (University of Waterloo)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 246\end{center}
\label{abs-sid:scl}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk scl-cz
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 15:30]\\}
{\Author{Chunming}{Zhang}{zhang}{2E-I5}%
\abshead{\absauthor{CHUNMING ZHANG}\absaffil{University of Wisconsin-Madison}}
        {Single-Index Modulated Multiple Testing\newline
        Tests multiples modulés à indice unique}

\absSideBySide{We present a SIM multiple testing procedure, by assuming the availability of a bivariate p-value, $(p_1,p_2)$, for each hypothesis. To find the optimal rejection 
region for the bivariate p-value, we propose a criteria based on the ratio of probability density functions of $(p_1,p_2)$ under the true null and nonnull. This 
criteria in the bivariate normal setting motivates us to project the bivariate p-value to a single-index, $p(\theta)$, for a wide range of directions $\theta$. The 
true null distribution of $p(\theta)$ is estimated via parametric and nonparametric approaches. To derive the optimal projection direction $\theta$, we propose a 
new approach, which is shown to be consistent under some mild conditions. Simulation evaluations and analysis of a real dataset will be illustrated.
}{Nous présentons une procédure de tests multiples SIM en supposant que la valeur p bidimensionnelle, $(p_1,p_2)$, est disponible pour chaque hypothèse. Pour trouver la région de rejet 
optimale pour la valeur p bidimensionnelle, nous proposons un critère basé sur le rapport des fonctions de fréquences de $(p_1,p_2)$ sous la vraie nulle et 
non nulle. Ce critère dans le cadre normal bidimensionnel nous motive à projeter la valeur p bidimensionnelle sur un indice unique, $p(\theta)$, pour un large éventail 
de directions $\theta$. La vraie loi nulle de $p(\theta)$ est estimée à l'aide d'approches paramétriques et non paramétriques. Pour dériver la direction de projection  
optimale $\theta$, nous proposons une nouvelle approche, qui est convergente sous de légères conditions. Des études de simulation et l'analyse d'un ensemble de données 
réelles seront illustrées.
}}
%% Talk scl-ws
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 16:00]\\}
{\Author{Wenguang}{Sun}{sun}{2E-I5}\Author{Tony}{Cai}{cai}{2E-I5}\Author{Weinan}{Wang}{wang}{2E-I5}%
\abshead{\absauthor{WENGUANG SUN}\absaffil{University of Southern California}, \absauthor{TONY CAI}\absaffil{University of Pennsylvania}, \absauthor{WEINAN WANG}\absaffil{University of Southern California}}
        {CARS: Covariate Assisted Ranking and Screening for Large-Scale Two-Sample Inference\newline
        CARS: Classement et triage assistés de covariables pour inférence à deux échantillons à grande échelle}

\absSideBySide{The conventional two-sample inference framework often leads to suboptimal multiple testing procedures due to the loss of information in the data reduction step. This 
article studies how to construct an auxiliary variable from the original data matrix and incorporate the variable in the inference procedure to improve the power 
in multiple testing. We study the problem in a decision-theoretic framework and develop oracle and data-driven procedures for false discovery rate (FDR) control. 
The proposed oracle procedure employs a covariate-assisted ranking and screening (CARS) scheme, which is shown to be optimal in the sense that it has the smallest 
missed discovery rate among all valid FDR procedures. We also develop a data-driven procedure and establish its asymptotic properties.
}{Le cadre conventionnel de l'inférence à deux échantillons mène souvent à des procédures de tests multiples sous-optimales en raison de la perte d'information à l'étape 
de la réduction de données. Cet article étudie la façon de construire une variable auxiliaire à partir de la matrice de données originales et incorpore la variable dans 
la procédure d'inférence pour améliorer la puissance dans les tests multiples. Nous étudions le problème dans un cadre de décision théorique et nous développons une 
procédure oracle ainsi qu'une procédure fondée sur les données pour le contrôle du taux de fausses découvertes (FDR). La méthode oracle proposée utilise un schéma de 
classement et de triage assistés de covariables (CARS) qui s'avère être optimal en ce sens qu'il a le plus petit taux de fausses découvertes parmi toutes les 
procédures valides FDR. Nous développons aussi une procédure fondée sur les données et établissons ses propriétés asymptotiques.
}}
%% Talk scl-rh
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 16:30]\\}
{\Author{Ruth}{Heller}{heller}{2E-I5}\Author{Marina}{Bogomolov}{bogomolov}{2E-I5}\Author{Daniel}{Yekutieli}{yekutieli}{2E-I5}%
\abshead{\absauthor{RUTH HELLER}\absaffil{Tel-Aviv University}, \absauthor{MARINA BOGOMOLOV}\absaffil{Technion}, \absauthor{DANIEL YEKUTIELI}\absaffil{Tel-Aviv University}}
        {Assessing Replicability Across High-Dimensional Studies\newline
        Évaluer la reproductibilité entre les études à grande dimension}

\absSideBySide{There is need in modern applications for procedures to identify features with two (or more) findings. For example, in GWAS, there is interest in generalization, where the aim is to identify the SNPs that are associated with a phenotype in more than one population,  as well as in identifying the loci with pleiotropic effects. 
We propose two formal statistical approaches for this purpose. In the empirical Bayes approach, we estimate the optimal rejection rule for discovering replicated findings. In the frequentist approach, we first select the promising features for each type of finding (e.g., study) separately, and then we test for replicability only the features that were selected for both types of findings (e.g., studies).
}{Il existe un besoin dans les applications modernes pour des procédures qui identifient les caractéristiques dans deux résultats (ou plus). Par exemple, dans GWAS, il y a un intérêt dans la généralisation lorsque le but est d'identifier les SNP associés à un phénotype dans plus d'une population, ainsi que dans l'identification des locus avec des effets pléiotropes.\\
\\
À cet effet, nous proposons deux approches statistiques formelles. Dans l'approche empirique de Bayes, nous estimons la règle de rejet optimal pour découvrir les résultats répliqués. Dans l'approche fréquentiste, nous choisissons d'abord les caractéristiques prometteuses pour chaque type de résultat (par exemple, étude) séparément, puis nous testons pour la reproductibilité uniquement les caractéristiques qui ont été sélectionnées pour les deux types de résultats (par exemple, études).
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 2E-I6: Survey Methodology in a Big Data World\\M\'ethodes d'enqu\^ete dans un monde de m\'egadonn\'ees}
\begin{center}{\large Organizer and Chair / Responsable et présidente:  Nancy Reid (University of Toronto)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Academic South 217\end{center}
\label{abs-sid:smb}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk smb-adm
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 15:30]\\}
{\Author{Anne}{de Moliner}{demoliner}{2E-I6}\Author{Hervé}{Cardot}{cardot}{2E-I6}\Author{Camelia}{Goga}{goga}{2E-I6}%
\abshead{\absauthor{ANNE DE MOLINER}\absaffil{Université de Montréal}, \absauthor{HERVÉ CARDOT} \& \absauthor{CAMELIA GOGA}\absaffil{Université de Bourgogne}}
        {Estimating a Mean Electricity Consumption Curve in the Presence of Missing Data Using Survey Sampling Techniques\newline
        Estimation de courbe moyenne de consommation électrique par sondage en présence de valeurs manquantes}

\absSideBySide{In the near future, millions of electricity load curves of French households measured at a fine scale will be available. All these collected load curves represent a huge amount of information difficult to store due to technical and budgetary constraints. In these situations, survey sampling techniques are attractive alternatives to signal compression to estimate mean consumption curves for different groups of clients.
Unfortunately, data collection may undergo technical problems resulting in missing values. This problem reduces the accuracy of the estimators and may generate bias. Different approaches can be adapted to deal with missing data in this functional framework: nearest neighbor imputation, kernel smoothing or linear interpolation to the differences around the mean.  We compare these methods on real data.
}{Dans un futur proche, des millions de courbes de consommation électrique de ménages français, mesurées à un pas de temps fin seront disponibles. Ces données constitueront une masse d'information considérable qui pourrait être exploitée grâce à des techniques d'échantillonnage pour estimer des courbes de consommation moyennes à différents périmètres. Malheureusement, des aléas techniques pourraient survenir, générant des valeurs manquantes qui risqueraient de détériorer la précision des estimateurs, voire de créer des biais. Différentes méthodes d'estimation en présence de valeurs manquantes pourraient être adaptées à notre contexte de données fonctionnelles : l'imputation par le plus proche voisin, le lissage par noyau ou encore l'interpolation linéaire des différences à la moyenne. Nous comparons ces approches sur des données réelles.
}}
%% Talk smb-je
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 16:15]\\}
{\Author{John}{Eltinge}{eltinge}{2E-I6}%
\abshead{\absauthor{JOHN ELTINGE}\absaffil{U.S. Bureau of Labor Statistics}}
        {Characterization, Modeling and Management of Data Quality, Risk and Cost  in the Integration of Surveys with Alternative Data Sources\newline
        Caractérisation, modélisation et gestion de la qualité, du risque et du coût de données dans l'intégration des enquêtes avec sources de données alternatives}

\absSideBySide{Government statistical agencies have missions centered on providing the public with high-quality information on a sustainable and cost-effective basis.  This 
requires the agencies to use practical design tools that allow them to characterize, model and manage multiple dimensions of data quality, risk and cost that are 
inherent in statistical production processes.  Historically, those tools have centered on sample surveys and some administrative record systems.  However, agencies 
now need to extend these tools to cover a broad suite of alternative data sources (sometimes called ``big data,'' ``non-designed data'' or ``organic data'').  This paper 
develops a framework for such tools, with special emphasis on (1) the level of precision required for a given design decision; and (2) use of available empirical 
information.
}{Les organismes de statistique gouvernementaux ont pour mission de fournir au public de l'information de haute qualité sur une base durable et rentable. Ceci exige que 
les organismes utilisent des outils pratiques qui leur permettent de caractériser, de modéliser et de gérer plusieurs dimensions de qualité, de risque et de coût de 
données qui sont inhérentes aux processus de production de statistiques. Historiquement, ces outils étaient centrés sur les enquêtes par sondage et sur quelques systèmes 
de dossiers administratifs. Par contre, ces organismes ont maintenant besoin d'élargir ces outils pour couvrir un large éventail de sources de données alternatives 
(quelques fois appelées «données volumineuses», «données non-planifiées» ou «données organiques»). Cet article développe un cadre pour de tels outils, avec un accent 
particulier sur (1) le niveau de précision requis pour une décision de conception donnée; et (2) l'utilisation de l'information empirique disponible.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 2E-C1: Biostatistics:  New Inferential Strategies\\Biostatistique : nouvelles strat\'egies d'inf\'erence}
\begin{center}{\large Chair/Président: Karen Kopciuk (University of Calgary)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 255\end{center}
\label{abs-sid:bni}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk bni-cg
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 15:30]\\}
{\Author{Christopher}{Gravel}{gravel}{2E-C1}\Author{Robert W.}{Platt}{platt}{2E-C1}%
\abshead{\absauthor{CHRISTOPHER GRAVEL} \& \absauthor{ROBERT W. PLATT}\absaffil{McGill University}}
        {Inverse Probability Weighted Estimation in Confounded Binary Data with Outcome Misclassification\newline
        Estimation pondérée selon la probabilité inverse pour les données binaires confondues avec classification erronée du résultat}

\absSideBySide{We consider a study with a binary exposure and outcome, subject to confounding and misclassification of the outcome variable.  We use inverse probability weighting and internal validation sampling to rebalance covariates across treatment groups while mitigating misclassification bias.  We discuss several validation sampling schemes and a Monte Carlo approach to approximate optimal sample size determination.  A parametric bootstrap is used for variance estimation. We explore finite sample properties of the weighted estimators via simulation, with particular attention to relative efficiency of different sampling schemes for validation. We demonstrate the use of the methods through an example using administrative data.
}{Nous considérons une étude avec une exposition et un résultat binaires, sous réserve de confusion et de classification erronée de la variable de résultat. Nous utilisons la pondération selon la probabilité inverse et un échantillonnage interne de validation pour rééquilibrer les covariables entre les groupes de traitement tout en atténuant le biais attribuable à la classification erronée. Nous discutons de plusieurs plans d'échantillonnage de validation et d'une approche de Monte-Carlo pour approximer la détermination de la taille d'échantillon optimale. Un bootstrap paramétrique est utilisé pour l'estimation de la variance. Nous explorons les propriétés des échantillons finis des estimateurs pondérés à l'aide de simulations, avec une attention particulière pour l'efficacité relative des différents plans d'échantillonnage pour la validation. Nous démontrons l'utilisation des méthodes à l'aide d'un exemple qui utilise des données administratives.
}}
%% Talk bni-qs
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 15:45]\\}
{\Author{Qiuguang}{Sang}{sang}{2E-C1}\Author{Guohua}{Yan}{yan}{2E-C1}\Author{Renjun}{Ma}{ma}{2E-C1}%
\abshead{\absauthor{QIUGUANG SANG}, \absauthor{GUOHUA YAN} \& \absauthor{RENJUN MA}\absaffil{University of New Brunswick}}
        {Bayesian Compound Poisson Mixed Models for Longitudinal Semi-Continuous Data with Non-Ignorable Missingness\newline
        Modèles composés mixtes bayésiens de Poisson pour des données longitudinales semi-continues manquantes de façon non-ignorable}

\absSideBySide{Longitudinal semi-continuous data, which usually contain a fair amount of zeros, is common in biomedical, econometric and health studies. The presence of missing values creates additional difficulty in data analysis. In this talk, we propose a Tweedie compound Poisson mixed model to incorporate positive observations and zeros in an integral way. We use a selection model for missing data. A simulation study is carried out to evaluate the performance of the proposed method. The method is also illustrated through the reanalysis of a fluoride intake dataset.
}{Les données longitudinales semi-continues, qui contiennent généralement un bon nombre de zéros, sont courantes dans les études biomédicales, économétriques et sur la santé. La présence de valeurs manquantes pose des difficultés supplémentaires dans l'analyse de données. Dans cet exposé, nous proposons un modèle composé mixte de Poisson de Tweedie afin d'incorporer de façon intégrative des observations positives et des zéros. Nous utilisons un modèle de sélection pour données manquantes. Nous effectuons une étude de simulation pour évaluer l'efficacité de la méthode proposée et l'illustrons par une nouvelle analyse d'un jeu de données sur l'apport en fluorure.
}}
%% Talk bni-fs
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 16:00]\\}
{\Author{Farhad}{Shokoohi}{shokoohi}{2E-C1}\Author{Abbas}{Khalili}{khalili}{2E-C1}\Author{Masoud}{Asgharian}{asgharian}{2E-C1}\Author{Shili}{Lin}{lin}{2E-C1}%
\abshead{\absauthor{FARHAD SHOKOOHI}, \absauthor{ABBAS KHALILI} \& \absauthor{MASOUD ASGHARIAN}\absaffil{McGill University}, \absauthor{SHILI LIN}\absaffil{The Ohio State University}}
        {Feature Selection in High-Dimensional Heterogeneous Time-to-Event Data; A Study on Ovarian Cancer\newline
        Sélection de traits pour des données de temps d'événement hétérogènes et de grande dimension; étude sur le cancer de l'ovaire}

\absSideBySide{Ovarian cancer is among the leading causes of cancer deaths. DNA methylation may be part of a genetic signature for survival time. It is of interest to study how recurrence time is related to methylations of gene promoters. A study on relationship between genes and recurrence time of ovarian cancer is carried out. The observations are subject to right censoring and indicate signs of heterogeneity. We consider a finite mixture of accelerated failure time model. We apply screening and penalization methods. Properties of the proposed method are studied theoretically and via simulation. The results of data analysis are discussed.
}{Le cancer de l'ovaire est parmi les causes principales de décès par cancer. La méthylation de l'ADN peut faire partie d'une signature génétique pour le temps de survie. Il est pertinent d'étudier comment la récurrence de la maladie est reliée à la méthylation des promoteurs de gènes. Nous avons mené une étude sur la relation entre les gènes et le temps de récurrence du cancer de l'ovaire. Les observations sont censurées à droite et présentent des signes d'hétérogénéité. Nous considérons un mélange fini du modèle de temps de défaillance accéléré et nous appliquons des méthodes de dépistage et de pénalisation. Les propriétés de la méthode proposée sont étudiées de façon théorique ainsi que par des simulations. Nous présentons aussi les résultats de l'analyse de données.
}}
%% Talk bni-mz
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 16:15]\\}
{\Author{Meng}{Zhao}{zhao}{2E-C1}\Author{Russell}{Steele}{steele}{2E-C1}\Author{Ian}{Shrier}{shrier}{2E-C1}%
\abshead{\absauthor{MENG ZHAO} \& \absauthor{RUSSELL STEELE}\absaffil{McGill University}, \absauthor{IAN SHRIER}\absaffil{Lady Davis Institute, Jewish General Hospital and McGill University}}
        {Application of the Trend-Renewal Process to Recurrent Sports Injury\newline
        Application du processus de tendance-renouvellement (PTR) aux blessures sportives récurrentes}

\absSideBySide{The two classical frameworks in intensity-based modelling of recurrent events are the renewal process and the Poisson process, respectively describing ``perfect repair'' and ``minimal repair'' after each failure. When modelling recurrent sports injury, these two extremes are often inadequate. The trend-renewal process, developed by B.H. Lindqvist, is a mathematically elegant compromise between the two and includes them as special cases. Until recently, it has only been applied in reliability. We discuss a new practical interpretation of the parameters in a parametric TRP and evaluate its finite sample inference, focusing on its relevance in modelling recurrent sports injury.
}{Les deux cadres classiques de la modélisation basée sur l'intensité des événements récurrents sont le processus de renouvellement et le processus de Poisson, chacun décrivant respectivement la « réparation intégrale » et la « réparation minimale » après chaque défaillance. Ces deux extrêmes sont souvent inadéquats pour la modélisation des blessures sportives récurrentes. Mis au point par B.H. Lindqvist, le processus de tendance-renouvellement constitue un élégant compromis mathématique entre les deux et les inclut comme cas particuliers. Jusqu'à tout récemment, il n'a été appliqué qu'en matière de fiabilité. Nous traitons d'une nouvelle interprétation pratique des paramètres dans un PTR paramétrique et en évaluons l'inférence en échantillon fini, mettant l'accent sur sa pertinence pour la modélisation des blessures sportives récurrentes.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 2E-C2: Probability and Estimation\\Probabilit\'e et estimation}
\begin{center}{\large Chair/Président: Fernando Camacho (Damos Inc.)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 257\end{center}
\label{abs-sid:pe}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk pe-sn
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 15:30]\\}
{\Author{Shanoja}{Naik}{naik}{2E-C2}\Author{Taehan}{Bae}{bae}{2E-C2}%
\abshead{\absauthor{SHANOJA NAIK} \& \absauthor{TAEHAN BAE}\absaffil{University of Regina}}
        {On Convolution of Inverse Gaussian and Gamma Distribution and Auto Regressive Model\newline
        A propos de la convolution des lois inverse gaussienne et Gamma pour le modèle autorégressif}

\absSideBySide{This paper investigate a new two parameter inverse Gaussian Gamma distribution obtain by convoluting an Inverse Gaussian and Gamma random variable. The goal of the article is to provide characteristics of the new model, to obtain certain statistical function related to the model, to estimate the parameters and obtain the auto regressive models of order one (AR1). The new distribution is promising fit for data set with positive support, compared to some well known distributions like Inverse Gaussian, Gamma ect. The applicability of the model has established for real-world data on US/Canada disasters.
}{Nous étudions une nouvelle loi inverse gaussienne Gamma à deux paramètres, obtenue par la convolution d'une variable aléatoire inverse gaussienne et d'une variable aléatoire Gamma. L'objectif de cet exposé est d'établir les caractéristiques du nouveau modèle, et d'obtenir une certaine fonction statistique liée au modèle, afin d'estimer les paramètres et obtenir des modèles autorégressifs d'ordre 1 (AR1). La nouvelle loi est prometteuse pour modéliser des jeux de données avec un support positif, par rapport à certaines lois bien connues, comme la loi inverse gaussienne, la loi Gamma, etc. L'applicabilité du modèle a été établie pour des données réelles sur les catastrophes aux États-Unis et au Canada.
}}
%% Talk pe-sp
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 15:45]\\}
{\Author{Steve}{Puglia}{puglia}{2E-C2}\Author{Xu}{Xiaojian}{xiaojian}{2E-C2}%
\abshead{\absauthor{STEVE PUGLIA} \& \absauthor{XU XIAOJIAN}\absaffil{Brock University}}
        {D-optimal Design of Linear Measurement Error Models\newline
        Système optimal D de modèles linéaires de mesure d'erreur}

\absSideBySide{The design of any experiment impacts the precision of model parameter estimates, and future predictions. Utilizing a D-optimal design ensures that the joint confidence regions for true model parameters will be as small as possible for a fixed sample size. Moreover, measurement error is present in the majority of models, and should be considered in the design stage of an experiment.
Considering a simple linear model with measurement error in the response, we have investigated the properties of exact and approximate D-optimal designs, which appear to be two-point balanced designs. When measurement error is symmetric, so is the D-optimal design.
}{La planification de toute expérience a des répercussions sur la précision de l'estimation des paramètres du modèle et sur les prédictions futures. L'utilisation d'un système optimal D permet aux régions de confiance conjointes des paramètres du modèle d'être aussi petites que possible, pour une taille d'échantillon fixe. De plus, l'erreur de mesure est présente dans la plupart des modèles et elle devrait être considérée dans l'étape de planification.
Dans le cadre d'un modèle linéaire simple avec une erreur de mesure dans la réponse, nous avons  étudié les propriétés des systèmes D optimaux exacts et approximatifs, qui se présentent comme étant des systèmes à deux points équilibrés. Lorsque l'erreur de mesure est asymétrique, le système optimal D l'est également.
}}
%% Talk pe-jv
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 16:00]\\}
{\Author{Jean}{Vaillancourt}{vaillancourt}{2E-C2}\Author{Bruno}{Rémillard}{remillard}{2E-C2}%
\abshead{\absauthor{JEAN VAILLANCOURT}\absaffil{École Nationale d'Administration Publique}, \absauthor{BRUNO RÉMILLARD}\absaffil{HEC Montréal}}
        {Loser Takes All!\newline
        Qui perd gagne!}

\absSideBySide{In this talk, we show how one can combine losing games of chance into a winning one. It illustrates the famous phenomenon known as Parrondo's Paradox. This phenomenon is extended to random combinations (or regime switchings) of simple random walks in random environments. The paradoxical behaviour of the resulting stochastic process can be explained through considerations regarding the random environment. This is joint work with Bruno Rémillard.
}{Dans cet exposé, on montre que l'on peut combiner des jeux de hasard perdants en un jeu gagnant. C'est une illustration du célèbre phénomène connu sous le vocable de paradoxe de Parrondo. Ce phénomène est étendu à des combinaisons aléatoires (ou changements de régime) de marches aléatoires simples dans des environnements aléatoires. Le comportement paradoxal du processus stochastique résultant est expliqué par l'effet de l'environnement aléatoire. Ce travail est conjoint avec Bruno Rémillard.
}}
%% Talk pe-mh
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 16:15]\\}
{\Author{Myron}{Hlynka}{hlynka}{2E-C2}\Author{Zishan}{Huang}{huang}{2E-C2}%
\abshead{\absauthor{MYRON HLYNKA} \& \absauthor{ZISHAN HUANG}\absaffil{University of Windsor}}
        {Probabilistic Laplace Transform Inversion\newline
        Transformée inverse probabiliste de Laplace}

\absSideBySide{We use the probabilistic interpretation of Laplace transforms
to create a technique to invert Laplace transforms. The method
requires the solution of a quadratic programming problem.
}{Nous utilisons l'interprétation probabiliste des transformées de Laplace pour créer une technique afin d'inverser la transformation de Laplace. La méthode
nécessite la résolution d'un problème de programmation quadratique.
}}
%% Talk pe-chl
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 16:30]\\}
{\Author{Chel Hee}{Lee}{lee}{2E-C2}\Author{Mikelis}{Bickis}{bickis}{2E-C2}%
\abshead{\absauthor{CHEL HEE LEE} \& \absauthor{MIKELIS BICKIS}\absaffil{University of Saskatchewan}}
        {Geometrical View of Imprecise Prior for Imprecise Inference\newline
        Vision géométrique des lois a priori imprécises pour l'inférence imprécise}

\absSideBySide{We are concerned with the problem of constructing an imprecise inferential framework. The concept of imprecise probabilities introduced by Walley (1991) is adopted.  A conjugate family of priors is formulated in the form of three-parameter exponential family. Two examples with Binomial and Poisson samplings models are provided to show that the advantage of standard conjugate family (i.e. Bayesian updating) still holds. Geometrically, it can also be viewed as translation on the hyperparameter space.  We also discuss general strategies of characterizing imprecise prior under the state of near or complete prior ignorance and show stochastic independence and focusing behaviors.
}{Nous sommes préoccupés par le problème de construction d'un cadre d'inférence imprécise. Nous adoptons le concept des probabilités imprécises présenté par Walley (1991). Nous formons une famille de lois a priori conjuguées sous la forme d'une famille exponentielle à trois paramètres. Nous présentons deux exemples de modèles d'échantillonnage binomial et de Poisson afin de démontrer que la famille conjuguée standard (à savoir, l'actualisation bayésienne) continue de procurer un avantage. Sur le plan géométrique, cette famille peut aussi être considérée comme une translation sur l'espace des hyperparamètres. Nous discutons également des stratégies générales de caractérisation des lois a priori imprécises lorsqu'on ne connaît pas ou presque pas l'information a priori, et nous montrons l'indépendance stochastique ainsi que certains comportements spécifiques.
}}
%% Talk pe-dl
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 16:45]\\}
{\Author{Deli}{Li}{li}{2E-C2}\Author{Han-Ying}{Liang}{liang}{2E-C2}\Author{Andrew}{Rosalsky}{rosalsky}{2E-C2}%
\abshead{\absauthor{DELI LI}\absaffil{Lakehead University}, \absauthor{HAN-YING LIANG}\absaffil{Tongji University, China}, \absauthor{ANDREW ROSALSKY}\absaffil{University of Florida, USA}}
        {A Note on the Symmetrization Procedures for the Laws of Large Numbers\newline
        Remarque sur les procédures de symétrisation des lois des grands nombres}

\absSideBySide{In this talk, general symmetrization procedures for both the weak and strong laws of large numbers concerning the row sums from arrays of rowwise independent Banach space valued random variables are established. Necessary and sufficient conditions are provided for the weak and strong laws of large numbers to hold in terms of corresponding laws of large numbers for a symmetrized version of the array. Several corollaries of the main result are provided.
}{Dans cet exposé, nous établissons des procédures de symétrisation des lois faibles et fortes des grands nombres concernant la somme des lignes de tableaux de variables aléatoires indépendantes d'espace de Banach. Nous présentons les conditions nécessaires et suffisantes des lois faibles et fortes des grands nombres selon les lois correspondantes des grands nombres pour une version symétrisée du tableau. Nous présentons plusieurs corollaires du résultat principal.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 2E-C3: Regression Models\\Mod\`eles de r\'egression}
\begin{center}{\large Chair/Président: Juli Atherton (UQAM)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 254\end{center}
\label{abs-sid:rm}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk rm-ma
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 15:30]\\}
{\Author{Mohamed}{Amezziane}{amezziane}{2E-C3}\Author{Syed Ejaz}{Ahmed}{ahmed}{2E-C3}%
\abshead{\absauthor{MOHAMED AMEZZIANE}\absaffil{Central Michigan University}, \absauthor{SYED EJAZ AHMED}\absaffil{Brock University}}
        {Variable Selection via Individual Shrinkage of Regression Coefficients\newline
        Sélection de variable par le rétrécissement individuel des coefficients de régression}

\absSideBySide{The proposed estimator is obtained by independently shrinking the parameters of some regression estimator towards zero. The mean square error of the estimator is 
minimized with respect to the shrinkage coefficients to simultaneously select relevant variables and estimate their associated regression coefficients. This results 
in a soft threshold estimator similar to the non-negative garrote, but does not employ a regularization parameter. The asymptotic properties of the estimator are 
derived and its performance is compared to that of penalized regression estimators through Monte Carlo simulation experiments.
}{L'estimateur proposé est obtenu en rétrécissant indépendamment les paramètres d'un estimateur de régression vers zéro. L'erreur carrée moyenne de l'estimateur est 
minimisée par rapport aux coefficients de rétrécissement pour sélectionner simultanément des variables pertinentes et pour estimer leurs coefficients de régression 
correspondants. Le résultat est un estimateur à seuil faible similaire au garrot non négatif mais qui n'utilise pas un paramètre de régularisation. Les propriétés 
asymptotiques de l'estimateur sont dérivées et sa performance est comparée à celle des estimateurs de régression pénalisés à l'aide d'expériences de simulation Monte 
Carlo.
}}
%% Talk rm-mg
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 15:45]\\}
{\Author{Mai}{Ghannam}{ghannam}{2E-C3}\Author{Sévérien}{Nkurunziza}{nkurunziza}{2E-C3}%
\abshead{\absauthor{MAI GHANNAM}\absaffil{University of Windsor}, \absauthor{SÉVÉRIEN NKURUNZIZA}\absaffil{University of Windsor/Université de Sherbrooke}}
        {Some Stein Rules Methods in Tensor Regression Models with High and Ultrahigh Dimensional Data\newline
        Certaines règles de Stein dans les modèles de régression tensorielle avec des données de dimensions supérieures}

\absSideBySide{In this talk, we consider an estimation problem in tensor regression models with
high and ultrahigh dimensional data, when the target parameters may or may not satisfy some restrictions. In particular, we construct James Stein-type estimators for the target parameter. Further, we derive the asymptotic distributional risk of the proposed estimators and prove that shrinkage estimators dominate the unrestricted estimator. The proposed method has some interesting applications as for example in neuroimaging data analysis.
}{Dans cette présentation, on s'intéresse au problème d'estimation dans les modèles de régression tensorielle avec des données de dimensions supérieures et ultra-supérieures, lorsque les paramètres d'intérêt sont susceptibles de satisfaire certaines restrictions. En particulier, nous construisons les estimateurs de type James-Stein pour le paramètre en question. Par ailleurs, nous élaborons le risque distributionnel asymptotique des estimateurs proposés et montrons que les estimateurs à rétrécissement dominent l'estimateur sans restriction. La méthode proposée a des applications intéressantes comme par exemple dans l'analyse des données de neuroimagerie.
}}
%% Talk rm-bk
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 16:00]\\}
{\Author{Bashir}{Khan}{khan}{2E-C3}\Author{A.K.Md. Ehsanes}{Saleh}{saleh}{2E-C3}%
\abshead{\absauthor{BASHIR KHAN}\absaffil{Saint Mary's University}, \absauthor{A.K.MD. EHSANES SALEH}\absaffil{Carleton University}}
        {Estimation in a Regression Model with Autocorrelated Errors\newline
        Estimation dans un modèle de régression avec erreurs autocorrélées}

\absSideBySide{We consider the model
$ {\bf y} =  {\bf X} {\bf B} + {\bf e}$
where,
${\bf X} = \left(X_1 , X_2 \right)$,
 ${\bf B} = ({\bf \beta_1^{'}}, {\bf \beta_2^{'}})$, and ${\bf e }$ satisfy the first-order stationary autocorrelated model.
We assume that $\bf e$ is normal with mean $\bf 0$
and covariance ${\sigma_z^2} { \Sigma}_{\rho},$ where ${ \Sigma}_{\rho}$ is the correlation matrix. 
We propose four estimators for ${\bf \beta_1}$ when it is suspected that $\bf \beta_2 = {\bf 0}.$  The performances of the estimators are compared through quadratic bias and risk criteria.
}{Nous considérons le modèle
$ {\bf y} =  {\bf X} {\bf B} + {\bf e}$
où
${\bf X} = \left(X_1 , X_2 \right)$,
 ${\bf B} = ({\bf \beta_1^{'}}, {\bf \beta_2^{'}})$, et ${\bf e }$ satisfont le modèle autocorrélé stationnaire de premier ordre.
Nous supposons que $\bf e$ est normal avec moyenne $\bf 0$
et covariance ${\sigma_z^2} { \Sigma}_{\rho},$ où ${ \Sigma}_{\rho}$ est la matrice de corrélation. 
Nous proposons quatre estimateurs pour ${\bf \beta_1}$ lorsque l'on soupçonne que $\bf \beta_2 = {\bf 0}.$ La performance des estimateurs est comparée à l'aide des critères du biais quadratique et du risque.
}}
%% Talk rm-sk
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 16:15]\\}
{\Author{Shahedul}{Khan}{khan}{2E-C3}%
\abshead{\absauthor{SHAHEDUL KHAN}\absaffil{University of Saskatchewan}}
        {Exponentiated Weibull Regression Model for Time-to-Event Data\newline
        Modèle de régression de Weibull exponentié pour des données de temps d'événement}

\absSideBySide{The exponentiated Weibull distribution has demonstrated considerable potential to model time-to-event data. It is a generalization of the Weibull distribution, and can be used to model data exhibiting increasing, decreasing, unimodal or bathtub-shaped hazard function. However, the main focus of many studies is to understand the relationship between lifetime and covariates. This leads to a consideration of regression models. In this study, we develop regression methodology based on the exponentiated Weibull distribution. Analyses of four data sets and a simulation study reveal that the exponentiated Weibull regression can be valuable in adequately describing different types of time-to-event data.
}{La distribution de Weibull exponentiée a démontré un potentiel considérable pour modéliser des données de temps d'événement. Il s'agit d'une généralisation de la distribution de Weibull et elle peut être utilisée pour modéliser des données présentant une fonction de défaillance croissante, décroissante, unimodale ou en forme de baignoire. Cependant, l'objectif principal de plusieurs études est de comprendre la relation entre la durée de vie et les covariables. Cela conduit à une examen des modèles de régression. Dans cette étude, nous développons une méthode de régression qui s'appuie sur la distribution de Weibull exponentiée. Les analyses des quatre jeux de données et une étude de simulation montrent que la régression de Weibull exponentiée peut être utile pour décrire adéquatement différents types de données de temps d'événement.
}}
%% Talk rm-khw
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 16:45]\\}
{\Author{Ka Ho}{Wu}{wu}{2E-C3}%
\abshead{\absauthor{KA HO WU}\absaffil{The Chinese University of Hong Kong}}
        {Error Model Fitting in Benchmarking\newline
        Ajustement du modèle d'erreur à l'étalonnage}

\absSideBySide{Two sources of data with different precisions and collecting frequencies may be available for a target socio-economic variable. Benchmarking is a process which uses less frequent and more reliable data to adjust more frequent and less reliable data. The regression method of benchmarking may lead to better results than widely used numerical methods. By properly choosing an AR(1) model as the ``working model'' for the error of monthly data, the regression method may work well in most real situations. In this paper, some new AR(1)-modelling procedures via inside-data-period benchmark forecasts are proposed. The performance of various modelling procedures is compared.
}{La disponibilité de deux sources de données comportant différentes précisions et fréquences de collecte est possible pour une variable socioéconomique cible. L'étalonnage est un processus qui fait appel à des données de moindre fréquence et de plus grande fiabilité pour ajuster des données plus fréquentes mais moins sûres. La méthode de régression de l'étalonnage peut produire de meilleurs résultats que les méthodes numériques largement utilisées. En choisissant judicieusement un processus AR(1) comme « modèle fonctionnel » pour l'erreur de données mensuelles, la méthode de régression peut fonctionner adéquatement dans la plupart des situations réelles. Cet article propose quelques nouveaux processus de modélisation AR(1) à l'aide de prévisions pour une période de référence de collecte interne de données. Les performances de diverses procédures de modélisation sont aussi comparées.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 2E-C4: Statistical Applications and Methods\\M\'ethodes et applications statistiques}
\begin{center}{\large Chair/Président: Duncan Murdoch (University of Western Ontario)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 256\end{center}
\label{abs-sid:ba2}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk ba2-bh
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 15:30]\\}
{\Author{Baktiar}{Hasan}{hasan}{2E-C4}\Author{Cong}{Chen}{chen}{2E-C4}\Author{Laurence}{Collette}{collette}{2E-C4}\Author{Xanthi}{Pedeli}{pedeli}{2E-C4}\Author{Yue}{Shentu}{shentu}{2E-C4}\Author{Jessica}{Menis}{menis}{2E-C4}\Author{Urania}{Dafni}{dafni}{2E-C4}%
\abshead{\absauthor{BAKTIAR HASAN}\absaffil{European Organisation for Research and Treatment of Cancer (EORTC), Brussels, Belgium}, \absauthor{CONG CHEN}\absaffil{Merck \& Co., USA}, \absauthor{LAURENCE COLLETTE}\absaffil{European Organisation for Research and Treatment of Cancer (EORTC), Brussels, Belgium}, \absauthor{XANTHI PEDELI}\absaffil{Frontier Science Foundation - Hellas, Athens, Greece}, \absauthor{YUE SHENTU}\absaffil{Merck  \& Co., USA}, \absauthor{JESSICA MENIS}\absaffil{European Organisation for Research and Treatment of Cancer (EORTC), Brussels, Belgium}, \absauthor{URANIA DAFNI}\absaffil{Frontier Science Foundation - Hellas, Athens, Greece.}}
        {Adaptive Study Design: Dealing with Differential Treatment Effect across Subgroups, a Case Study from an EORTC-ETOP Randomized, Phase III Trial in Early Stage Non-Small Cell Lung Cancer (NSCLC)\newline
        Plan d'étude adapté à un effet différentiel de traitement selon une classification en sous-groupes: une phase III de EORTC-ETOP en exemple}

\absSideBySide{Previous studies suggested that Pembrolizumab, a checkpoint inhibitor agent, might work better in patients expressing higher PD-L1. Nevertheless, patients with low/no PD-L1 expression may still benefit from treatment. The challenge is to incorporate the level of PD-L1 expression [negative(-): 0 staining, positive(+): $1-49\%$ staining, strong positive(++): $\ge 50\%$ staining] into a phase III study design in adjuvant NSCLC.
Taking into account potential heterogeneity of treatment effect across subgroups, the co-primary endpoints of disease-free survival (DFS) in the whole population and DFS in the ++ subgroup are used.  Study population enrichment is allowed by a futility interim analysis for the – subgroup.
}{Des essais récents suggèrent que le Pembrolizumab, un inhibiteur de point de contrôle, pourrait être plus efficace contre les cancers du poumon à petites cellules qui expriment fortement le PD-L1 (PDL1++ : $ \ge 50\%$ de cellules expriment le PDL1) que contre ceux qui ne l'expriment pas (PDL1-) ou peu (PDL1+: $1-49\%$ de cellules positives). Un design de phase III avec deux critères principaux conjoints est utilisé pour intégrer ces hypothèses: la survie sans maladie pour la population complète et pour la sous-population PD-L1++. L'enrichissement de la population en cours d'étude est opéré via une analyse intermédiaire de futilité dans le sous-groupe PD-L1-.
}}
%% Talk ba2-mm
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 15:45]\\}
{\Author{Michael}{McIsaac}{mcisaac}{2E-C4}%
\abshead{\absauthor{MICHAEL MCISAAC}\absaffil{Queen's University}}
        {Overcoming Missing Data in Physical Activity Studies Involving Accelerometers\newline
        Surmonter le problème des données manquantes dans les études d'activité physique impliquant des accéléromètres}

\absSideBySide{Accelerometers are an increasingly important measure of physical activity in studies of healthy lifestyles, such as our Active Play Study. However, accelerometers sometimes go unworn. Typical approaches involving deleting or averaging over non-wear periods can undermine study findings, particularly since nonwear can coincide with participation in organized sport.
This talk will discuss employing modern missing data methodology and using self-report journals to make missing-at-random assumptions more plausible in this important setting. Focus will be on multiple imputation and the unique challenges arising in these incomplete high-dimensional count data that are to be analyzed at the aggregate level.
}{Les accéléromètres sont de plus en plus utilisés pour mesurer l'activité physique dans les études sur les modes de vie sains, tels que notre étude sur les jeux actifs. Cependant, les accéléromètres ne sont pas toujours portés par les sujets. Des approches classiques qui impliquent la suppression ou le calcul de moyennes sur des périodes où les accéléromètres ne sont pas portés, peuvent  nuire aux résultats des études, notamment lorsque le fait de ne pas porter les accéléromètres coïncide avec la participation à un sport organisé.
Dans cet exposé, nous discuterons de l'emploi de méthodes modernes permettant de traiter les données manquantes, ainsi que de l'utilisation de registres auto-déclarés qui permettent de rendre les hypothèses de « manquant entièrement dû au hasard » plus plausibles dans ce contexte important. Nous mettrons l'accent sur l'imputation multiple et sur les défis uniques liés à ces données incomplètes de dénombrement à grande dimension qui doivent être analysées au niveau agrégé.
}}
%% Talk ba2-kcs
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 16:00]\\}
{\Author{Khokan C.}{Sikdar}{sikdar}{2E-C4}\Author{Georgiy}{Bobashev}{bobashev}{2E-C4}\Author{Hude}{Quan}{quan}{2E-C4}\Author{Danielle}{Southern}{southern}{2E-C4}\Author{Matthew Thomas}{James}{james}{2E-C4}%
\abshead{\absauthor{KHOKAN C. SIKDAR}\absaffil{University of Calgary}, \absauthor{GEORGIY BOBASHEV}\absaffil{RTI International}, \absauthor{HUDE QUAN}, \absauthor{DANIELLE SOUTHERN} \& \absauthor{MATTHEW THOMAS JAMES}\absaffil{University of Calgary}}
        {Shifting Epidemiologic Paradigm from Population-level to Individual-level: Cardiovascular Mortality and Morbidity in Hypertensive Patients\newline
        Déplacer le paradigme épidémiologique du niveau de la population au niveau individuel : mortalité et morbidité cardiovasculaires chez des patients hypertendus}

\absSideBySide{Population-based epidemiological studies provide evidence of risk factors for disease at a population level. Although focusing on a specific individual is not of interest, inferences from population-based research findings are often used as reference to provide patient-specific care. This is an issue in predicting individual-level outcomes. We propose to use various predictive models, including logistic and survival, to outline the advances as well as newer fundamental findings as an improvement for within-individual epidemiology. We will model cardiovascular outcomes using data from a hypertension cohort. This work may provide a direction for future improvement in big data analytics for precision medicine.
}{Les études épidémiologiques basées sur la population montrent l'existence de facteurs de risques de maladie à l'échelle de la population. Bien qu'il soit futile de se concentrer sur un individu spécifique, les inférences issues des résultats de recherche basés sur la population servent souvent de références pour fournir des soins spécifiques à un patient. Ceci pose un problème pour la prédiction de résultats à l'échelle individuelle. Nous proposons l'utilisation de divers modèles de prédiction, y compris des modèles logistiques et de survie, afin de mettre en évidence les avancées ainsi que les résultats fondamentaux les plus récents, soulignant l'amélioration épidémiologique intra-sujet. Nous modéliserons des données de maladies cardiovasculaires provenant d'une étude de cohorte sur l'hypertension. Cette étude pourrait fournir une orientation pour des améliorations futures dans les analyses de mégadonnées pour une médecine de précision.
}}
%% Talk ba2-js
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 16:15]\\}
{\Author{Jenna}{Sykes}{sykes}{2E-C4}\Author{Sanja}{Stanojevic}{stanojevic}{2E-C4}\Author{Christopher H.}{Goss}{goss}{2E-C4}\Author{Bradley S.}{Quon}{quon}{2E-C4}\Author{Anne L.}{Stephenson}{stephenson}{2E-C4}%
\abshead{\absauthor{JENNA SYKES}\absaffil{St. Michael's Hospital}, \absauthor{SANJA STANOJEVIC}\absaffil{The Hospital for Sick Children}, \absauthor{CHRISTOPHER H. GOSS}\absaffil{University of Washington}, \absauthor{BRADLEY S. QUON}\absaffil{University of British Columbia}, \absauthor{ANNE L. STEPHENSON}\absaffil{St. Michael's Hospital}}
        {A Standardized Approach to Estimating Survival Statistics for Population-Based Registry Cohorts\newline
        Approche standardisée pour estimer les statistiques de survie de cohortes de registres basées sur des populations}

\absSideBySide{It is often of interest to compare disease epidemiology among countries; however, direct comparison of independently-produced registry reports can be challenging due to differences in data processing and statistical methodology. Simulating data from the Canadian Cystic Fibrosis registry, the impact of various registry pitfalls (loss to follow-up, missing death dates) on the estimation of the median age of survival was quantified. The median age of survival was calculated and compared using the common life-table and Cox proportional-hazards methods. We illustrate these effects in comparing Canada to the US and present guidelines to help standardize reporting of disease survival outcomes.
}{Il est souvent pertinent de comparer l'épidémiologie des maladies entre les pays. Cependant, la comparaison directe de rapports de registres produits de façon indépendante peut présenter des défis en raison des différences dans le traitement des données et de la méthodologie statistique. En simulant des données à partir du registre canadien de la fibrose kystique, nous avons pu quantifier les répercussions de différent problèmes reliés aux registres (perte de suivis et dates de décès manquantes) sur l'estimation de l'âge médian de survie. L'âge médian de survie a été calculé et comparé au moyen de la table de survie classique et des modèles de risques proportionnels de Cox. Nous illustrons ces effets en comparant le Canada et les États-Unis et présentons des lignes directrices pour aider à standardiser la communication des résultats de survie à la maladie.
}}
%% Talk ba2-ed
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 16:30]\\}
{\Author{Ernest}{Dankwa}{dankwa}{2E-C4}\Author{Taraneh}{Abarin}{abarin}{2E-C4}\Author{Hong}{Wang}{wang}{2E-C4}%
\abshead{\absauthor{ERNEST DANKWA}, \absauthor{TARANEH ABARIN} \& \absauthor{HONG WANG}\absaffil{Memorial University}}
        {Bayesian Approach to a Linear Regression model with Measurement Error in Covariate\newline
        Approche bayésienne pour un modèle de régression linéaire avec erreur de mesure dans la covariable}

\absSideBySide{This research presents a Bayesian perspective to measurement error problems. We consider a linear regression model with one continuous predictor that is subject to 
measurement error and additional precisely measured covariates.
Common designs used in the presence of measurement errors such as validation and replication designs are discussed in this work. We conduct simulation studies to 
demonstrate how the Bayesian approach accounts for measurement error in explanatory variables using Markov Chain Monte Carlo methods.\\
\\
Keywords: Bayesian approach, Linear regression, Monte Carlo, Measurement
error.
}{Cette recherche présente une perspective bayésienne face aux problèmes des erreurs de mesure. Nous considérons un modèle de régression linéaire avec une variable 
indépendante continue qui est sujette à des erreurs de mesure et avec des covariables supplémentaires mesurées précisément.
Les modèles courants utilisés en présence d'erreurs de mesure tels que les modèles de validation et de réplication sont abordés dans cette étude. Nous réalisons des 
études de simulation pour démontrer la façon dont l'approche bayésienne tient compte des erreurs de mesure dans les variables explicatives en utilisant des méthodes Monte-Carlo par chaînes de Markov.\\
\\
Mots-clés: Approche bayésienne, régression linéaire, Monte-Carlo, erreur de mesure.
}}
%% Talk ba2-yk
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 16:45]\\}
{\Author{Yisub}{Kye}{kye}{2E-C4}\Author{Edward}{Furman}{furman}{2E-C4}%
\abshead{\absauthor{YISUB KYE} \& \absauthor{EDWARD FURMAN}\absaffil{York University}}
        {Economic Capital Analysis within a Portfolio of Dependent and Heavy-tailed Risks\newline
        Analyse du capital économique d'un portefeuille de risques dépendants et à ailes lourdes}

\absSideBySide{The problem of evaluating the economic capital (EC) for the aggregate
risk of a portfolio as well as of consequently allocating the aforementioned
EC to risk sources is fundamental in many areas of actuarial science, be it
risk management or pricing. In this talk, I will discuss a complete solution 
to this problem within a portfolio of Paretian risks with exchangeable and
non-exchangeable underlying copulas. As the EC allocation rule, I will 
use some popular members of the class of weighted EC allocation functionals.\\
\\
This is a joint work with Ed Furman of York University, Toronto, Canada.
}{Le problème de l'évaluation du capital économique (CE) pour le risque agrégé d'un portefeuille ainsi que, par conséquent, l'allocation du CE mentionné précédemment aux sources de risque, est fondamental dans de nombreux domaines de la science actuarielle, que ce soit en gestion ou en tarification des risques. Dans cet exposé, je discuterai d'une solution complète à ce problème dans un portefeuille de risques parétiens avec copules sous-jacentes échangeable et non-échangeables. Comme règle de répartition du CE, j'utiliserai certains membres populaires de la classe des allocations fonctionnelles pondérées du CE.\\
\\
Ce travail est réalisé conjointement avec Ed Furman de l'Université York, Toronto, Canada.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 2E-C5: Statistical Inference and Applications 2\\Inf\'erence statistique et applications 2}
\begin{center}{\large Chair/Président: Ali Karimnezhad (University of Ottawa)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 259\end{center}
\label{abs-sid:sia3}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk sia3-jdt
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 15:30]\\}
{\Author{Jules}{de Tibeiro}{detibeiro}{2E-C5}\Author{Luigi}{D'Ambra}{dambra}{2E-C5}\Author{Antonello}{D'Ambra}{dambra}{2E-C5}%
\abshead{\absauthor{JULES DE TIBEIRO}\absaffil{Université de Moncton}, \absauthor{LUIGI D'AMBRA}\absaffil{Dipartimento di Economia, Management Istituzioni, University of Naples, Italy.}, \absauthor{ANTONELLO D'AMBRA}\absaffil{Dipartimento di Economia, Second University of Naples, Italy}}
        {Different Constrained Ordination Methods for Contingency Table with External Information\newline
        Différentes méthodes d'ordination sous contraintes pour des tableaux de contingence à variables instrumentales}

\absSideBySide{We study the information contained in dataset K = (X, Z) using the statistical method named Correspondence Analysis. A descriptive approach is applied on the whole data set before we partition the dataset into two parts: the first part is data subset X for which we focus on the explanation and prediction; the second part is the data subset Z which consists of additional qualitative variables. We also perform Non-Symmetrical Correspondence Analysis, Canonical Correspondence Analysis, and Canonical Non-Symmetrical Correspondence Analysis on K. We predict  expenses of goods and services knowing the global amount and the profile of a new subject.
}{L'analyse des correspondances trouve toute sa place lors de la phase exploratoire du tableau K = (X, Z). Une approche descriptive est appliquée sur l'ensemble des données avant de partitionner ce tableau en deux parties : la première partie X est un sous-tableau analysé pour expliquer et prédire; la deuxième partie Z est un sous-tableau à variables qualitatives additionnelles. 
    Nous effectuons sur K, une analyse non symétrique des correspondances, une analyse canonique des correspondances et une analyse canonique des correspondances non symétriques.  Nous voulons enfin prévoir les dépenses des biens et des services, le montant global et le profil du nouveau sujet.
}}
%% Talk sia3-ok
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 15:45]\\}
{\Author{Othmane}{Kortbi}{kortbi}{2E-C5}\Author{Aziz}{L'Moudden}{lmoudden}{2E-C5}\Author{Éric}{Marchand}{marchand}{2E-C5}\Author{William E.}{Strawderman}{strawderman}{2E-C5}%
\abshead{\absauthor{OTHMANE KORTBI}\absaffil{UAE University}, \absauthor{AZIZ L'MOUDDEN} \& \absauthor{ÉRIC MARCHAND}\absaffil{Université de Sherbrooke}, \absauthor{WILLIAM E. STRAWDERMAN}\absaffil{Rutgers University}}
        {On Predictive Density Estimation with Constraints under Kullback-Leibler Loss\newline
        Estimation de densité prédictive avec des contraintes sous la perte Kullback-Leibler}

\absSideBySide{We consider the estimation of a predictive density for $Y \sim Gamma( \alpha_{2}, \beta)$, based on $X \sim Gamma( \alpha_{1}, \beta)$, with   $\beta \in C = (a, b)$. 
We obtain representations for Bayes and the MRE predictive densities in the unconstrained problem. We show that the generalized Bayes against the truncation of the non-
informative prior onto $C$ dominates the MRE predictive density and is
minimax whenever $a = 0$ or $b = \infty$. 
Comparisons of plug-in 
$Gamma( \alpha_{2}, \hat{\beta})$, which include the predictive mle density, are obtained with results applying
as well for point estimation under dual entropy loss.
}{Nous considérons l'estimation d'une densité prédictive pour $Y \sim Gamma( \alpha_{2}, \beta)$ basée sur $X \sim Gamma( \alpha_{1}, \beta)$, avec $\beta \in C = (a, b)$. 
Nous obtenons des représentations des densités prédictives de Bayes et MRE pour un problème sans contraintes. Nous montrons que le Bayes généralisé contre la troncation de la non-information a priori sur $C$ domine la densité prédictive MRE et est un minimax lorsque $a = 0$ ou $b = \infty$. 
Nous comparons les plugs-in $Gamma( \alpha_{2}, \hat{\beta})$, qui incluent la densité prédictive de l'EMV, obtenus avec des résultats appliqués également à l'estimation ponctuelle sous la double perte d'entropie.
}}
%% Talk sia3-cl
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 16:00]\\}
{\Author{Chantal}{Larose}{larose}{2E-C5}\Author{Ofer}{Harel}{harel}{2E-C5}\Author{Katarzyna}{Kordas}{kordas}{2E-C5}\Author{Dipak}{Dey}{dey}{2E-C5}%
\abshead{\absauthor{CHANTAL LAROSE}\absaffil{State University of New York at New Paltz}, \absauthor{OFER HAREL}\absaffil{University of Connecticut}, \absauthor{KATARZYNA KORDAS}\absaffil{University of Bristol}, \absauthor{DIPAK DEY}\absaffil{University of Connecticut}}
        {An Entropy-Based Method for Choosing a Number of Classes for Incomplete Data LCA\newline
        Méthode fondée sur l'entropie pour choisir le nombre de structures dans l'analyse de structures latentes}

\absSideBySide{The current method for performing latent class analysis on incomplete data requires the number of classes to be specified before imputation of missing values. We present an entropy-based model selection criterion that extends incomplete data LCA to consider more than one number of classes. The performance of our new criterion is demonstrated in a simulation study, and its effect on the analysis of a family studies data set is presented.
}{La méthode actuelle pour réaliser une analyse de structures latentes sur des données incomplètes nécessite la spécification du nombre de structures avant l'imputation des valeurs manquantes. Nous présentons un critère de sélection de modèle fondé sur l'entropie qui généralise l'analyse de structures latentes pour données incomplètes et qui considère plus qu'un nombre de classes. Nous démontrons l'efficacité de notre nouveau critère dans une étude de simulation et présentons son effet sur l'analyse d'un jeu de données sur une étude de familles.
}}
%% Talk sia3-tm
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 16:15]\\}
{\Author{Tiago}{Magalhães}{magalhaes}{2E-C5}\Author{Denise}{Botter}{botter}{2E-C5}\Author{Monica}{Sandoval}{sandoval}{2E-C5}%
\abshead{\absauthor{TIAGO MAGALHÃES}, \absauthor{DENISE BOTTER} \& \absauthor{MONICA SANDOVAL}\absaffil{University of São Paulo}}
        {A Wald-type Test in Proper Dispersion Models\newline
        Test de type Wald dans les modèles de dispersion propre}

\absSideBySide{The proper dispersion models (PDM) contain several important non-exponential models, for instance the von Mises regression model for data distributed along the unit circle and the simplex model for data distributed in the standard unit interval $(0, 1)$. In this work, we find the second-order covariance matrix of the bias-corrected maximum likelihood estimator of the regression parameter $\beta$ in PDM. Based on the obtained second-order covariance matrix, we modify the Wald test, improving its performance. We evaluate the result by using a Monte Carlo simulation and apply the expression of the modified Wald test to a real data set.
}{Les modèles de dispersion propre contiennent plusieurs modèles importants non exponentiels, comme le modèle de régression de von Mises pour des données distribuées le long du cercle d'unité, et le modèle simplexe pour les données distribuées dans l'intervalle unitaire standard $(0, 1)$. Dans ces travaux, nous trouvons la matrice de covariance de second ordre de l'estimateur du maximum de vraisemblance corrigé pour le biais du paramètre de régression $\beta$ dans les modèles de régression propre. En se basant sur cette matrice estimée, nous modifions le test de Wald et améliorons son efficacité. Nous évaluons le résultat au moyen d'une étude de simulation de Monte-Carlo et appliquons l'expression du test de Wald modifié à un jeu de données réelles.
}}
%% Talk sia3-fm
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 16:30]\\}
{\Author{Fahimeh}{Moradi}{moradi}{2E-C5}\Author{Ali}{Karimnezhad}{karimnezhad}{2E-C5}%
\abshead{\absauthor{FAHIMEH MORADI} \& \absauthor{ALI KARIMNEZHAD}\absaffil{University of Ottawa}}
        {Bayesian Parameter Learning in Diagnostic Networks\newline
        Apprentissage de paramètres bayésiens dans les réseaux de diagnostiques}

\absSideBySide{In most studies in the literature, parameter learning in diagnostic networks is conducted through maximizing the posterior probability function, but the resulting maximum a posteriori estimator, as a Bayes estimator under the zero-one loss function, gives no credit to over/under-learning. To take the effect of over/under-learning into account, we suggest using the Bayes estimator under the entropy loss function. Addressing a real world problem, we conduct a simulation study to measure performance of the proposed estimator. Further, we carry out a sensitivity analysis w.r.t. hyperparameters of a chosen prior to examine effects of changing the hyperparameters on the learning procedure.
}{Dans la plupart des études dans la littérature, l'apprentissage de paramètres dans des réseaux de diagnostiques est réalisé en maximisant la fonction de probabilité a posteriori. Cependant, l'estimateur du maximum a posteriori qui en résulte, tel que l'estimateur bayésien basé sur la fonction de perte de zéro à un, ne prend pas en considération le sur-apprentissage ni le sous-apprentissage. Afin de prendre en compte ces effets de sur-apprentissage et de sous-apprentissage, nous suggérons d'utiliser l'estimateur bayésien basé sur la fonction de perte entropique. Dans le cadre d'un problème concret, nous menons une étude de simulation afin de mesurer l'efficacité de l'estimateur proposé. De plus, nous effectuons une analyse de sensibilité des hyperparamètres d'une loi a priori choisie afin d'examiner les effets de changement de ces hyperparamètres sur la procédure d'apprentissage.
}}
%% Talk sia3-cx
\def\whenwhere{[Tuesday May 31 / mardi 31 mai, 16:45]\\}
{\Author{Chen}{Xu}{xu}{2E-C5}\Author{Shaobo}{Lin}{lin}{2E-C5}\Author{Jian}{Fang}{fang}{2E-C5}\Author{Runze}{Li}{li}{2E-C5}%
\abshead{\absauthor{CHEN XU}\absaffil{University of Ottawa}, \absauthor{SHAOBO LIN} \& \absauthor{JIAN FANG}\absaffil{Xi'an Jiaotong University}, \absauthor{RUNZE LI}\absaffil{The Pennsylvania State University}}
        {Prediction-Based Termination Rule for Greedy Learning with Massive Data\newline
        Règle de terminaison fondée sur la prévision pour l'apprentissage glouton avec des données volumineuses}

\absSideBySide{In massive-data analysis, the orthogonal greedy algorithm (OGA) has been revitalized as an efficient  method for accurate prediction. To implement OGA, an appropriate termination is crucial. In this talk, I introduce a new termination rule for OGA via investigating its predictive performance. The proposed rule is conceptually simple and convenient for implementation, which suggests an $O(\sqrt{n/\log n})$ number of essential updates in an OGA process. It therefore provides an appealing route to conduct efficient learning for massive-data. Under mild conditions, the proposed method is strongly consistent with an $O(\sqrt{\log n/n})$ convergence rate to the oracle prediction.
}{Dans l'analyse de données volumineuses, l'algorithme glouton orthogonal (OGA) a été réactualisé comme méthode efficace de prévision exacte. Pour mettre à l'œuvre un OGA, une terminaison appropriée est essentielle.  Dans cet exposé, je présente une nouvelle règle de terminaison pour OGA en examinant sa performance prévisionnelle. La règle proposée est d'une conceptualisation simple et pratique pour sa mise en œuvre, ce qui suggère un nombre $O(\sqrt{n/\log n})$ de mises à jour essentielles dans un processus OGA. Par conséquent, elle est d'un grand intérêt pour offrir un apprentissage efficace en matière de données volumineuses. Sous des hypothèses faibles, la méthode proposée est fortement convergente avec un taux de convergence $O(\sqrt{\log n/n})$ à la prévision oracle.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 3A-A1: CRM-SSC Prize in Statistics Address\\Allocution du r\'ecipiendaire du Prix CRM-SSC en statistique}
\begin{center}{\large Chair/Président: Jose Garrido (Concordia University)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 325\end{center}
\label{abs-sid:cps}
\begin{center}{\large\bfseries Abstract/R\'esum\'e}
\end{center}
%% Talk cps-rc
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 8:45]\\}
{\Author{Radu V.}{Craiu}{craiu}{3A-A1}%
\abshead{\absauthor{RADU V. CRAIU}\absaffil{University of Toronto}}
        {Design Strategies for Adaptive MCMC\newline
        Stratégies de conception pour le MCMC adaptatif}

\absSideBySide{Markov chain Monte Carlo (MCMC) algorithms  have become essential for modern statistical computation. However, the proper tuning of MCMC samplers can be a complicated affair. An Adaptive MCMC (AMCMC) algorithm  is designed to gain information about the target from  the very samples it produces and to use it in order to optimize the simulation parameters on the fly. This approach, while effective, can be challenging to validate theoretically. We will discuss  and illustrate with examples recent theoretical results that simplify the validation task and make the design of adaptive strategies considerably easier.
}{Les algorithmes de Monte-Carlo par chaînes de Markov (MCMC) sont devenus des outils essentiels pour le calcul statistique moderne. Cependant, l'ajustement des échantillonneurs MCMC peut s'avérer compliqué. Un algorithme de MCMC adaptatif permet d'obtenir des informations sur la cible à partir des échantillons mêmes qu'il produit et d'utiliser ces informations afin d'optimiser les paramètres de simulation à la volée. Cette approche, bien qu'efficace, peut être difficile à valider en théorie. Nous discutons et illustrons par des exemples de récents résultats théoriques qui simplifient la tâche de validation et facilitent nettement la conception de stratégies adaptatives.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 3A-A2: SSC Impact Award Session\\Allocution du r\'ecipiendaire du Prix pour impact de la SSC}
\begin{center}{\large Chair/Président: Rick Routledge (Simon Fraser University)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Welch - David Howes\end{center}
\label{abs-sid:sia}
\begin{center}{\large\bfseries Abstract/R\'esum\'e}
\end{center}
%% Talk sia-sb
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 8:45]\\}
{\Author{Shelley B.}{Bull}{bull}{3A-A2}%
\abshead{\absauthor{SHELLEY B. BULL}\absaffil{University of Toronto}}
        {Statistical Issues and Opportunities in Molecular and Genetic Epidemiology\newline
        Problèmes et possibilités statistiques en épidémiologie moléculaire et génétique}

\absSideBySide{Molecular and genetic association studies conducted in well-characterized longitudinal cohorts offer a powerful approach to investigate factors influencing disease course or complex trait expression. As measurement technologies continue to develop and evolve, studies based on existing cohorts raise methodological questions that I will illustrate in two long-term inter-disciplinary collaborations. In one, we are investigating molecular genetic prognostic factors in the natural history of node-negative breast cancer using a combination of hypothesis-testing and hypothesis-generating molecular approaches. In the other, we aim to identify genes for multiple traits in participants of a therapeutic RCT in type 1 diabetes using genome-wide genetic analysis of extended follow-up data. I will close with brief comments on emerging challenges for insight into biological mechanisms.
}{Les études d'associations moléculaires et génétiques menées sur des cohortes longitudinales bien caractérisées permettent une étude approfondie des facteurs qui influencent l'évolution de la maladie ou l'expression de caractères complexes. Alors que les technologies de mesure continuent de progresser et d'évoluer, les études fondées sur des cohortes existantes soulèvent des questions méthodologiques que j'illustrerai dans le cadre de deux collaborations interdisciplinaires à long terme. Dans l'une, nous étudions les facteurs pronostiques génétiques moléculaires dans l'histoire naturelle du cancer du sein sans atteinte des ganglions à l'aide d'une combinaison d'approches moléculaires de génération et de vérification d'hypothèses. Dans l'autre, nous tentons d'identifier les gènes qui contrôlent plusieurs caractères chez les participants à un ERC thérapeutique sur le diabète de type 1 via une analyse génétique sur l'ensemble du génome de données de suivi prolongé. Je conclurai par quelques mots sur les nouveaux défis que présentent l'étude des mécanismes biologiques.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 3B-I1: Analysis of Complex Survey Data\\Analyse de donn\'ees d'enqu\^ete complexes}
\begin{center}{\large Chair/Président: Karla Fox (Statistics Canada)\protect\\[5pt]
Organizer/Responsable: Lenka Mach (Statistics Canada)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 258\end{center}
\label{abs-sid:acs}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk acs-yb
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 10:20]\\}
{\Author{Yves}{Berger}{berger}{3B-I1}\Author{Melike}{Oguz-Alper}{oguzalper}{3B-I1}%
\abshead{\absauthor{YVES BERGER}\absaffil{University of Southampton}, \absauthor{MELIKE OGUZ-ALPER}\absaffil{Statistics Norway}}
        {An Empirical Likelihood Approach for Complex Sampling\newline
        Une approche par vraisemblance empirique pour l'échantillonnage complexe}

\absSideBySide{Survey data are often collected with unequal probabilities from a stratified population.  We propose a new empirical likelihood approach for sample data selected with unequal probabilities. We show that the empirical likelihood ratio statistic follows a chi-squared distribution asymptotically. The approach proposed does not rely on variance estimates, re-sampling or joint-inclusion probabilities, even when the parameter of interest is not linear. Standard confidence intervals based on variance estimates may give poor coverages, when normality does not hold. This can be the case with skewed data and outlying values. The empirical likelihood confidence interval proposed has good coverages, even when the sampling distribution of the point estimator is not normal.
}{Les données d'enquêtes sont souvent collectées à probabilités inégales à partir de populations stratifiées. Nous proposons une nouvelle approche basée sur la vraisemblance empirique pour des données sélectionnées avec des probabilités inégales. Nous montrons que le ratio de vraisemblance empirique suit une distribution chi-carré asymptotiquement. L'approche proposée ne repose pas sur des estimations de la variance, le re-échantillonnage ou les probabilités d'inclusion jointes, même lorsque le paramètre d'intérêt n'est pas linéaire. Les intervalles de confiance standards basés sur l'estimation de variance peuvent donner des couvertures pauvres en l'absence de normalité. Cela peut être le cas avec des données asymétriques et des valeurs aberrantes. L'intervalle de confiance proposé a des bonnes couvertures, même si l'estimateur ponctuel n'est pas normal.
}}
%% Talk acs-en
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 10:50]\\}
{\Author{Elisabeth}{Neusy}{neusy}{3B-I1}\Author{Harold}{Mantel}{mantel}{3B-I1}%
\abshead{\absauthor{ELISABETH NEUSY} \& \absauthor{HAROLD MANTEL}\absaffil{Statistics Canada}}
        {Confidence Intervals for Proportions Estimated from Complex Survey Data\newline
        Intervalles de confiance pour des proportions estimées à partir de données d'enquêtes complexes}

\absSideBySide{A large number of methods of constructing confidence intervals for proportions have been proposed and studied for non-survey data. A few of these methods have been adapted for complex survey data. A simulation study was undertaken to evaluate the performance of the logit transformation interval, the modified Clopper-Pearson interval, the modified Wilson interval and the bootstrap percentile interval in the context of stratified sampling and two-stage sampling. The paper will briefly review these methods, and present the results and conclusions of the study.
}{Un grand nombre de méthodes ont été proposées et étudiées pour la construction d'intervalles de confiance pour les données non issues d'enquêtes. Certaines d'entre elles ont été adaptées aux données d'enquêtes complexes. Une étude par simulation a été menée pour évaluer la performance de l'intervalle par transformation logit, l'intervalle modifié de Clopper-Pearson, l'intervalle modifié de Wilson et l'intervalle bootstrap par percentiles dans le contexte de l'échantillonnage stratifié et à deux degrés. J'examinerai brièvement ces méthodes et présenterai les résultats et les conclusions de l'étude.
}}
%% Talk acs-cw
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 11:20]\\}
{\Author{Changbao}{Wu}{wu}{3B-I1}\Author{J.N.K.}{Rao}{rao}{3B-I1}%
\abshead{\absauthor{CHANGBAO WU}\absaffil{University of Waterloo}, \absauthor{J.N.K. RAO}\absaffil{Carleton University}}
        {Empirical Likelihood for Public-Use Survey Data\newline
        Vraisemblance empirique pour données de sondage d'utilité publique}

\absSideBySide{In this paper we develop empirical likelihood methods for analyzing public-use survey data that contain only the variables of interest and the final adjusted and calibrated survey weights along with final replication weights. Asymptotic distributions of the empirical likelihood ratio statistics are derived for parameters defined through estimating equations. Finite sample performances of the empirical likelihood ratio confidence intervals, with comparisons to methods based on the estimating equation theory, are investigated through simulation studies. The proposed approaches make empirical likelihood a practically useful tool for users of complex survey data.
}{Dans ce travail, nous mettons au point des méthodes de vraisemblance empirique pour l'analyse de données de sondage d'utilité publique qui ne contiennent que les variables d'intérêt et les pondérations d'enquête ajustées et calibrées finales, ainsi que les pondérations de réplication finales. Nous dérivons les distributions asymptotiques du rapport des vraisemblances empiriques pour des paramètres définis par des équations d'estimation. Nous étudions les propriétés des intervalles de confiance du rapport des vraisemblances empiriques dans le cas d'échantillons finis et les comparons aux méthodes basées sur la théorie des équations d'estimation par des études en simulation. Les approches proposées font de la vraisemblance empirique un outil pratique pour les utilisateurs de données de sondage complexes.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 3B-I2: Analysis of Genomic Sequencing Data in Families\\Analyse des donn\'ees de s\'equen\c cage g\'enomique dans les familles}
\begin{center}{\large Organizer and Chair / Responsable et présidente:  Jinko Graham (Simon Fraser University)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Welch - David Howes\end{center}
\label{abs-sid:ags}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk ags-mllc
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 10:20]\\}
{\Author{M'Hamed Lajmi Lakhal}{Chaieb}{chaieb}{3B-I2}\Author{Martin}{Leclerc}{leclerc}{3B-I2}\Author{Jacques}{Simard}{simard}{3B-I2}%
\abshead{\absauthor{M'HAMED LAJMI LAKHAL CHAIEB}, \absauthor{MARTIN LECLERC} \& \absauthor{JACQUES SIMARD}\absaffil{Université Laval}}
        {SNP-Set Association Testing for Survival Outcomes in the Presence of Intrafamilial Correlation\newline
        Tests d'association d'ensembles de SNPs pour des réponses de survie en présence de corrélation intra-familiale}

\absSideBySide{In this talk, we propose a SNP-set association test for censored phenotypes in the presence of a family-based design. The proposed test is valid for both common and rare variants. A proportional hazards Cox model is specified for the marginal distribution of the trait and the familial dependence is modeled via a Gaussian copula.
Censored values are treated as partially missing data and a multiple imputation procedure is proposed in order to compute the test statistics. The P-value is deduced analytically. The empirical properties of the proposed method are evaluated and compared to existing competitors by simulations and its use is illustrated using a breast cancer data set from the Consortium of Investigators of Modifiers of BRCA1 and BRCA2.
}{Dans cet exposé, nous proposons un test d'association d'ensembles de SNP pour des phénotypes censurés en présence de données familiales. Le test proposé est valide pour les variants courants et rares. Un modèle des risques proportionnels de Cox est spécifié pour la distribution marginale du trait, et la dépendance familiale est modélisée par une copule gaussienne.
Les valeurs censurées sont traitées comme des données partiellement manquantes, et une procédure d'imputation multiple est proposée afin de calculer des statistiques de tests. La valeur P est déduite de façon analytique. Les propriétés empiriques de la méthode proposée sont évaluées et comparées par des simulations avec des méthodes concurrentes existantes. Nous illustrons l'utilisation de cette méthode par un jeu de données sur le cancer du sein provenant du Consortium of Investigators of Modifiers of BRCA1 and BRCA2.
}}
%% Talk ags-mr
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 10:50]\\}
{\Author{Marie-Hélène}{Roy-Gagnon}{roygagnon}{3B-I2}\Author{Kelly}{Burkett}{burkett}{3B-I2}\Author{Jean-François}{Lefebvre}{lefebvre}{3B-I2}\Author{Cheng}{Wang}{wang}{3B-I2}\Author{Bénédicte}{Fontaine-Bisson}{fontainebisson}{3B-I2}\Author{Lise}{Dubois}{dubois}{3B-I2}%
\abshead{\absauthor{MARIE-HÉLÈNE ROY-GAGNON}, \absauthor{KELLY BURKETT}, \absauthor{JEAN-FRANÇOIS LEFEBVRE}, \absauthor{CHENG WANG}, \absauthor{BÉNÉDICTE FONTAINE-BISSON} \& \absauthor{LISE DUBOIS}\absaffil{University of Ottawa}}
        {A Comparison of Statistical Methods for the Discovery of Genetic Risk Factors Using Longitudinal Family Study Designs\newline
        Comparaison de méthodes statistiques pour l'identification de facteurs de risque génétiques dans des devis d'études familiales longitudinales}

\absSideBySide{Phenotypes measured at one time point fail to capture time-varying genetic disease mechanisms. Longitudinal studies can shed light on complex disease mechanisms but require specialized statistical methods, especially in family studies where multiple sources of correlation must be modeled. Using simulations we compared analytical methods for investigating genetic associations in longitudinal twin data modeled on the Québec Newborn Twin Study, including family-based methods applied to summaries of the observations over time, longitudinal-based methods simplifying the familial correlation and Bayesian family-based methods simplifying the temporal correlation. For estimation of genetic effects, all methods gave estimates close to simulated values with similar power. Thus, simpler approaches may likely be adequate to detect genetic effects but their interpretation is more challenging.
}{Les phénotypes mesurés une seule fois ne capturent pas les mécanismes de maladies génétiques variant dans le temps. Les études longitudinales peuvent aider à comprendre ces maladies complexes, mais requièrent des méthodes d'analyses spécialisées, surtout les études familiales incluant plusieurs sources de corrélation. À l'aide de simulations, nous avons comparé des méthodes d'analyse d'association génétique pour des données longitudinales de jumeaux basées sur l'étude des jumeaux nouveau-nés du Québec, incluant des méthodes d'analyse familiale de mesures phénotypes résumées à travers le temps, longitudinale simplifiant la corrélation familiale, et familiale bayésienne simplifiant la corrélation temporelle. Toutes les méthodes donnent des estimés d'effets génétiques proches des valeurs simulées avec puissance comparable. Ainsi, les méthodes simples pourraient être adéquates, mais interprétables plus difficilement.
}}
%% Talk ags-js
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 11:20]\\}
{\Author{Jianping}{Sun}{sun}{3B-I2}\Author{Karim}{Oualkacha}{oualkacha}{3B-I2}\Author{Celia}{Greenwood}{greenwood}{3B-I2}%
\abshead{\absauthor{JIANPING SUN}\absaffil{McGill}, \absauthor{KARIM OUALKACHA}\absaffil{Université du Québec à Montréal}, \absauthor{CELIA GREENWOOD}\absaffil{Lady Davis Institute for Medical Research, Jewish General Hospital, Montreal}}
        {Multivariate Association Test for Rare Variants Controlling for Cryptic and Family Relatedness\newline
        Test d'association à plusieurs variables pour variantes rares contrôlées pour parenté familiale et cryptique}

\absSideBySide{Since rare causal genetic variants are often enriched in families containing multiple affected individuals, family-based study designs are back in vogue for 
studying rare genetic variants.  Recently, we have developed an approach for multivariate-phenotype rare-variant analysis in families by combining univariate 
test statistics for each trait. However, it is usually infeasible to derive the distribution of the optimal combination due to the complex unknown covariance 
structure among these univariate test statistics. Hence, we propose two different approaches by using either a copula model or a perturbation method to approximate 
the distribution of the combined test statistic. In this talk, we will present this work together with some simulations describing the performance of these two 
options.
}{Puisque les variantes génétiques rares de causalité sont souvent enrichies dans les familles ayant plusieurs individus affectés, les plans d'étude basés sur la famille 
reviennent en vogue pour étudier les variantes génétiques rares. Récemment, nous avons développé une approche pour l'analyse des phénotypes multivariés et des variantes 
rares dans les familles en combinant les statistiques de test univarié pour chaque trait. Cependant, il est habituellement impossible de dériver la loi de la combinaison
optimale en raison de la structure complexe de la covariance inconnue parmi ces statistiques de test univarié. Par conséquent, nous proposons deux 
approches différentes par l'utilisation soit d'un modèle de copules ou d'une méthode de perturbation pour approximer la loi des statistiques de test combiné. 
Dans cet exposé, nous présenterons ce travail ainsi que des simulations pour décrire la performance de ces deux options.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 3B-I3: High-dimensional Statistics: Challenges and Recent Developments\\Statistiques en haute dimension : d\'efis et progr\`es r\'ecents}
\begin{center}{\large Organizer and Chair / Responsable et président:  Yingli Qin (University of Waterloo)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 242\end{center}
\label{abs-sid:hsc}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk hsc-sc
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 10:20]\\}
{\Author{Shoja'eddin}{Chenouri}{chenouri}{3B-I3}\Author{Jiaxi}{Liang}{liang}{3B-I3}\Author{Christopher G.}{Small}{small}{3B-I3}%
\abshead{\absauthor{SHOJA'EDDIN CHENOURI}, \absauthor{JIAXI LIANG} \& \absauthor{CHRISTOPHER G. SMALL}\absaffil{University of Waterloo}}
        {Nonlinear Dimension Reduction: Performance Analysis and Robustness\newline
        Réduction de la dimension non linéaire : analyse de l'efficacité et robustesse}

\absSideBySide{Nonlinear dimension reduction has been of much interest in the recent years and many methods have been introduced. Each method works only under certain underlying assumptions and their robustness is generally unknown. In the example of PCA, a few robust alternatives have been introduced. In the nonlinear framework, the robustness of dimension reduction methods has not been explored thoroughly. In this talk we attempt to tackle this problem. We introduce a goodness measure called local rank correlation for assessing the performance of dimension reduction methods. We discuss how the local rank correlation can be used to select tuning parameters of dimension reduction algorithms. We also introduce influence functions to measure local robustness and discuss outlier detection in dimension reduction framework.
}{La réduction de la dimension non linéaire a suscité beaucoup d'intérêt au cours des dernières années et de nombreuses méthodes ont été présentées. Chaque méthode fonctionne seulement sous certaines hypothèses sous-jacentes et la robustesse est généralement inconnue. Dans l'exemple de l'analyse en composantes principales, quelques méthodes robustes alternatives ont été proposées. Dans le cadre non linéaire, la robustesse des méthodes de réduction de la dimension n'a pas encore été examinée en profondeur. Dans cet exposé, nous tentons de résoudre ce problème. Nous présentons une mesure de l'adéquation appelée corrélation locale de rang, permettant d'évaluer l'efficacité des méthodes de réduction de la dimension. Nous discutons de la façon dont la corrélation locale de rang peut être utilisée pour sélectionner les paramètres de réglage des algorithmes de réduction de la dimension. Nous présentons également les fonctions d'influence pour mesurer la robustesse locale et discutons de la détection des valeurs aberrantes dans le cadre de la réduction de la dimension.
}}
%% Talk hsc-jy
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 10:50]\\}
{\Author{Jeff}{Yao}{yao}{3B-I3}\Author{Zeng}{Li}{li}{3B-I3}\Author{Wang}{Qinwen}{qinwen}{3B-I3}%
\abshead{\absauthor{JEFF YAO}\absaffil{University of Hong Kong}, \absauthor{ZENG LI}\absaffil{The University of Hong Kong}, \absauthor{WANG QINWEN}\absaffil{University of Pennsylvania}}
        {Identifying the Number of Factors from Singular Values of a Large Sample Auto-Covariance Matrix\newline
        Identification du nombre de facteurs à l'aide des valeurs singulières d'une grande matrice d'autocovariance empirique}

\absSideBySide{This paper  first proposes a  complete theory for  singular values of lagged sample autocovariance matrices from a high-dimensional factor model covering both the factor part and the noise part assuming that the dimension and the sample size proportionally grow to infinity.
In particular, we  provide  an  exact description of the phase transition phenomenon that determines whether a factor is strong enough to be detected asymptotically. Next, we propose a new and strongly consistent estimator for the number of significant factors including both weak and pervasive factors. In all tested cases, the new estimator largely outperforms existing estimators using the same ratios of singular values.\\
\\
This is a joint work with Zeng Li and Qinwen Wang.
}{Cet article propose d'abord une théorie complète des valeurs singulières d'une matrice d'autocovariance empirique dans un modèle à facteurs de grande dimension, couvrant à la fois la partie facteurs et la partie bruit et lorsque la dimension et la taille d'échantillon croissent proportionnellement. En particulier, nous déterminons la frontière exacte de transition pour qu'un facteur soit détectable asymptotiquement.  Ensuite, nous proposons un nouvel estimateur du nombre de facteurs incluant à la fois les facteurs faibles et forts qui est fortement convergent.  Dans tous les cas testés, ce nouvel estimateur est nettement meilleur que les estimateurs existants qui utilisent ces mêmes valeurs singulières.\\
\\
C'est un travail joint avec Zeng Li et Qinwen Wang.
}}
%% Talk hsc-pz
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 11:20]\\}
{\Author{Ping-Shou}{Zhong}{zhong}{3B-I3}\Author{Runze}{Li}{li}{3B-I3}%
\abshead{\absauthor{PING-SHOU ZHONG}\absaffil{Michigan State University}, \absauthor{RUNZE LI}\absaffil{Pennsylvania State University}}
        {Homogeneity Test of Covariance Matrices and Change-Points Identification with High-Dimensional Longitudinal Data\newline
        Test d'homogénéité de matrices de covariance et l'identification de points de rupture avec des données longitudinales de grande dimension}

\absSideBySide{High-dimensional longitudinal data are widely available. One important feature of such data is that, for each individual, high-dimensional measurements are 
repeatedly collected over time and they are spatially and temporally dependent. This paper focuses on testing the homogeneity of covariance matrices of 
high-dimensional measurements over time against the change-point type alternatives. We allow the dimension of measurements to be much larger than the number 
of individuals. An estimator of the location of the change point is also given whose rate of convergence is established. The proposed method is then extended 
to locate multiple change points by applying a binary segmentation approach, which is shown to be consistent. Simulation studies and an application are presented 
to demonstrate the proposed methods.
}{Des données longitudinales de grandes dimensions sont largement disponibles. Une caractéristique importante de ces données est que, pour chaque individu, 
des mesures de grande dimension sont recueillies à maintes reprises au fil du temps et celles-ci sont dépendantes du temps et de l'espace. Cet article se concentre sur
les tests d'homogénéité des matrices de covariance des mesures de grande dimension au fil du temps, par rapport aux alternatives de type points de rupture. Nous 
permettons à la dimension des mesures d'être beaucoup plus grande que le nombre d'individus. Un estimateur de l'emplacement du point de rupture est aussi donné 
dont la vitesse de convergence est établie. La méthode proposée est ensuite élargie pour inclure la localisation de multiples points de rupture en appliquant une 
approche de segmentation binaire, qui est démontrée comme étant cohérente. Des études de simulation et une application sont présentées pour démontrer les méthodes 
proposées.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 3B-I4: Statistical Theory and Modeling for Functional Data Analysis\\Th\'eorie et mod\'elisation statistiques pour l'analyse de donn\'ees fonctionnelles}
\begin{center}{\large Organizer and Chair / Responsable et président:  Haocheng Li (University of Calgary)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 246\end{center}
\label{abs-sid:stm}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk stm-kj
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 10:20]\\}
{\Author{Kexin}{Ji}{ji}{3B-I4}\Author{Joel A.}{Dubin}{dubin}{3B-I4}%
\abshead{\absauthor{KEXIN JI} \& \absauthor{JOEL A. DUBIN}\absaffil{University of Waterloo}}
        {Semiparametric Stochastic Mixed Models for Bivariate Periodic Longitudinal Data\newline
        Modèles stochastiques mixtes semi-paramétriques pour des données longitudinales périodiques bidimensionnelles}

\absSideBySide{We propose and consider inference for a semiparametric stochastic mixed model for bivariate periodic repeated measures data.  The bivariate model uses parametric 
fixed effects for modelling covariate effects and periodic smooth nonparametric functions for each of the two underlying time effects. In addition, the 
between-subject and within-subject correlations are modelled using separate but correlated random effects and a bivariate Gaussian random field, respectively. 
We derive maximum penalized likelihood estimators for both the fixed effects regression coefficients and the nonparametric functions. The smoothing parameters and 
all variance components are estimated simultaneously using restricted maximum likelihood.  We investigate the proposed methodology through simulation.  We also 
illustrate the model by analyzing bivariate longitudinal female hormone data collected daily over multiple consecutive menstrual cycles.
}{Nous proposons et étudions un modèle stochastique mixte semi-paramétrique pour des données de mesures répétées périodiques bidimensionnelles. Le modèle bidimensionnel 
utilise des effets paramétriques fixes pour modéliser les effets de covariables et les fonctions périodiques lissées non paramétriques pour chacun des deux effets 
chronologiques sous-jacents. De plus, les corrélations entre sujets et intra-sujets sont modélisées en utilisant, respectivement, des effets aléatoires séparés mais 
corrélés et un champ aléatoire gaussien bidimensionnel. Nous dérivons des estimateurs de maximum de vraisemblance pénalisée pour les coefficients de régression à effets 
fixes et pour les fonctions non paramétriques. Les paramètres lissants et toutes les composantes de la variance sont estimés simultanément en utilisant le maximum de 
vraisemblance restreint. Nous examinons la méthodologie proposée par la simulation. Nous illustrons aussi le modèle en analysant des données longitudinales 
bidimensionnelles d'hormones féminines recueillies quotidiennement durant de multiples cycles menstruels.
}}
%% Talk stm-bj
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 10:50]\\}
{\Author{Bei}{Jiang}{jiang}{3B-I4}\Author{Eva}{Petkova}{petkova}{3B-I4}\Author{Thaddeus}{Tarpey}{tarpey}{3B-I4}\Author{R. Todd}{Ogden}{ogden}{3B-I4}%
\abshead{\absauthor{BEI JIANG}\absaffil{University of Alberta}, \absauthor{EVA PETKOVA}\absaffil{New York University}, \absauthor{THADDEUS TARPEY}\absaffil{Wright State University}, \absauthor{R. TODD OGDEN}\absaffil{Columbia University}}
        {Latent Class Modeling Using Matrix-valued Covariates with Application to Identifying Early Placebo Responders Based on EEG Signals\newline
        Modélisation de classes latentes utilisant des covariables à valeurs matricielles pour identifier les répondeurs hâtifs au placebo basée sur les signaux EEG}

\absSideBySide{In this talk, we extend existing latent class models to incorporate matrix covariates. The proposed method is built upon a low rank Candecomp/Parafac (CP) 
decomposition to express the target coefficient matrix through low-dimensional latent variables, which effectively reduces the model dimensionality, and utilizes a 
Bayesian hierarchical modeling approach to estimating these latent variables, which provides a way to incorporate prior knowledge on the patterns of covariate effect 
heterogeneity and provides a data-driven method of regularization. Our simulation studies suggest that our proposed method is robust against potentially misspecified 
rank in the CP decomposition. We show that the proposed method allows us to extract useful information from baseline EEG measurements that explains the likelihood of 
belonging to the early responder subgroup.
}{Dans cet exposé, nous élargissons les modèles de classes latentes existantes pour incorporer les covariables matricielles. La méthode proposée est fondée sur une décomposition 
Candecomp/Parafac (CP) à rang inférieur pour exprimer la matrice de coefficients par des variables latentes à basses dimensions, ce qui peut efficacement réduire la 
dimension du modèle, et utilise une approche de modélisation bayésienne hiérarchique pour estimer ces variables latentes, ce qui donne une façon d'incorporer des 
connaissances préalables sur les modèles de l'hétérogénéité des effets de covariables et fournit une méthode de régularisation basée sur les données. Nos études de 
simulation suggèrent que notre méthode est robuste face aux rangs potentiellement mal spécifiés dans la décomposition CP. Nous démontrons que la méthode proposée nous 
permet d'extraire de l'information importante des mesures de références EEG qui explique la vraisemblance d'appartenir à un sous-groupe de répondeurs hâtifs.
}}
%% Talk stm-fy
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 11:20]\\}
{\Author{Zhenhua}{Lin}{lin}{3B-I4}\Author{Fang}{Yao}{yao}{3B-I4}\Author{Hans G.}{M\"uller}{muller}{3B-I4}%
\abshead{\absauthor{ZHENHUA LIN} \& \absauthor{FANG YAO}\absaffil{University of Toronto}, \absauthor{HANS G. M\"ULLER}\absaffil{University of California at Davis}}
        {Mixture Models and Densities for Functional Data\newline
        Modèles de mélange et de densités pour données fonctionnelles}

\absSideBySide{We propose a novel perspective to represent infinite-dimensional functional data as mixtures, where the number of basis functions that constitutes the mixture may 
be arbitrarily large, but the number of included mixture components is finite and specifically adapted for each random trajectory. Within this framework, we show 
that a probability density can be well defined without the need for finite truncation or approximation. Dimensions of individual trajectories are treated as latent 
random variables and a modified expectation-maximization algorithm is adopted for model fitting. Simulations confirm that comparing to traditional FPCA the proposed 
method achieves similar or better data recovery while using fewer components on average. The practical merits of functional mixture modeling are demonstrated by 
analyzing egg-laying trajectories for medflies.
}{Nous proposons un point de vue novateur pour la représentation des données fonctionnelles à dimension infinie par des mélanges, où le nombre de fonctions de base qui 
constituent le mélange peut être arbitrairement élevé, mais le nombre de composantes incluses dans le mélange est fini et spécifiquement adapté à chaque trajectoire 
aléatoire. Dans ce cadre, nous démontrons qu'une densité de probabilité peut être bien définie sans avoir besoin d'une approximation ou d'une troncature finie. Les 
dimensions des trajectoires individuelles sont traitées comme des variables aléatoires latentes et un algorithme modifié de maximisation d'espérance est adopté pour 
l'ajustement du modèle. Des simulations confirment qu'en la comparant au FPCA traditionnel, la méthode proposée réalise une récupération de données similaire ou 
meilleure tout en utilisant en moyenne moins de composantes. L'intérêt pratique de la modélisation de mélange fonctionnel est démontré par l'analyse des trajectoires 
de pontes d'oeufs de la mouche méditerranéenne.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 3B-I5: Staying Up-To-Date As a Practicing Biostatistician\\Comment rester \`a la page en tant que biostatisticien praticien}
\begin{center}{\large Organizer and Chair / Responsable et présidente:  Eleanor Pullenayegum (Hospital for Sick Children)}
\end{center}
\par \vspace{3pt}

\begin{center}\large\bfseries Room/Salle: Thistle 325\end{center}
\label{abs-sid:sua}
\begin{center}{\large\bfseries Description}\end{center}
\bigskip
\absSideBySide{A medical colleague recently pointed out that when he cares for his patients, he is expected to be up to date with all the relevant medical research. So, when he goes to see a biostatistician, that person should be up-to-date with all recent developments in biostatistics. Perhaps you're thinking, “That's impossible”? This roundtable will discuss: a) As a practicing biostatistician, what do you need to stay up to date with?  b) How do you stay up to date?  c) Are there ways we could organize ourselves as a discipline to make it easier to stay up to date?}{Un collègue du milieu médical faisait remarquer dernièrement qu'on s'attendait à ce qu'il soit à jour sur les études de recherche pertinentes lorsqu'il prodigue des soins aux patients. Par conséquent, s'il s'adresse à un biostatisticien, celui-ci doit être au fait des plus récents développements dans son domaine. Impossible, croyez-vous? Dans le cadre de cette table ronde, nous nous demanderons a) De quel contenu informatif un biostatisticien praticien doit-il être au fait?  b) Comment mettre à jour nos connaissances? c) De quelle façon les praticiens de notre discipline peuvent-ils s'organiser pour faciliter l'actualisation de leur savoir?}\bigskip
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 3B-I6: The Theory of Dispersion Models with Applications\\La th\'eorie de la dispersion avec des applications}
\begin{center}{\large Organizer and Chair / Responsable et président:  Renjun Ma (University of New Brunswick)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Academic South 217\end{center}
\label{abs-sid:ttd}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk ttd-rl
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 10:40]\\}
{\Author{Rodrigo}{Labouriau}{labouriau}{3B-I6}%
\abshead{\absauthor{RODRIGO LABOURIAU}\absaffil{Aarhus University}}
        {Multivariate Generalized Linear Mixed Models with High Complexity\newline
        Modèles linéaires généralisés mixtes multivariés avec complexité élevée}

\absSideBySide{The theory of exponential dispersion models (EDM), for which Bent Jørgensen made substantial contributions, provides a flexible framework of models alternative to 
the classic Gaussian linear models (e.g. generalized linear models and additive models). I will review some multivariate extensions of those models that allow the 
distribution of the different dimensions to belong to different EDMs. As an illustration, I will present some applications in quantitative genetics with high 
complexity (several hundreds of thousand observations and deep pedigrees). In all the presented applications, it is crucial to understand the underlying stochastic 
process related to the EDMs used to represent well and interpret the biological questions of interest. Bent Jørgensen advocated similar ideas in his work since the 
1980s.
}{La théorie des modèles de dispersion exponentielle (EDM), à laquelle Bent Jørgensen a apporté d'importantes contributions, fournit un cadre flexible de modèles 
alternatifs aux modèles linéaires gaussiens classiques (par exemple, les modèles linéaires et les modèles additifs).  J'examinerai quelques extensions multivariées 
de ces modèles qui permettent à la loi des différentes dimensions d'appartenir à différents EDM. À titre d'exemple, je présenterai quelques applications à 
la génétique quantitative avec complexité élevée (plusieurs centaines de milliers d'observations et des pedigrees profonds). Dans toutes les applications présentées, il est 
essentiel de comprendre le processus stochastique sous-jacent associé aux EDMs pour bien représenter et interpréter les questions biologiques d'intérêt. Bent 
Jørgensen prônait des idées similaires dans son travail depuis les années 1980.
}}
%% Talk ttd-pxs
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 11:20]\\}
{\Author{Peter XK}{Song}{song}{3B-I6}%
\abshead{\absauthor{PETER XK SONG}\absaffil{University of Michigan}}
        {Dispersion Models and Beyond\newline
        Les modèles de dispersion et au-delà}

\absSideBySide{Seminal work by Dr. Bent Jørgensen for the theory of dispersion models has made profound impact on the statistical distribution theory, in particular providing the 
novel extension of the classical theory of generalized linear models.   In this talk, I will present a review on several important developments of multivariate 
dispersion models useful to analyze multi-dimensional data, longitudinal data and spatially correlated data.  My review will focus on multivariate dispersion models 
generated by copulas and the method of generalized estimating functions, as well as their applications in regression analysis.   A few R packages will be introduced 
to demonstrate the usefulness of dispersion models in practical studies.  In addition, some data applications will be discussed.
}{Le travail précurseur du Dr Bent Jørgensen sur la théorie des modèles de dispersion a eu un profond impact sur la théorie des lois statistiques, particulièrement avec 
l'extension novatrice de la théorie classique des modèles linéaires généralisés. Dans cet exposé, je présenterai une revue de plusieurs développements importants dans 
les modèles de dispersion multivariés qui sont utiles pour l'analyse de données multidimensionnelles, de données longitudinales et de données corrélées spatialement. Ma revue se 
concentrera sur les modèles de dispersion multivariés générés par des copules et sur la méthode des fonctions d'estimation généralisées, ainsi que leurs applications dans 
l'analyse de régression. Quelques paquets R seront présentés pour démontrer l'utilité des modèles de dispersion dans des études pratiques. De plus, je discuterai de 
quelques applications des données.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 3B-C1: Biostatistics: Survival Analysis\\Biostatistique : analyse de survie}
\begin{center}{\large Chair/Président: John Koval (University of Western Ontario)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 256\end{center}
\label{abs-sid:bsa}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk bsa-sc
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 10:20]\\}
{\Author{Samantha-Jo}{Caetano}{caetano}{3B-C1}\Author{Gregory}{Pond}{pond}{3B-C1}\Author{Peter}{Ellis}{ellis}{3B-C1}\Author{David}{Dawe}{dawe}{3B-C1}%
\abshead{\absauthor{SAMANTHA-JO CAETANO}, \absauthor{GREGORY POND}, \absauthor{PETER ELLIS} \& \absauthor{DAVID DAWE}\absaffil{McMaster University}}
        {Proposing and Analysing Methods which Improve the Kaplan-Meier Estimates of Altered Time-to-Event Data\newline
        Proposer et analyser des méthodes qui améliorent les estimations Kaplan-Meier de données altérées de temps d'événement}

\absSideBySide{Patient registry databases must be de-identified prior to analysis to ensure confidentiality. This de-identification alters the data, which can impact the results, and potentially lead to incorrect conclusions. A recent analysis of lung cancer patients with data obtained from registry databases was observed to contain a missing time bias due to de-identification. Five statistical methods were proposed and investigated to examine how to reduce this bias when calculating time-to-event estimates. Through statistical simulation, the bias, mean-squared error and coverage probability of each method was assessed. The optimal method was then applied to the original dataset and estimates re-evaluated.
}{Les bases de données de registre des patients doivent être dépersonnalisées avant l'analyse pour assurer la confidentialité. Cette dépersonnalisation modifie les données, ce qui peut avoir une incidence sur les résultats, et potentiellement conduire à des conclusions erronées. Une récente analyse des patients atteints d'un cancer du poumon avec des données obtenues à partir des bases de données de registre contenait un biais de temps manquant en raison de la dépersonnalisation. Cinq méthodes statistiques ont été proposées et étudiées pour examiner comment réduire ce biais lors du calcul des estimations de temps à l'événement. Grâce à la simulation statistique, le biais, l'erreur quadratique moyenne et la probabilité de couverture de chaque méthode ont été évalués. La méthode optimale a ensuite été appliquée au jeu de données initial et les estimations ont été réévaluées.
}}
%% Talk bsa-na
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 10:35]\\}
{\Author{Noori}{Akhtar-Danesh}{akhtardanesh}{3B-C1}\Author{Lina}{Jansen}{jansen}{3B-C1}\Author{Hermann}{Brenner}{brenner}{3B-C1}%
\abshead{\absauthor{NOORI AKHTAR-DANESH}\absaffil{McMaster University}, \absauthor{LINA JANSEN} \& \absauthor{HERMANN BRENNER}\absaffil{German Cancer Research Centre}}
        {Estimating Complete Life Tables from Abridged Life Tables in Relative Survival Analysis\newline
        Estimer des tables de mortalité complètes dans une analyse de survie relative à partir de tables de mortalité abrégées}

\absSideBySide{Complete life tables (CLT) are required for the computation of relative survival in cancer patients.  Different methods are used to estimate CLT from abridged life 
tables (ALT). However, the impact of these methods on relative survival estimates is unknown.  We used Elandt-Johnson method, improved Akima method, Brass logit 
model, Kostaki method, and a simple arithmetic progression method (APM) to estimate the CLT from an ALT. The maximum difference in relative survival estimates within 
10 years of follow-up was less than 2 percent in the oldest age group.  Most importantly, the APM was as good as the other complex methods.
}{Des tables complètes de mortalité (CLT) sont nécessaires pour calculer la survie relative des patients atteints d'un cancer. Différentes méthodes sont utilisées pour 
l'estimation de CLT à partir de tables de mortalité abrégées (ALT). Par contre, l'impact de ces méthodes sur les estimations de survie relative est inconnu. Nous 
avons utilisé la méthode Elandt-Johnson, amélioré la méthode Akima, le modèle Brass logit, la méthode Kostaki, et une méthode simple de progression arithmétique (APM) 
pour estimer une CLT à partir d'une ALT. La différence maximale dans les estimations de survie relative lors des 10 années de suivi fut de moins de 2 pour cent dans la 
tranche d'âge la plus âgée et qui plus est, la APM était aussi bonne que les autres méthodes complexes.
}}
%% Talk bsa-cd
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 10:50]\\}
{\Author{Coraline}{Danieli}{danieli}{3B-C1}\Author{Michal}{Abrahamowicz}{abrahamowicz}{3B-C1}%
\abshead{\absauthor{CORALINE DANIELI} \& \absauthor{MICHAL ABRAHAMOWICZ}\absaffil{McGill University}}
        {Competing Risk Modelling of Time-Varying Exposures\newline
        Modélisation du risque concurrent d'expositions variant dans le temps}

\absSideBySide{We propose a new method to address two important challenges, often encountered when modeling time-varying exposures in time-to-event analyses: the need to account for cumulative effects of past exposures and competing risks of alternative endpoints (e.g. different sources of mortality). 
Our approach combines flexible regression spline-based weighted cumulative exposure methodology, with competing risks extension of time-dependent Cox model, that adapts the Lunn-McNeil data augmentation approach. We also propose a systematic approach for testing significance of the difference between the effects of the same exposure on alternative outcomes. Simulations demonstrate the accuracy of the proposed estimates and tests.
}{Nous proposons une nouvelle méthode pour faire face à deux défis importants, souvent rencontrés lors de la modélisation des expositions variant dans le temps dans les analyses de temps d'événement : la nécessité de tenir compte des effets cumulatifs des expositions passées et des risques concurrents des terminaux alternatifs (par exemple, différentes sources de mortalité).
Notre approche combine une méthodologie flexible d'exposition pondérée cumulative de régression spline, avec une extension des risques concurrents du modèle de Cox dépendant du temps qui adapte l'approche d'augmentation de données de Lunn-McNeil. Nous proposons également une approche systématique pour tester la signification de la différence entre les effets d'une même exposition sur les résultats alternatifs. Les simulations démontrent l'exactitude des estimations et des tests proposés.
}}
%% Talk bsa-hs
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 11:05]\\}
{\Author{Hua}{Shen}{shen}{3B-C1}%
\abshead{\absauthor{HUA SHEN}\absaffil{University of Calgary}}
        {Methods for Incomplete Covariates in Clustered Survival Data with Left Truncation\newline
        Méthodes pour les covariables incomplètes dans des données de survie regroupées avec troncation à gauche}

\absSideBySide{In studies of chronic diseases individuals are often routinely sampled subject to certain conditions on an event time of interest. These conditions yield response-biased samples featuring left-truncated event times. The failure times of the individuals within some groups can be correlated due to natural common features or shared environmental factors. Incomplete covariate data are widely encountered in these settings. The fact that the covariate distribution is affected by the left truncation selection criterion and the clustering is often ignored in standard methods leading to biased estimates. An algorithm is developed to deal with these challenges and applied to case studies.
}{Dans les études de maladies chroniques, les individus sont souvent échantillonnés régulièrement en fonction de certaines conditions reliées à un temps d'événement d'intérêt. Ces conditions engendrent des échantillons dont les réponses sont biaisées et avec des temps d'événement tronqués à gauche. Les temps de défaillance des individus dans certains groupes peuvent être corrélés en raison de caractéristiques communes naturelles ou de facteurs environnementaux partagés. On retrouve régulièrement des covariables incomplètes dans ces contextes. La plupart du temps, les méthodes standards ne tiennent pas compte du fait que la distribution des covariables est affectée par le critère de sélection à troncation à gauche ainsi que le regroupement, ce qui biaise les estimations. Un algorithme est conçu pour relever ces défis et est appliqué dans des études de cas.
}}
%% Talk bsa-yx
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 11:20]\\}
{\Author{Yingying}{Xu}{xu}{3B-C1}\Author{Joon}{Lee}{lee}{3B-C1}\Author{Joel A.}{Dubin}{dubin}{3B-C1}%
\abshead{\absauthor{YINGYING XU}, \absauthor{JOON LEE} \& \absauthor{JOEL A. DUBIN}\absaffil{University of Waterloo}}
        {Personalized Time-to-Event Prediction in Intensive Care\newline
        Prévision personnalisée des occurrences dans le temps pour les soins intensifs}

\absSideBySide{Predicting the time to clinical outcomes for patients in intensive care units (ICU's) helps to support doctor's treatment decisions.  The time could be, for example, survival time or time to recovery from a disease within the ICU. We developed methodology that extends current survival analysis models and improves the prediction accuracy by systematically identifying similar patients and making personalized predictions. Both time-fixed and time-dependent covariates can be incorporated in our proposed model. Our method is validated in the Multi-Parameter Intelligent Monitoring for Intensive Care (MIMIC-III) database, and we will also present properties of our methodology through a comprehensive simulation study.
}{La prévision du temps aux réponses cliniques chez les patients admis dans une unité de soins intensifs (USI) aide à appuyer les traitements décidés par le médecin. Le facteur temps pourrait être, par exemple, la durée de survie ou de rétablissement à la suite d'une maladie pendant la période passée aux soins intensifs. Nous avons mis au point une méthodologie qui étend la portée des modèles courants d'analyse de survie et améliore l'exactitude prévisionnelle en identifiant systématiquement les cas similaires et en avançant des prévisions personnalisées. Le modèle que nous proposons peut comporter à la fois des covariables à temps fixe et dépendantes du temps. Notre méthode est validée par la base de données de veille sanitaire intelligente multiparamétrique en soins intensifs (MIMIC-III). Nous présenterons également les propriétés de notre méthodologie à l'aide d'une étude de simulation approfondie.
}}
%% Talk bsa-yz
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 11:35]\\}
{\Author{Yayuan}{Zhu}{zhu}{3B-C1}\Author{Jerald F.}{Lawless}{lawless}{3B-C1}\Author{Cecilia A.}{Cotton}{cotton}{3B-C1}%
\abshead{\absauthor{YAYUAN ZHU}, \absauthor{JERALD F. LAWLESS} \& \absauthor{CECILIA A. COTTON}\absaffil{University of Waterloo}}
        {Non- and Semiparametric Analysis of Dependently Interval-Censored Failure Time Data Due to Intermittent Visits at Disease Clinics\newline
        Analyse non paramétrique et semi-paramétrique de données de temps de défaillance avec censure par intervalle dépendante en raison de visites intermittentes à une clinique de traitement de maladies}

\absSideBySide{Event history studies based on disease clinic data often have intermittent clinic visits. Irregular inspection times may depend on the history of disease-related variables; this can cause event times or failure times to be dependently interval-censored. A so-called inverse-intensity-of-visit (IIV) weighting method was developed for irregularly observed longitudinal or repeated measures data. We extend this method to failure time data analysis with dependent interval censoring. This presentation focuses on non- and semiparametric estimation of failure time distributions with monotone smoothing techniques applied. As an illustration, the proposed methodologies are applied to the Toronto Psoriatic Arthritis (PsA) Cohort Study.
}{Les études événementielles fondées sur les données de cliniques de traitement de maladies tiennent souvent compte des visites intermittentes à la clinique. Les périodes irrégulières d'inspection peuvent dépendre de l'historique des variables liées à la maladie, ce qui peut faire en sorte que les temps d'événements ou de défaillance soient censurées par intervalle de manière dépendante. Une soi-disant méthode de pondération en fonction de l'intensité inverse des visites (IIV) a été conçue pour les mesures longitudinales ou répétées observées de façon irrégulière. Nous élargissons cette méthode à l'analyse de données sur le temps des défaillances avec censure par intervalle dépendante. Cette présentation est axée sur une estimation non paramétrique et semi-paramétrique des distributions des temps de défaillance appliquant des techniques de lissage monotone. Pour illustrer les méthodologies proposées, nous les appliquons à l'étude de cohorte de la clinique de polyarthrite psoriasique de Toronto.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 3B-C2: Time Dependent Data\\Donn\'ees d\'ependantes du temps}
\begin{center}{\large Chair/Président: Edward Chen (Statistics Canada)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 255\end{center}
\label{abs-sid:tdd}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk tdd-fc
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 10:20]\\}
{\Author{Fuqi}{Chen}{chen}{3B-C2}\Author{Rogemar}{Mamon}{mamon}{3B-C2}\Author{Sévérien}{Nkurunziza}{nkurunziza}{3B-C2}%
\abshead{\absauthor{FUQI CHEN} \& \absauthor{ROGEMAR MAMON}\absaffil{Western University}, \absauthor{SÉVÉRIEN NKURUNZIZA}\absaffil{University of Windsor}}
        {Estimation and Inference for a Change-point Problem under a Generalised Ornstein-Uhlenbeck Setting\newline
        Estimation et inférence pour un problème de point de changement sous une situation généralisée Ornstein-Uhlenbeck}

\absSideBySide{We develop two methods in locating the change points of a generalised Ornstein-Uhlehnbeck process. The proposed methods were motivated by the least sum of squared error (LSSE) and the maximum log-likelihood (MLL) approaches. We show further that our methods provide consistent estimators, as well as the asymptotic normality of the MLE for the drift parameters. The case where both the number and the location of the change points are unknown is investigated and an informational methodology is employed to address these issues. We also provide some numerical simulations to illustrate the performance of the proposed methods.
}{Nous développons deux méthodes pour localiser les points de changement d'un processus généralisé d'Ornstein-Uhlenbeck. Les méthodes proposées ont été motivées par les approches de la somme des erreurs des moindres carrés (SEMC) et du maximum de la log-vraisemblance (MLV). Nous démontrons en outre que les méthodes fournissent des estimateurs convergents de même que la normalité asymptotique du EMV pour les paramètres de dérive. Le cas où le nombre et l'emplacement des points de changement sont inconnus est étudié et une méthodologie d'information est utilisée pour répondre à ces questions. Nous fournissons également des simulations numériques afin d'illustrer la performance des méthodes proposées.
}}
%% Talk tdd-fm
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 10:35]\\}
{\Author{François}{Marshall}{marshall}{3B-C2}%
\abshead{\absauthor{FRANÇOIS MARSHALL}\absaffil{Queen's University}}
        {Robust Spectrum Estimation: Application to Relative Ionospheric Opacity Meters\newline
        Estimation spectrale robuste : application à des instruments de mesure de l'opacité ionosphérique relative}

\absSideBySide{Relative ionospheric opacity meters (riometers) are antennae which measure the effects of the solar wind on the upper atmosphere. Their response is a voltage series, and because they are outdoors measuring natural phenomena, riometers are prone to the full range of noise sources and contaminating agents.
In this presentation, a review will be given of robust techniques for computing an estimate of the spectral density. In particular, techniques are employed which make use of a base spectrum and a detail spectrum, in an effort to account for the non-centrality of the chi-squared distribution commonly associated with the noise.
}{Les instruments de mesure de l'opacité ionosphérique relative (riomètres) sont des antennes qui mesurent les effets du vent solaire sur la haute atmosphère. Ils réagissent par des séries de voltage, et parce qu'ils se trouvent à l'extérieur pour mesurer les phénomènes naturels, les riomètres sont soumis à toute une série de sources de bruit et d'agents de contamination.
Dans cet exposé, nous examinerons les techniques robustes de calcul d'une estimation de la densité spectrale. Les techniques employées font notamment appel à un spectre de base et à un spectre détaillé afin de prendre en compte la non-centralité de la loi du x2 souvent associée au bruit.
}}
%% Talk tdd-fp
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 10:50]\\}
{\Author{Frédéric}{Picard}{picard}{3B-C2}\Author{Steve}{Matthews}{matthews}{3B-C2}%
\abshead{\absauthor{FRÉDÉRIC PICARD} \& \absauthor{STEVE MATTHEWS}\absaffil{Statistics Canada}}
        {The Addition of Trend-Cycle Estimates to Selected Publications at Statistics Canada\newline
        L'ajout de la tendance-cycle à quelques publications de Statistique Canada}

\absSideBySide{This talk will cover a number of aspects related to the trend-cycle estimates that Statistics Canada has recently included in selected publications. Statistics Canada uses a trend estimation method proposed by Dagum and Luati (2009). We will briefly describe the method and explain why it was chosen. In particular, an evaluation study to compare a number of options for estimating the trend-cycle will be presented and the findings will be discussed.  As well, several issues related to the use of the trend-cycle estimates will be explored, including how they are presented and recommendations for data users in interpreting the results.
}{Dans cette présentation, plusieurs aspects reliés à l'estimation de la tendance-cycle, que Statistique Canada a récemment incluse dans certaines de ses publications, seront traités. Statistique Canada utilise une méthode d'estimation de tendance proposée par Dagum et Luati (2009). La méthode sera brièvement décrite et les raisons pour lesquelles elle a été choisie seront expliquées. En particulier, une étude évaluative pour comparer quelques méthodes d'estimation de tendance sera présentée et suivie d'une discussion des résultats. Aussi, plusieurs enjeux reliés à l'utilisation de la tendance-cycle seront explorés. Ceci inclut la façon de la présenter et les recommandations aux utilisateurs.
}}
%% Talk tdd-rr
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 11:05]\\}
{\Author{Reza}{Ramezan}{ramezan}{3B-C2}\Author{Paul}{Marriott}{marriott}{3B-C2}\Author{Shoja'eddin}{Chenouri}{chenouri}{3B-C2}%
\abshead{\absauthor{REZA RAMEZAN}\absaffil{California State University, Fullerton}, \absauthor{PAUL MARRIOTT} \& \absauthor{SHOJA'EDDIN CHENOURI}\absaffil{University of Waterloo}}
        {Time Scale in Neuronal Data: A Multiscale Model\newline
        Échelle de temps dans les données neuronales : un modèle à multi-échelles}

\absSideBySide{A large portion of our knowledge about the brain relies on the statistical models developed for the analysis of neuronal data. In this talk, we show that our conclusions depend on the time scales in which spike trains are analyzed. Two flexible models within the inhomogeneous Poisson process framework are discussed. Both of these formulations account for simultaneous biological phenomena, from multiple time scales, such as neural refractory period, burst spiking activity, brain rhythms, and learning. Simulations and real data analyses highlight the promising results of these computationally efficient models.
}{Une grande partie de nos connaissances sur le cerveau se fonde sur les modèles statistiques conçus pour l'analyse de données neuronales. Dans cet exposé, nous démontrons que nos conclusions dépendent des échelles de temps utilisées dans l'analyse des  trains d'impulsions. Nous discutons de deux modèles flexibles dans le cadre du processus non homogène de Poisson. Chacune de ces formulations prend en compte le phénomène biologique simultané à partir de plusieurs échelles de temps, comme par exemple la période réfractaire neuronale, les pics d'activité, les rythmes cérébraux et l'apprentissage. Des simulations et des analyses de données réelles mettent en évidence les résultats prometteurs de ces modèles informatiques efficaces.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 3C-I1: Isobel Loutit Lecture\\Allocution invit\'ee Isobel Loutit}
\begin{center}{\large Organizer and Chair / Responsable et président:  Hugh Chipman (Acadia)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 242\end{center}
\label{abs-sid:il}
\begin{center}{\large\bfseries Abstract/R\'esum\'e}
\end{center}
%% Talk il-cma
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 13:30]\\}
{\Author{Christine M.}{Anderson-Cook}{andersoncook}{3C-I1}%
\abshead{\absauthor{CHRISTINE M. ANDERSON-COOK}\absaffil{Los Alamos National Laboratory}}
        {Realistic Optimal Designs in a Resource-Constrained World\newline
        Plans optimaux réalistes dans un monde à ressources limitées}

\absSideBySide{	Historically designed experiments were chosen from a collection of designs itemized in the statistics literature, with limited size and design region shape options. These designs had good general properties with some robustness across applications. Next came computer-generated designs optimal for a single criterion. These designs allowed greater size and region flexibility, but lacked overall robustness. An emerging area seeks to combine the best of both worlds: well-rounded performance with increased flexibility. Priorities of experimenters differ and economic pressures on experiments are increasing. Hence, identifying leading candidate designs based on user-specified design criteria allows for better designs. A flexible two-stage Pareto front approach is described, which objectively finds promising candidates using experimenter priorities. Diverse examples illustrate the approach.
}{Historiquement, les expériences étaient conçues d'après une collection de plans énumérés dans la littérature statistique, avec des choix limités quant à la taille et à la forme de la région du plan. Ces plans possédaient de bonnes propriétés générales et une certaine robustesse d'une application à l'autre. Par la suite, la conception assistée par ordinateur a permis d'optimiser les expériences pour un critère donné. Ces plans offraient plus de souplesse en ce qui concerne la taille et la région, mais manquaient de robustesse globale. Un nouveau domaine de recherche cherche à combiner le meilleur des deux mondes: performance homogène et souplesse accrue. Les expérimentateurs n'ont pas tous les mêmes priorités et les pressions économiques sur les expériences ne font qu'augmenter. L'identification des meilleures options de plans en fonction des critères de conception de l'utilisateur est donc essentielle. Je décris une approche de front de Pareto souple à deux étapes qui permet objectivement de trouver des candidats prometteurs en fonction des priorités de l'expérimentateur. J'illustrerai cette approche avec divers exemples.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 3C-I2: Survey Methods Section Presidential Invited Address\\Allocution de l'invit\'e du pr\'esident du Groupe des m\'ethodes d'enqu\^ete}
\begin{center}{\large Organizer and Chair / Responsable et présidente:  Karla Fox (Statistics Canada)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Academic South 204\end{center}
\label{abs-sid:sms}
\begin{center}{\large\bfseries Abstract/R\'esum\'e}
\end{center}
%% Talk sms-mh
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 13:30]\\}
{\Author{Michael}{Hidiroglou}{hidiroglou}{3C-I2}%
\abshead{\absauthor{MICHAEL HIDIROGLOU}\absaffil{Statistics Canada}}
        {Data Collection\newline
        Collecte de données}

\absSideBySide{A large-scale survey can be divided into several stages. Given that a survey is selected from a good quality frame, a survey methodologist should be in good control of all stages. One of the most critical steps is data collection, and it is increasingly more difficult to control. There are several reasons for this. Data collection costs are increasing and response is decreasing. In addition, the mode of collection has changed in recent years.\\
\\
This talk will provide an overview of some the difficulties encountered in data collection. It will also outline a number of solutions that include methodological innovations, as well as the use of alternative data sources to model (substitute) the data of interest.
}{Une enquête à grande échelle peut être divisée en plusieurs étapes. Un méthodologiste d'enquête devrait être en mesure d'avoir une bonne maîtrise de toutes les étapes. Une des étapes les plus cruciales est la collecte de données, et elle devient de plus en plus difficile à contrôler. Il y a plusieurs raisons pour ceci. Les coûts de collecte de données sont en augmentation et la réponse est en baisse. En plus, le mode de collecte a beaucoup changé pendant ces dernières années.\\
\\
Cette présentation donnera un aperçu des difficultés rencontrées lors de la collecte des données. Il décrira également un certain nombre de solutions qui incluent des innovations méthodologiques, ainsi que l'utilisation d'autres sources de données.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 3C-A1: CJS Award Address\\Allocution du r\'ecipiendaire du Prix de la RCS}
\begin{center}{\large Chair/Président: Richard Lockhart (Simon Fraser University)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Welch - David Howes\end{center}
\label{abs-sid:caa}
\begin{center}{\large\bfseries Abstract/R\'esum\'e}
\end{center}
%% Talk caa-gyy
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 13:30]\\}
{\Author{Grace Y.}{Yi}{yi}{3C-A1}%
\abshead{\absauthor{GRACE Y. YI}\absaffil{University of Waterloo}}
        {Variable Selection and Inference Procedures for Marginal Analysis of Longitudinal Data with Missing Observations and Covariate Measurement Error\newline
        Sélection de variables et procédures d'inférence pour l'analyse marginale de données longitudinales avec observations manquantes et erreur de mesure des covariables}

\absSideBySide{Model selection for univariate data has received extensive attention whereas research on model selection for longitudinal data remains largely unexplored. This is particularly true for data subject to missingness and measurement error. To address this important problem, we propose marginal methods that simultaneously carry out model selection and estimation for longitudinal data with missing responses and error-prone covariates. Our methods have several appealing features: broad applicability because development is for a unified framework with marginal generalized linear models; minimal model assumptions in that no full distribution is required for the response process and the distribution of the mismeasured covariates is left unspecified; straightforward implementation. To justify the proposed methods, we provide both theoretical properties and numerical assessments.
}{La sélection de modèles pour les données univariées a fait l'objet de nombreuses recherches, contrairement à la sélection de modèles pour les données longitudinales. Ceci est particulièrement vrai pour les données avec observations manquantes et erreur de mesure. Pour remédier à ce problème important, nous proposons des méthodes marginales qui permettent à la fois la sélection de modèles et l'estimation pour les données longitudinales avec réponses manquantes et covariables sujettes à erreur. Nos méthodes présentent plusieurs caractéristiques intéressantes: elles sont largement applicables car développées pour un cadre unifié avec des modèles linéaires généralisés marginaux; les modèles ne nécessitent que des hypothèses minimales dans la mesure où le processus de réponse n'exige pas une distribution complète et la distribution des covariables mal mesurées n'est pas spécifiée; ces méthodes sont faciles à mettre en oeuvre. À titre de justification, nous présentons les propriétés théoriques et des évaluations numériques des méthodes proposées.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 3C-A2: Pierre Robillard Award Address\\Allocution du r\'ecipiendaire du Prix Pierre-Robillard}
\begin{center}{\large Chair/Président: Juli Atherton (UQAM)\protect\\[5pt]
Organizer/Responsable: Gordon (Fick)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 325\end{center}
\label{abs-sid:pra}
\begin{center}{\large\bfseries Abstract/R\'esum\'e}
\end{center}
%% Talk pra-j
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 13:30]\\}
{\Author{Yujie}{Zhong}{zhong}{3C-A2}%
\abshead{\absauthor{YUJIE ZHONG}\absaffil{Cambridge Institute of Public Health}}
        {Inference About Within-Family Associations in Disease Onset Times Under Biased Sampling\newline
        Inférence sur les associations familiales dans les moments d'apparition de la maladie en cas d'échantillonnage biaisé}

\absSideBySide{The heritability of disease is often studied by examining within-family associations in onset times. Families for such studies typically employ biased sampling schemes in which affected individuals are sampled, along with relatives providing right-censored or current status information on disease onset times. We develop likelihood and composite likelihood methods for modeling within-family associations using copula functions and second-order regression models in which dependencies are characterized by Kendall's tau. Auxiliary data from independent individuals are exploited by augmenting composite likelihoods to increase precision of marginal parameter estimates and consequently increase efficiency in dependence parameter estimation. An application to a motivating family study in psoriatic arthritis illustrates the method and provides evidence of excessive paternal transmission of risk.
}{L'héritabilité des maladies s'étudie souvent via un examen des associations familiales en ce qui concerne le moment d'apparition de la maladie. Dans de telles études, les schémas d'échantillonnage sont souvent biaisés dans la mesure où les individus sont échantillonnés avec leurs proches qui fournissent des informations de statut censurées à droite ou actuelles concernant le moment d'apparition de la maladie. Nous mettons au point des méthodes de vraisemblance et de vraisemblance composée pour modéliser les associations familiales à l'aide de fonctions de copule et de modèles de régression de second ordre où les dépendances sont caractérisées par le tau de Kendall. Nous exploitons des données auxiliaires d'individus indépendants en augmentant les vraisemblances composées pour améliorer la précision des estimations des paramètres marginaux et l'efficacité de l'estimation des paramètres de dépendance. Nous illustrons la méthode par une application à une étude motivante familiale en arthrite psoriasique et prouvons la transmission paternelle excessive.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 3D-I1: Financial and Actuarial Mathematics\\Math\'ematique actuarielle et financi\`ere}
\begin{center}{\large Organizer and Chair / Responsable et président:  Cody Hyndman (Concordia University)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 242\end{center}
\label{abs-sid:afm}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk afm-ab
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 15:00]\\}
{\Author{Alex}{Badescu}{badescu}{3D-I1}\Author{Juan-Pablo}{Ortega}{ortega}{3D-I1}\Author{Zhenyu}{Cui}{cui}{3D-I1}%
\abshead{\absauthor{ALEX BADESCU}\absaffil{University of Calgary}, \absauthor{JUAN-PABLO ORTEGA}\absaffil{CNRS, Université de Franche-Compté}, \absauthor{ZHENYU CUI}\absaffil{Stevens Institute of Technology}}
        {Non-Affine GARCH Option Pricing Models, Variance Dependent Kernels, and Diffusion Limits\newline
        Modèles d'évaluation d'options GARCH non affine, noyaux dépendant de la variance et limites de diffusion}

\absSideBySide{This paper investigates the pricing and weak convergence of an asymmetric non-affine, non-Gaussian GARCH model when the risk-neutralization is based on a variance dependent exponential linear pricing kernel with stochastic risk aversion parameters. The risk-neutral dynamics are obtained for a general setting and its weak limit is derived. We show how several GARCH diffusions, martingalized via well-known pricing kernels, are obtained as special cases and we derive necessary and sufficient conditions for the presence of financial bubbles. An extensive empirical analysis using both historical returns and options data illustrates the advantage of coupling this pricing kernel with non-Gaussian innovations.
}{J'étudie l'établissement du prix et la faible convergence d'un modèle GARCH non affine non gaussien asymétrique dans lequel la neutralisation du risque est basée sur un noyau d'établissement du prix linéaire exponentiel dépendant de la variance avec des paramètres d'aversion au risque stochastiques. La dynamique risque neutre est obtenue pour des conditions générales et sa limite faible est dérivée. Je montre comment plusieurs diffusions GARCH, martingalisées via des noyaux d'établissement du prix bien connus, sont obtenues à titre de cas spéciaux; j'en dérive les conditions nécessaires et suffisantes pour la présence d'une bulle financière. Une analyse empirique approfondie de rendements historiques et de donnes relatives aux options illustre l'avantage de combiner ce noyau d'établissement du prix avec des innovations non gaussiennes.
}}
%% Talk afm-ch
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 15:30]\\}
{\Author{Cody}{Hyndman}{hyndman}{3D-I1}\Author{Samira}{Shirgir}{shirgir}{3D-I1}%
\abshead{\absauthor{CODY HYNDMAN} \& \absauthor{SAMIRA SHIRGIR}\absaffil{Concordia University}}
        {Issuing a Convertible Bond with Call-Spread Overlay: Incorporating the Effects of Convertible Arbitrage\newline
        Émission d'une obligation convertible avec superposition d'opérations mixtes sur option d'achat~: comment inclure les effets de l'arbitrage convertible}

\absSideBySide{Firms may attempt to mitigate some of the negative impacts of issuing convertible bonds, such as the dilution of existing shares, by concurrently entering into ``call-spread overlays'' or other transactions. Previous empirical studies show the stock prices of convertible bond issuers drops on the issue announcement date due to the activities of convertible bond arbitrageurs. We explore the motivation for using these combined transactions and price the convertible bonds with call-spread subject to default risk. We propose a model to estimate the drop in the stock price due to convertible bond arbitrage activities at the time of planning and designing the security to be offered but before announcement. We examine the features of the model with simulated and real-world data.
}{Les entreprises tentent parfois de mitiger certains effets négatifs de l'émission d'obligations convertibles, comme la dilution d'actions existantes, en effectuant simultanément des transactions de «~call-spread overlays~» (superposition d'opérations mixtes sur option d'achat) ou autres transactions. Des études empiriques ont montré que le cours des actions des émetteurs d'obligations convertibles chute à la date de l'annonce de l'émission en raison des activités des arbitragistes en obligations convertibles. Nous explorons la motivation de l'utilisation de ces transactions combinées et établissons le prix des obligations convertibles avec «~call-spread~» sous réserve du risque de défaut. Nous proposons un modèle permettant d'estimer la réduction du cours de l'action résultant des activités d'arbitrage en obligations convertibles au moment de la planification et de la conception de la valeur mobilière, avant son annonce. Nous examinons les caractéristiques du modèle avec des données simulées et réelles.
}}
%% Talk afm-am
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 16:00]\\}
{\Author{Adam}{Metzler}{metzler}{3D-I1}\Author{Alexandre}{Scott}{scott}{3D-I1}%
\abshead{\absauthor{ADAM METZLER}\absaffil{Wilfred Laurier University}, \absauthor{ALEXANDRE SCOTT}\absaffil{BMO}}
        {Importance Sampling for Portfolio Loss Probabilities under Conditional Independence\newline
        Échantillonnage préférentiel pour probabilités de perte sur portefeuille avec indépendance conditionnelle}

\absSideBySide{We develop a novel importance sampling algorithm for estimating the probability of large portfolio losses in the conditional independence framework. We apply exponential tilts to (i) the distribution of the natural sufficient statistic of the systematic risk factors and (ii) the conditional distribution of portfolio loss, given the simulated values of the systematic risk factors, and select parameter values by minimizing the Kullback-Leibler divergence of the resulting parametric family from the ideal (zero-variance) importance density. Optimal parameter values are shown to satisfy intuitive moment-matching conditions, and the asymptotic behaviour of large portfolios is used to approximate the requisite moments. In a sense we extend Glasserman and Li (2005) to allow for heavy-tailed risk factors and/or PD-LGD correlation.
}{Nous développons un nouvel algorithme d'échantillonnage préférentiel permettant d'estimer la probabilité de lourdes pertes sur portefeuille dans un cadre d'indépendance conditionnelle. Nous appliquons des inclinaisons exponentielles à (i) la distribution de la statistique exhaustive naturelle des facteurs de risque systémiques et (ii) la distribution conditionnelle de la perte sur portefeuille, compte tenu des valeurs simulées des facteurs de risque systémiques, puis nous sélectionnons des valeurs de paramètres en minimisant la divergence de Kullback-Leibler de la famille paramétrique résultante de la densité d'importance idéale (variance zéro). Nous montrons que les valeurs de paramètres optimales respectent des conditions de mise en correspondance des moments intuitives et utilisons le comportement asymptotique de gros portefeuilles pour évaluer les moments nécessaires. Dans un certain sens, nous avons généralisé Glasserman et Li (2005) pour permettre des facteurs de risque «~à ailes lourdes~» et/ou une corrélation PD-LGD.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 3D-I2: High Frequency Data\\Donn\'ees de haute fr\'equence}
\begin{center}{\large Organizer and Chair / Responsable et président:  Rafal Kulik (University of Ottawa)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 325\end{center}
\label{abs-sid:hfd}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk hfd-sj
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 15:00]\\}
{\Author{Sebastian}{Jaimungal}{jaimungal}{3D-I2}\Author{Alvaro}{Cartea}{cartea}{3D-I2}\Author{Luhui}{Gan}{gan}{3D-I2}%
\abshead{\absauthor{SEBASTIAN JAIMUNGAL}\absaffil{University of Toronto}, \absauthor{ALVARO CARTEA}\absaffil{Oxford University}, \absauthor{LUHUI GAN}\absaffil{University of Toronto}}
        {Option Pricing and Hedging with Limit and Market Orders\newline
        Fixation des prix en fonction de la valeur d'option et couverture avec ordres à cours limité et ordres au cours du marché}

\absSideBySide{Traditionally, option valuation is carried out using continuous time models for the underlier which are based on diffusive models, sometimes augmented with jump processes. However, such models are far from the reality, e.g., stock prices are inherently discrete and there is a bid-ask spread. Spreads act as costs for market orders, but income for limit orders. In this paper, we introduce a class of pure jump models, in continuous time, develop a calibration methodology, and demonstrate how to value and hedge options using an optimal mix of limit and market orders. Finally, we investigate the qualitative features of the strategies, and demonstrate how they differ from the na\"{\i}ve ones generated from continuous time models.
}{En général, l'évaluation des options est effectuée au moyen de modèles en temps continu pour l'assureur de base. Ces modèles sont basés sur des modèles de diffusion, parfois amplifiés par des processus de saut. Cependant, de tels modèles sont loin d'être réalistes. Par exemple, les cours des actions sont en soi discrets et il y a un écart acheteur-vendeur. Ces écarts agissent comme des coûts pour les ordres au cours du marché, mais comme des revenus pour les ordres à cours limité. Dans cet exposé, nous présentons une classe de modèles purs de saut en temps continu. Nous élaborons une méthode de calage, et nous démontrons comment évaluer et couvrir des options au moyen d'un mélange optimal d'ordres à cours limité et d'ordres au cours du marché. Enfin, nous étudions les caractéristiques qualitatives des stratégies et nous démontrons comment elles diffèrent des caractéristiques na\"{\i}ves produites à partir des modèles de temps continu.
}}
%% Talk hfd-fv
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 15:30]\\}
{\Author{Frederi}{Viens}{viens}{3D-I2}\Author{Baron}{Law}{law}{3D-I2}%
\abshead{\absauthor{FREDERI VIENS} \& \absauthor{BARON LAW}\absaffil{Purdue University}}
        {A Pure-Jump Market-Making Model for High-Frequency Trading using Constrained Forward-Backward Stochastic Differential Equations\newline
        Un modèle de sauts pur pour teneur de marché faisant usage des équations différentielles stochastiques avant-arrière avec contrainte}

\absSideBySide{We propose a new market-making model which incorporates a number of realistic features relevant
for high-frequency trading. We model the dependency structure of prices and order
arrivals with cross-exciting point processes. We formulate the market
maker's decisions as an optimal problem for discrete switching, allowing for over-trading risk,
with market orders modeled as impulse control, avoiding excessive
inventory. Because of the stochastic intensities, the
optimality condition falls outside of the scope of classical Hamilton-Jacobi-Bellman
quasi-variational inequalities, so we leverage a newly developed constrained forward backward
stochastic differential equation (FBSDE) to solve the optimal control problem. The method's
implementability, which includes a Monte-Carlo requirement, is illustrated thanks to full-scale
simulations. This is joint work in progress with Mr. Baron Law.
}{Nous proposons un nouveau modèle de tenue de marché qui intègre des caractéristiques réalistes relatives aux transactions financières à haute fréquence, avec des processus ponctuels à excitation croisée. Les décisions du teneur de marché forment un problème de commutation optimale, avec risque lié à un excès de transactions, et usage des ordres de marché pour se défaire de l'excès d'inventaire. La condition d'optimalité est hors du champ d'application des inégalités quasi-variationnelles classiques de Hamilton-Jacobi-Bellman. Nous nous appuyons sur un concept d'équation différentielle stochastique avant-arrière (FBSDE) pour résoudre le problème de la commande optimale, dont l'applicabilité comprend une composante de Monte-Carlo. Ceci est un travail commun en cours avec M. Baron Law.
}}
%% Talk hfd-ia
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 16:00]\\}
{\Author{Ibrahim}{Abdelrazeq}{abdelrazeq}{3D-I2}%
\abshead{\absauthor{IBRAHIM ABDELRAZEQ}\absaffil{Rhodes College}}
        {Model Checking for the Stochastic Volatility Model: Lévy Driven CARMA(2,1)\newline
        Vérification de modèle pour modèle de volatilité stochastique~: processus CARMA(2,1) régi par un processus de Lévy}

\absSideBySide{The Lévy-driven CARMA(2,1) process is a popular one with which to model stochastic volatility. However, there has been little development of statistical tools to verify this model assumption and assess the goodness-of-fit of real world data (Realized Volatility). When this process is observed at high frequencies, the unobserved driving process can be approximated from the observed process. Since, under general conditions, the Lévy-driven CARMA(2,1) process can be written as a sum of two dependent Lévy-driven Ornstein-Uhlenbeck processes, the methods developed for Ornstein-Uhlenbeck can be employed in order to use the approximated increments of the driving process to test the assumption that the process is Lévy-driven. Performance of the test is illustrated through simulation assuming that the model parameters are known.
}{Le processus CARMA(2,1) régi par un processus de Lévy est un processus populaire pour la modélisation de la volatilité stochastique. Cependant, peu d'outils statistiques ont été développés pour vérifier cette hypothèse de modèle et évaluer l'adéquation à des données réelles (volatilité réalisée). Lorsque ce processus est observé à haute fréquence, le processus gouvernant non observé peut être estimé à partir du processus observé. Puisque, dans des conditions générales, le processus CARMA(2,1) régi par un processus de Lévy peut s'écrire comme la somme de deux processus d'Ornstein-Uhlenbeck dépendants régis par un processus de Lévy, les méthodes développées pour Ornstein-Uhlenbeck peuvent être utilisées pour utiliser les incréments approximatifs du processus gouvernant pour tester l'hypothèse selon laquelle le processus est régi par un processus de Lévy. La performance du test est illustrée par une simulation sous l'hypothèse que les paramètres du modèle sont connus.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 3D-I3: Network Meta-Analysis\\M\'eta-analyse de r\'eseaux}
\begin{center}{\large Organizers/Responsables: Jemila Hamid (St. Michael's Hospital) and/et Lehana Thabane (McMaster University)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Thistle 246\end{center}
\label{abs-sid:nm}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk nm-cd
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 15:00]\\}
{\Author{Caitlin}{Daly}{daly}{3D-I3}\Author{Sharon}{Straus}{straus}{3D-I3}\Author{Lehana}{Thabane}{thabane}{3D-I3}\Author{Jemila S.}{Hamid}{hamid}{3D-I3}%
\abshead{\absauthor{CAITLIN DALY}\absaffil{McMaster University) (dalych@mcmaster.ca}, \absauthor{SHARON STRAUS}\absaffil{Li Ka Shing Knowledge Institute, St. Michael's Hospital}, \absauthor{LEHANA THABANE}\absaffil{McMaster University}, \absauthor{JEMILA S. HAMID}\absaffil{Li Ka Shing Knowledge Institute, St. Michael's Hospital}}
        {Ranking Probabilities and Surface Under Cumulative Ranking Probabilities (SUCRA) in Network Meta-Analysis (NMA)\newline
        Probabilités de classement et surface selon le classement cumulatif de probabilités (SUCRA) dans la méta-analyse en réseau (NMA)}

\absSideBySide{Bayesian NMA has gained more popularity over the frequentist approach since, among other mathematical and computational advantages, it leads to a framework that 
supports decision making through probabilities, rankings and predictions. The Bayesian method provides probabilities, indicating which of the treatments is the 
most effective or safest for each outcome. In particular, it provides the probability that a treatment is the first, second, third, $n^{th}$ best and allows a 
ranking of treatments with respect to their comparative effectiveness. In this talk we will consider ranking probabilities in NMA, cumulative ranking probabilities, 
and SUCRA. NMA of chronic obstructive pulmonary disease interventions will be used to illustrate each method and their sensitivity to the type and amount of 
evidence available.
}{La NMA bayésienne est devenue plus populaire que l'approche fréquentiste puisque, entre autres avantages mathématiques et informatiques, elle conduit à un cadre qui 
supporte la prise de décision fondée sur les probabilités, les classements et les prédictions. La méthode bayésienne donne des probabilités, indiquant ainsi quel 
traitement est le plus efficace ou le plus sûr pour chaque résultat. Notamment, elle donne la probabilité qu'un traitement soit le premier, deuxième, troisième ou $n^{ième}$ 
meilleur et permet un classement des traitements par rapport à leur efficacité relative. Dans cet exposé, nous examinerons les probabilités de classement dans la NMA, 
les probabilités cumulatives de classement et SUCRA. Les NMA d'interventions pour les maladies pulmonaires obstructives chroniques seront utilisées pour illustrer 
chaque méthode ainsi que leur sensibilité au type et à la quantité de preuves disponibles.
}}
%% Talk nm-jh
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 15:30]\\}
{\Author{Jemila S.}{Hamid}{hamid}{3D-I3}\Author{Caitlin}{Daly}{daly}{3D-I3}\Author{Sharon}{Straus}{straus}{3D-I3}%
\abshead{\absauthor{JEMILA S. HAMID}\absaffil{St. Michael’s Hospital) (jhamid@mcmaster.ca}, \absauthor{CAITLIN DALY}\absaffil{McMaster University}, \absauthor{SHARON STRAUS}\absaffil{St. Michael's Hospital}}
        {Integrating Ranking Probabilities in Network Meta-Analysis\newline
        Intégrer les probabilités de classement dans la méta-analyse en réseau}

\absSideBySide{Network meta-analysis (NMA) is a powerful statistical framework for synthesizing evidence from multiple treatments. NMA is often performed at an outcome level 
and results are provided for each outcome separately. Stakeholders and knowledge users are provided with multiple and at times conflicting comparative effectiveness 
and safety rankings of the treatments, leading to difficulties in decision making. Researchers are, therefore, faced with the challenge of how to integrate the 
information from multiple NMAs to aid decision making. The overall objective of this talk is to present conceptual, statistical and methodological frameworks for 
effectively integrating results from multiple NMAs. We will use data from Randomized Controlled Trials comparing treatments for Alzheimer's Dementia to illustrate 
the methods and show application of the methods.
}{La méta-analyse en réseau (NMA) est un cadre statistique puissant pour synthétiser des preuves provenant de traitements multiples. La NMA est souvent accomplie à 
un niveau d'objectifs et les résultats sont donnés pour chaque objectif séparément. De classements de sécurité et d'efficacités multiples et parfois contradictoires 
sont présentés aux parties concernées et aux utilisateurs des connaissances, ce qui rend difficile la prise de décision. Les chercheurs font donc face au défi 
d'intégrer l'information de multiples NMAs pour aider la prise de décision. L'objectif global de cet exposé est de présenter des cadres conceptuels, statistiques et 
méthodologiques pour une intégration efficace des résultats de multiples NMAs. Nous utiliserons des données d'essais contrôlés aléatoires comparant des traitements de 
la démence d'Alzheimer pour illustrer les méthodes et pour démontrer les applications des méthodes.
}}
%% Talk nm-av
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 16:00]\\}
{\Author{Argie}{Veroniki}{veroniki}{3D-I3}%
\abshead{\absauthor{ARGIE VERONIKI}\absaffil{St. Michael’s Hospital) (veronikiA@smh.ca}}
        {Addressing Consistency in Networks of Randomized Trials\newline
        Traiter de la cohérence dans les réseaux des essais aléatoires}

\absSideBySide{Network Meta-Analysis (NMA) is most justifiable if some underlying assumptions are satisfied including consistency between different sources of evidence 
(eg. direct and indirect treatment comparisons) in the network. In this presentation we will examine the various prerequisite assumptions in network meta-analysis 
with particular focus on the evaluation of consistency necessary to obtain valid results. Methodologies to evaluate consistency in networks of interventions with 
complex evidence structure, along with their properties will be presented. Empirical and simulation findings on inconsistency will be also discussed, including 
the identification of potential factors that might control its statistical detection.
}{La méta-analyse en réseau (NMA) est justifiée si quelques hypothèses sous-jacentes sont satisfaites, incluant la cohérence entre différentes sources de preuves 
(par exemple, des comparaisons de traitements directes et indirectes) dans le réseau. Dans cette présentation, nous examinerons les différentes hypothèses préalables 
dans la méta-analyse en réseau en se concentrant sur l'évaluation de la cohérence nécessaire pour l'obtention de résultats valides. Des méthodologies pour évaluer 
la cohérence dans des réseaux d'interventions avec une structure de la preuve complexe ainsi que leurs propriétés seront présentées. Des conclusions empiriques 
et de simulation sur l'incohérence seront aussi discutées, incluant l'identification d'autres facteurs qui pourraient contrôler sa détection statistique.
}}
\newpage
\chead{\vspace{2pt}\large\sffamily\bfseries 3D-I4: Statistical Methods for Analysing Cancer Data\\M\'ethodes statistiques pour analyse des donn\'ees sur le cancer}
\begin{center}{\large Chair/Président: Gregory Pond (McMaster University)\protect\\[5pt]
Organizer/Responsable: Amy Liu (Cancer Care Ontario)}
\end{center}
\begin{center}\large\bfseries Room/Salle: Welch - David Howes\end{center}
\label{abs-sid:sma2}
\begin{center}{\large\bfseries Abstracts/R\'esum\'es}
\end{center}
%% Talk sma2-pb
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 15:00]\\}
{\Author{Patrick}{Brown}{brown}{3D-I4}\Author{Jamie}{Stafford}{stafford}{3D-I4}\Author{Hedy}{Jiang}{jiang}{3D-I4}\Author{Paul}{Nguyen}{nguyen}{3D-I4}\Author{Lennon}{Li}{li}{3D-I4}\Author{Reuben}{Pereira}{pereira}{3D-I4}\Author{Laura}{Feldman}{feldman}{3D-I4}%
\abshead{\absauthor{PATRICK BROWN} \& \absauthor{JAMIE STAFFORD}\absaffil{University of Toronto}, \absauthor{HEDY JIANG}\absaffil{Cancer Care Ontario}, \absauthor{PAUL NGUYEN} \& \absauthor{LENNON LI}\absaffil{Public Health Ontario}, \absauthor{REUBEN PEREIRA} \& \absauthor{LAURA FELDMAN}\absaffil{University of Toronto}}
        {Spatial Statistics with Cancer Registry Data\newline
        Les statistiques spatiales avec les données des registres du cancer}

\absSideBySide{Spatial data in cancer registries is messy; data quality and confidentiality mean only area-level data are available and low counts are suppressed. Area-level spatial models can give results which change with the spatial resolution, a phenomenon known as the 'modifiable area unit problem'. Treating aggregated spatial data as censored point data eliminates this problem, with aggregation reducing power without incurring bias. This approach will be shown with: a spatial point process model for cancer data with exact locations; a kernel-smoothing local-EM algorithm for aggregated case counts; and a spatially discrete model for case counts with censoring.
}{Les données spatiales dans les registres du cancer sont malcommodes; la qualité des données et la confidentialité rend nécessaire que les données soient disponibles au niveau régional seulement et les données avec faibles fréquences sont supprimées. Les modèles spatiaux au niveau de la région peuvent donner des résultats changeant avec la résolution spatiale, un phénomène nommé << problème de l'unité de la zone modifiable >>. Considérer les données spatiales agrégées comme points censurés élimine ce problème, l'agrégation réduisant la puissance sans induire de biais. Cette approche sera illustrée avec: un modèle de processus ponctuel pour les données sur le cancer avec emplacements exacts; un lissage << local-EM >> pour les dénombrements de cas agrégés; et un modèle spatialement discret pour dénombrements censurés.
}}
%% Talk sma2-kk
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 15:30]\\}
{\Author{Karen}{Kopciuk}{kopciuk}{3D-I4}\Author{Laurent}{Briollais}{briollais}{3D-I4}\Author{Yun-Hee}{Choi}{choi}{3D-I4}\Author{Patrick}{Parfrey}{parfrey}{3D-I4}\Author{Jane}{Green}{green}{3D-I4}%
\abshead{\absauthor{KAREN KOPCIUK}\absaffil{University of Calgary}, \absauthor{LAURENT BRIOLLAIS}\absaffil{Lunenfeld-Tanenbaum Research Institute}, \absauthor{YUN-HEE CHOI}\absaffil{The University of Western Ontario}, \absauthor{PATRICK PARFREY} \& \absauthor{JANE GREEN}\absaffil{Memorial University}}
        {Risk Estimation in Family Data\newline
        Estimation du risque dans les données familiales}

\absSideBySide{Estimating risk in families who harbour a genetic mutation predisposing them to several types of cancer presents many statistical challenges: (a) families are identified and selected directly from population disease registries or high risk cancer clinics or from two-stage sampling designs, (b) missing genetic information is common or may be completely unknown for putative genes, (c) residual familial correlation exists when additional risk genes or environmental factors are shared, and (d) sequential cancers are frequent. Models need to take into account these features for age-at-onset outcomes, as well as direct interventions to the disease process. Genetic and disease risk models for family data we have developed will be described as well as our new R package, FamEvent.
}{L'estimation du risque chez les familles porteuses d'une mutation génétique qui les prédispose à certains types de cancer présente divers défis statistiques~: (a) les familles sont identifiées et sélectionnées directement dans les registres des maladies, les centres anticancéreux à haut risque ou à l'aide de plans d'échantillonnage à deux degrés, (b) des informations génétiques manquent ou sont inconnues pour certains gènes putatifs, (c) il existe une corrélation familiale résiduelle si d'autres gènes à risque ou facteurs environnementaux sont partagés, et (d) les cancers séquentiels sont fréquents. Les modèles doivent tenir compte de ces éléments pour l'étude d'événements de type «~âge au diagnostic~» et tenir compte des interventions directes dans le processus de la maladie. Nous décrivons des modèles de risque génétique et de maladie que nous avons développé pour les données familiales et notre nouveau progiciel R, FamEvent.
}}
%% Talk sma2-os
\def\whenwhere{[Wednesday June 1 / mercredi 1er juin, 16:00]\\}
{\Author{Olli}{Saarela}{saarela}{3D-I4}\Author{Zhihui (Amy)}{Liu}{liu}{3D-I4}%
\abshead{\absauthor{OLLI SAARELA}\absaffil{University of Toronto}, \absauthor{ZHIHUI (AMY) LIU}\absaffil{Cancer Care Ontario}}
        {A New Weighted Partial Likelihood Method for Estimating Marginal Structural Hazard Models\newline
        Nouvelle méthode de vraisemblance partielle pondérée pour l'estimation des modèles de risques structurels marginaux}

\absSideBySide{Parameters in marginal structural Cox models can be estimated through an inverse probability of treatment and censoring (IPTC) weighted Cox partial likelihood. We propose an alternative weighted partial likelihood for estimating flexible parametric marginal structural hazard models, based on case-base sampling of person-moments, resulting in a weighted logistic regression form estimating function. This enables estimation of absolute hazards, and can accommodate continuous-time IPTC weights. In terms of computational convenience, the proposed method resembles the conventional discrete time pooled logistic regression method, but works in continuous time. We study the properties of the resulting estimator theoretically and through simulations, and illustrate the method in modeling the effects of repeated treatment procedures in cancer patients.
}{Dans les modèles structurels marginaux de Cox, les paramètres peuvent être estimés par une vraisemblance partielle de Cox pondérée selon la probabilité de traitement et de censure (IPTC). Nous proposons une autre vraisemblance partielle pondérée pour l'estimation flexible des modèles de risques structurels marginaux paramétriques, fondée sur un échantillonnage des personnes-moments selon une approche basée sur les cas, résultant en une fonction d'estimation sous la forme de régression logistique. Cela permet une estimation des risques absolus et l'utilisation de poids IPTC en temps continu. En ce qui concerne la commodité computationnelle, la méthode proposée ressemble à la méthode de régression logistique groupée en temps discret conventionnelle, mais elle fonctionne en temps continu. Nous étudions les propriétés de l'estimateur résultant à la fois en théorie et via des simulations et illustrons la méthode en modélisant les effets de procédures de traitement répétées chez des patients atteints de cancer.
}}
